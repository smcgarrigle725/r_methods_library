{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# SHAP Values for Model Interpretation\n",
    "\n",
    "## Overview\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) assigns each feature a contribution to each prediction, grounded in cooperative game theory. The SHAP value for feature $j$ on observation $i$ is its average marginal contribution across all possible feature orderings.\n",
    "\n",
    "**Guarantees (axioms):**\n",
    "- **Efficiency:** SHAP values sum to prediction − baseline\n",
    "- **Symmetry:** identical features get identical SHAP values\n",
    "- **Dummy:** a feature with no effect gets SHAP = 0\n",
    "- **Additivity:** SHAP values combine correctly across additive models\n",
    "\n",
    "**SHAP plot types:**\n",
    "\n",
    "| Plot | Level | Shows |\n",
    "|---|---|---|\n",
    "| **Bar (mean \\|SHAP\\|)** | Global | Overall feature importance |\n",
    "| **Beeswarm** | Global | Importance + direction + distribution |\n",
    "| **Dependence** | Global | Marginal effect + interaction |\n",
    "| **Waterfall** | Local | How each feature drives one prediction |\n",
    "| **Force** | Local | Compact push/pull representation |\n",
    "\n",
    "Note: this notebook covers tree-based models via `shapviz`. For a shorter intro in the context of classification, see `10_classification/shap_explainability_R.ipynb`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(xgboost)\n",
    "library(shapviz)       # shapviz(), sv_importance(), sv_dependence(), sv_waterfall()\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "n <- 600\n",
    "shap_data <- tibble(\n",
    "  nitrate      = runif(n, 1, 10),\n",
    "  water_qual   = runif(n, 2, 9),\n",
    "  elevation    = rnorm(n, 200, 80),\n",
    "  slope_pct    = abs(rnorm(n, 10, 5)),\n",
    "  distance_km  = rexp(n, 0.5),\n",
    "  richness     = round(\n",
    "    28 - 2.2*nitrate + 1.5*water_qual +\n",
    "    0.015*elevation - 0.3*slope_pct - 0.8*distance_km +\n",
    "    rnorm(n, 0, 3)\n",
    "  )\n",
    ")\n",
    "\n",
    "feat_cols <- c(\"nitrate\",\"water_qual\",\"elevation\",\"slope_pct\",\"distance_km\")\n",
    "X <- as.matrix(shap_data[, feat_cols])\n",
    "y <- shap_data$richness\n",
    "\n",
    "# Train/test split\n",
    "idx_train  <- sample(n, n*0.8)\n",
    "dtrain     <- xgboost::xgb.DMatrix(X[idx_train,], label=y[idx_train])\n",
    "dtest      <- xgboost::xgb.DMatrix(X[-idx_train,], label=y[-idx_train])\n",
    "\n",
    "xgb_fit <- xgboost::xgb.train(\n",
    "  params = list(objective=\"reg:squarederror\", eta=0.05,\n",
    "                max_depth=4, subsample=0.8, colsample_bytree=0.8),\n",
    "  data   = dtrain,\n",
    "  nrounds= 300,\n",
    "  watchlist = list(val=dtest),\n",
    "  early_stopping_rounds = 30,\n",
    "  verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-compute",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing Tree SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-compute-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use held-out test data for SHAP — reflects generalisation, not training fit\n",
    "X_explain <- X[-idx_train, ]\n",
    "\n",
    "shap_obj <- shapviz::shapviz(xgb_fit, X_pred=X_explain)\n",
    "\n",
    "# Verify additivity: SHAP values + baseline = prediction\n",
    "shap_vals  <- shapviz::get_shap_values(shap_obj)\n",
    "baseline   <- shapviz::get_baseline(shap_obj)\n",
    "preds      <- predict(xgb_fit, dtest)\n",
    "\n",
    "reconstruction_error <- max(abs(\n",
    "  rowSums(shap_vals) + baseline - preds\n",
    "))\n",
    "cat(sprintf(\"Max reconstruction error: %.6f (should be near 0)\\n\",\n",
    "            reconstruction_error))\n",
    "cat(sprintf(\"Baseline (mean training prediction): %.2f\\n\", baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-global",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Global Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-global-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Beeswarm: importance + direction + distribution ───────────────────────────\n",
    "# Each point = one observation\n",
    "# Position on x-axis = SHAP value (impact on prediction)\n",
    "# Colour = original feature value (red=high, blue=low)\n",
    "# Features sorted by mean |SHAP| (importance)\n",
    "p_bee <- shapviz::sv_importance(shap_obj, kind=\"beeswarm\") +\n",
    "  labs(title=\"SHAP Beeswarm\",\n",
    "       subtitle=\"Each dot = one observation; colour = feature value; x = prediction impact\")\n",
    "\n",
    "# ── Bar: mean |SHAP| (classic importance) ─────────────────────────────────────\n",
    "p_bar <- shapviz::sv_importance(shap_obj, kind=\"bar\") +\n",
    "  labs(title=\"SHAP Feature Importance\",\n",
    "       subtitle=\"Mean |SHAP| across all test observations\")\n",
    "\n",
    "(p_bar | p_bee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-dependence",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-dep-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plot: SHAP value vs. feature value\n",
    "# Shows the marginal effect of each feature on predictions\n",
    "# colour_var: colours by a second feature to reveal interactions\n",
    "\n",
    "p_nit  <- shapviz::sv_dependence(shap_obj, v=\"nitrate\",\n",
    "                                  color_var=\"water_qual\") +\n",
    "  labs(title=\"Nitrate SHAP Dependence\",\n",
    "       subtitle=\"Colour = water quality; spread = interaction effect\")\n",
    "\n",
    "p_wq   <- shapviz::sv_dependence(shap_obj, v=\"water_qual\",\n",
    "                                  color_var=\"nitrate\") +\n",
    "  labs(title=\"Water Quality SHAP Dependence\")\n",
    "\n",
    "(p_nit | p_wq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-local",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Local Interpretation: Single Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-local-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall: how each feature drove the prediction for one observation\n",
    "# Starting from baseline → each bar adds or subtracts → final prediction\n",
    "\n",
    "# Find a high-richness and a low-richness prediction for contrast\n",
    "pred_order <- order(preds)\n",
    "low_idx    <- pred_order[5]\n",
    "high_idx   <- pred_order[length(pred_order)-4]\n",
    "\n",
    "p_wf_low  <- shapviz::sv_waterfall(shap_obj, row_id=low_idx) +\n",
    "  labs(title=sprintf(\"Waterfall: Low prediction (obs %d)\", low_idx),\n",
    "       subtitle=sprintf(\"Predicted richness: %.1f\", preds[low_idx]))\n",
    "\n",
    "p_wf_high <- shapviz::sv_waterfall(shap_obj, row_id=high_idx) +\n",
    "  labs(title=sprintf(\"Waterfall: High prediction (obs %d)\", high_idx),\n",
    "       subtitle=sprintf(\"Predicted richness: %.1f\", preds[high_idx]))\n",
    "\n",
    "(p_wf_low | p_wf_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Computing SHAP on training data**  \n",
    "SHAP values computed on training data reflect how the model fits those specific observations — including overfitting. Always compute SHAP on held-out test data to understand generalisation behaviour. The difference between training-set and test-set SHAP importance rankings can reveal features the model has overfit.\n",
    "\n",
    "**2. Interpreting SHAP as causal effect**  \n",
    "SHAP values explain the model's predictions, not the data-generating process. A high SHAP for nitrate means the model uses nitrate strongly — not that nitrate causally drives richness in the real world. The model may be using nitrate as a proxy for an unmeasured confounder.\n",
    "\n",
    "**3. Confusing SHAP scale with original feature scale**  \n",
    "For XGBoost regression with objective `reg:squarederror`, SHAP values are in the units of the response (species richness). For classification with `binary:logistic`, SHAP values are in log-odds — not probability. Always check the model objective before interpreting SHAP magnitude.\n",
    "\n",
    "**4. Using gain-based importance instead of SHAP for final reporting**  \n",
    "Gain importance is biased toward features with many split points (continuous variables, high-cardinality categoricals). SHAP importance is consistent and accounts for interactions. For any publication or stakeholder report, use SHAP mean |SHAP| rather than gain importance.\n",
    "\n",
    "**5. Treating the beeswarm as a regression plot**  \n",
    "The beeswarm x-axis shows SHAP values (prediction impact), not the response variable. A positive SHAP for observation i means that feature increases prediction_i above baseline — not that the feature predicts a high response absolutely.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

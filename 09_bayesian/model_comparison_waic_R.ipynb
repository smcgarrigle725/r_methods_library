{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison: WAIC and LOO-CV\n",
    "\n",
    "## Overview\n",
    "\n",
    "Bayesian model comparison evaluates models by their **expected log predictive density (ELPD)** — how well a model predicts new data it has not seen. Two estimators of ELPD:\n",
    "\n",
    "| Method | Full name | Key property |\n",
    "|---|---|---|\n",
    "| **LOO-CV** | Leave-one-out cross-validation | Approximate via PSIS-LOO; gold standard |\n",
    "| **WAIC** | Widely Applicable (Watanabe-Akaike) IC | Asymptotically equivalent to LOO; less stable in tails |\n",
    "\n",
    "Both are computed from the **log-likelihood of each observation** averaged over the posterior — they do not require refitting the model.\n",
    "\n",
    "**Key quantities:**\n",
    "- **ELPD_loo**: expected log predictive density; higher is better\n",
    "- **LOOIC** = −2 × ELPD_loo; lower is better (analogous to AIC scale)\n",
    "- **ELPD difference (Δ ELPD)**: how much better one model is than another\n",
    "- **SE of Δ ELPD**: uncertainty in the comparison\n",
    "- **p_loo**: effective number of parameters; much larger than actual → model is overfit or misspecified\n",
    "\n",
    "**Rule of thumb:** A difference of |Δ ELPD| / SE > 2 indicates a meaningful difference between models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(brms)\n",
    "library(loo)         # loo(), waic(), loo_compare()\n",
    "library(tidybayes)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Simulate data: true relationship is linear ────────────────────────────────\n",
    "n <- 150\n",
    "comp_data <- tibble(\n",
    "  habitat   = factor(rep(c(\"reference\",\"restored\",\"degraded\"), each=n/3),\n",
    "                     levels=c(\"reference\",\"restored\",\"degraded\")),\n",
    "  nitrate   = rnorm(n, 3, 1),\n",
    "  richness  = pmax(round(\n",
    "    25 +\n",
    "    case_when(habitat==\"reference\"~0, habitat==\"restored\"~-3, habitat==\"degraded\"~-8) +\n",
    "    -2 * nitrate +\n",
    "    rnorm(n, 0, 3)\n",
    "  ), 1)\n",
    ")\n",
    "\n",
    "# ── Fit four candidate models ─────────────────────────────────────────────────\n",
    "fit_args <- list(data=comp_data, chains=4, iter=3000,\n",
    "                 warmup=1000, cores=4, seed=42, silent=2)\n",
    "\n",
    "# M1: intercept only\n",
    "m1 <- do.call(brm, c(list(\n",
    "  formula = richness ~ 1,\n",
    "  prior   = c(prior(normal(20,10), class=Intercept),\n",
    "              prior(exponential(1), class=sigma))\n",
    "), fit_args))\n",
    "\n",
    "# M2: nitrate only\n",
    "m2 <- do.call(brm, c(list(\n",
    "  formula = richness ~ nitrate,\n",
    "  prior   = c(prior(normal(20,10), class=Intercept),\n",
    "              prior(normal(0,3),   class=b),\n",
    "              prior(exponential(1), class=sigma))\n",
    "), fit_args))\n",
    "\n",
    "# M3: nitrate + habitat\n",
    "m3 <- do.call(brm, c(list(\n",
    "  formula = richness ~ nitrate + habitat,\n",
    "  prior   = c(prior(normal(20,10), class=Intercept),\n",
    "              prior(normal(0,3),   class=b),\n",
    "              prior(exponential(1), class=sigma))\n",
    "), fit_args))\n",
    "\n",
    "# M4: nitrate * habitat interaction\n",
    "m4 <- do.call(brm, c(list(\n",
    "  formula = richness ~ nitrate * habitat,\n",
    "  prior   = c(prior(normal(20,10), class=Intercept),\n",
    "              prior(normal(0,3),   class=b),\n",
    "              prior(exponential(1), class=sigma))\n",
    "), fit_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-loo",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LOO-CV: Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-loo-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LOO criterion to each model (computed from posterior log-likelihoods)\n",
    "m1 <- add_criterion(m1, \"loo\")\n",
    "m2 <- add_criterion(m2, \"loo\")\n",
    "m3 <- add_criterion(m3, \"loo\")\n",
    "m4 <- add_criterion(m4, \"loo\")\n",
    "\n",
    "# Inspect LOO for the best candidate\n",
    "print(m3$criteria$loo)\n",
    "# Key output:\n",
    "#   elpd_loo:  ELPD estimate; more positive = better\n",
    "#   p_loo:     effective number of parameters\n",
    "#   looic:     -2 * elpd_loo; lower = better\n",
    "#   Monte Carlo SE: sampling uncertainty in the LOO estimate\n",
    "#\n",
    "# Pareto k values:\n",
    "#   k < 0.5:  good (reliable PSIS approximation)\n",
    "#   k in 0.5–0.7: OK\n",
    "#   k > 0.7:  bad — observation is highly influential; refits needed\n",
    "\n",
    "# ── Pareto k diagnostics ─────────────────────────────────────────────────────\n",
    "plot(m3$criteria$loo, diagnostic=\"k\", label_points=TRUE)\n",
    "# High-k observations are overly influential (outliers or model misspecification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-compare",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-compare-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all four models\n",
    "comp <- loo_compare(m1, m2, m3, m4)\n",
    "print(comp, simplify=FALSE)\n",
    "# Models ranked by ELPD (best first)\n",
    "# elpd_diff: how much worse each model is than the best\n",
    "# se_diff:   SE of the difference\n",
    "# Models within 2 SE of the best are plausible alternatives\n",
    "\n",
    "# ── Formatted comparison table ────────────────────────────────────────────────\n",
    "comp_tidy <- as.data.frame(comp) %>%\n",
    "  rownames_to_column(\"model\") %>%\n",
    "  mutate(\n",
    "    z_score   = round(elpd_diff / se_diff, 2),\n",
    "    looic     = round(-2 * elpd_loo, 1),\n",
    "    delta_looic = round(-2 * elpd_diff, 1)\n",
    "  ) %>%\n",
    "  select(model, elpd_loo, se_elpd_loo, elpd_diff, se_diff, z_score, p_loo, looic)\n",
    "print(comp_tidy)\n",
    "\n",
    "# ── Interpretation ────────────────────────────────────────────────────────────\n",
    "cat(\"\\nKey rule: |elpd_diff| / se_diff > 2 → meaningful difference\\n\")\n",
    "cat(\"p_loo >> k (params): possible overfitting or misspecification\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-waic",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## WAIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-waic-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAIC: an alternative information criterion on the log-score scale\n",
    "# Generally prefer LOO for Bayesian models (more stable in the tails)\n",
    "# WAIC is equivalent to LOO asymptotically but differs in small samples\n",
    "\n",
    "m3 <- add_criterion(m3, \"waic\")\n",
    "m4 <- add_criterion(m4, \"waic\")\n",
    "print(m3$criteria$waic)\n",
    "\n",
    "# ── LOO vs. WAIC: compare estimates ──────────────────────────────────────────\n",
    "tibble(\n",
    "  criterion  = c(\"ELPD (LOO)\",\"ELPD (WAIC)\"),\n",
    "  m3_elpd    = c(m3$criteria$loo$estimates[\"elpd_loo\",\"Estimate\"],\n",
    "                 m3$criteria$waic$estimates[\"elpd_waic\",\"Estimate\"]),\n",
    "  m4_elpd    = c(m4$criteria$loo$estimates[\"elpd_loo\",\"Estimate\"],\n",
    "                 m4$criteria$waic$estimates[\"elpd_waic\",\"Estimate\"])\n",
    ") %>%\n",
    "  mutate(\n",
    "    diff = round(m3_elpd - m4_elpd, 2),\n",
    "    across(where(is.numeric), ~round(.x, 2))\n",
    "  ) %>% print()\n",
    "# LOO and WAIC should agree when models are well-specified\n",
    "# Large discrepancy signals problematic observations (check Pareto k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-loo-plot",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualising Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-plot-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELPD comparison plot\n",
    "comp_df <- as.data.frame(comp) %>%\n",
    "  rownames_to_column(\"model\") %>%\n",
    "  mutate(model=factor(model, levels=rev(rownames(comp))))\n",
    "\n",
    "ggplot(comp_df, aes(x=elpd_diff, xmin=elpd_diff - 2*se_diff,\n",
    "                    xmax=elpd_diff + 2*se_diff, y=model)) +\n",
    "  geom_pointrange(color=\"#4a8fff\", linewidth=0.8, fatten=3) +\n",
    "  geom_vline(xintercept=0, linetype=\"dashed\", color=\"gray50\") +\n",
    "  labs(title=\"LOO-CV Model Comparison\",\n",
    "       subtitle=\"ELPD difference vs. best model; bars = 2 × SE\",\n",
    "       x=\"ELPD difference from best model\", y=NULL) +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── Point-wise LOO contributions ─────────────────────────────────────────────\n",
    "# Which observations drive the model comparison?\n",
    "pw_loo_m3 <- m3$criteria$loo$pointwise[,\"elpd_loo\"]\n",
    "pw_loo_m4 <- m4$criteria$loo$pointwise[,\"elpd_loo\"]\n",
    "pw_diff   <- pw_loo_m3 - pw_loo_m4\n",
    "\n",
    "tibble(obs=1:n, diff=pw_diff, nitrate=comp_data$nitrate) %>%\n",
    "  ggplot(aes(x=obs, y=diff,\n",
    "             color=diff > 0)) +\n",
    "  geom_col(alpha=0.7) +\n",
    "  scale_color_manual(values=c(\"TRUE\"=\"#4a8fff\",\"FALSE\"=\"#ff6b6b\"),\n",
    "                     labels=c(\"M4 better here\",\"M3 better here\")) +\n",
    "  geom_hline(yintercept=0, color=\"gray30\") +\n",
    "  labs(title=\"Point-wise ELPD: M3 vs. M4\",\n",
    "       subtitle=\"Each bar = one observation; shows where models differ in predictive performance\",\n",
    "       x=\"Observation\", y=\"ELPD(M3) - ELPD(M4)\", color=NULL) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Selecting the best model by LOO then acting as if it were known to be true**  \n",
    "LOO ranks models by predictive performance — it does not validate any model as the correct data-generating process. When models are within 2 SE of each other, the comparison is inconclusive and model averaging should be considered.\n",
    "\n",
    "**2. Ignoring Pareto k > 0.7 warnings**  \n",
    "High Pareto k values indicate that the LOO approximation is unreliable for specific observations — those observations are highly influential and the model struggles to predict them. This signals either outliers or model misspecification. Do not proceed without investigating.\n",
    "\n",
    "**3. Using p_loo as a model complexity penalty like AIC's k**  \n",
    "p_loo is the effective number of parameters, not the actual parameter count. p_loo >> actual parameters usually indicates overfitting or a misspecified model. p_loo < actual parameters (possible in hierarchical models with strong pooling) is fine.\n",
    "\n",
    "**4. Comparing models with different likelihoods using LOOIC**  \n",
    "Like AIC, LOOIC values are only comparable across models with the same response variable and the same likelihood family. You cannot compare a Gaussian model with a Poisson model using LOOIC — they are on different log-likelihood scales.\n",
    "\n",
    "**5. Reporting WAIC when LOO is available**  \n",
    "WAIC is less stable than PSIS-LOO in the tails of the predictive distribution, and `brms` computes LOO efficiently. Use WAIC only when LOO is computationally intractable (very large datasets with exact LOO).\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

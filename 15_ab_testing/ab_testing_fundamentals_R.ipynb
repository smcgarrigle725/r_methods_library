{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# A/B Testing Fundamentals: Proportions and Means\n",
    "\n",
    "## Overview\n",
    "\n",
    "An A/B test is a randomised experiment comparing two conditions. The same logic applies whether the outcome is binary (conversion, response, presence/absence) or continuous (richness, concentration, score). The workflow is identical in both cases:\n",
    "\n",
    "1. **Pre-specify:** primary outcome, MDE, α, power → compute required n\n",
    "2. **Randomise** units to A and B\n",
    "3. **Collect** data to the pre-specified n (do not peek)\n",
    "4. **Test** using the appropriate test\n",
    "5. **Report** the effect size with confidence interval, not just p-value\n",
    "\n",
    "**Test selection:**\n",
    "\n",
    "| Outcome | Equal variance? | Test |\n",
    "|---|---|---|\n",
    "| Continuous | Assumed equal | Student's t-test |\n",
    "| Continuous | Not assumed | Welch's t-test (default in R) |\n",
    "| Binary / proportions | — | Two-proportion z-test |\n",
    "| Count | Overdispersed | Negative binomial regression |\n",
    "\n",
    "**Effect size reporting:**\n",
    "- Continuous: Cohen's d, mean difference with CI\n",
    "- Binary: absolute risk difference, relative risk, odds ratio — all three; never OR alone\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(infer)         # tidy hypothesis testing framework\n",
    "library(effectsize)    # cohens_d(), interpret_d()\n",
    "library(pwr)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Scenario A: Continuous outcome — species richness ─────────────────────────\n",
    "# Control: riparian zones without restoration; Treatment: restored zones\n",
    "n_per <- 65   # from power analysis: d=0.35, α=0.05, power=0.80\n",
    "\n",
    "cont_data <- tibble(\n",
    "  group    = rep(c(\"control\",\"treatment\"), each=n_per),\n",
    "  richness = c(rnorm(n_per, mean=18, sd=5),\n",
    "               rnorm(n_per, mean=20, sd=5.2))\n",
    ")\n",
    "\n",
    "# ── Scenario B: Binary outcome — species presence ─────────────────────────────\n",
    "# Control: p=0.30 presence; Treatment: p=0.42 presence\n",
    "n_bin <- 200\n",
    "bin_data <- tibble(\n",
    "  group   = rep(c(\"control\",\"treatment\"), each=n_bin),\n",
    "  present = c(rbinom(n_bin, 1, 0.30),\n",
    "              rbinom(n_bin, 1, 0.42))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-continuous",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Continuous Outcome: Welch's t-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-cont-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Descriptive summary ───────────────────────────────────────────────────────\n",
    "cont_summary <- cont_data %>%\n",
    "  group_by(group) %>%\n",
    "  summarise(n=n(), mean=mean(richness), sd=sd(richness),\n",
    "            se=sd/sqrt(n), .groups=\"drop\")\n",
    "print(cont_summary)\n",
    "\n",
    "# ── Welch's t-test (var.equal=FALSE: default; recommended) ───────────────────\n",
    "t_result <- t.test(richness ~ group, data=cont_data,\n",
    "                   var.equal=FALSE, alternative=\"two.sided\")\n",
    "print(t_result)\n",
    "\n",
    "# ── Effect size: Cohen's d ────────────────────────────────────────────────────\n",
    "d_result <- effectsize::cohens_d(richness ~ group, data=cont_data)\n",
    "cat(sprintf(\"\\nCohen's d = %.3f [%.3f, %.3f] — %s\\n\",\n",
    "            d_result$Cohens_d, d_result$CI_low, d_result$CI_high,\n",
    "            effectsize::interpret_d(d_result$Cohens_d)))\n",
    "\n",
    "# ── Visualisation ─────────────────────────────────────────────────────────────\n",
    "p_cont <- ggplot(cont_data, aes(x=group, y=richness, fill=group)) +\n",
    "  geom_violin(alpha=0.4, trim=FALSE) +\n",
    "  geom_boxplot(width=0.15, alpha=0.9) +\n",
    "  geom_jitter(width=0.08, alpha=0.3, size=1) +\n",
    "  scale_fill_manual(values=c(control=\"#4a8fff\",treatment=\"#4fffb0\"),\n",
    "                    guide=\"none\") +\n",
    "  labs(title=sprintf(\"A/B Test: Species Richness (Δ=%.1f, d=%.2f, p=%.3f)\",\n",
    "                     diff(rev(cont_summary$mean)),\n",
    "                     abs(d_result$Cohens_d), t_result$p.value),\n",
    "       x=NULL, y=\"Species richness\") +\n",
    "  theme_minimal()\n",
    "p_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-proportion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Binary Outcome: Two-Proportion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-prop-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed proportions\n",
    "prop_summary <- bin_data %>%\n",
    "  group_by(group) %>%\n",
    "  summarise(n=n(), successes=sum(present), proportion=mean(present), .groups=\"drop\")\n",
    "print(prop_summary)\n",
    "\n",
    "# ── Two-proportion z-test ─────────────────────────────────────────────────────\n",
    "prop_test <- prop.test(\n",
    "  x = prop_summary$successes,\n",
    "  n = prop_summary$n,\n",
    "  alternative = \"two.sided\",\n",
    "  correct     = FALSE   # no Yates correction for large n\n",
    ")\n",
    "print(prop_test)\n",
    "\n",
    "# ── Effect sizes: ARD, RR, OR ─────────────────────────────────────────────────\n",
    "p_ctrl <- prop_summary$proportion[prop_summary$group==\"control\"]\n",
    "p_trt  <- prop_summary$proportion[prop_summary$group==\"treatment\"]\n",
    "\n",
    "ARD <- p_trt - p_ctrl                      # absolute risk difference\n",
    "RR  <- p_trt / p_ctrl                      # relative risk\n",
    "OR  <- (p_trt/(1-p_trt)) / (p_ctrl/(1-p_ctrl))  # odds ratio\n",
    "\n",
    "# CI for ARD using normal approximation\n",
    "se_ARD <- sqrt(p_trt*(1-p_trt)/n_bin + p_ctrl*(1-p_ctrl)/n_bin)\n",
    "ard_ci <- ARD + c(-1,1)*1.96*se_ARD\n",
    "\n",
    "cat(sprintf(\"\\nAbsolute risk difference: %.3f [%.3f, %.3f]\\n\", ARD, ard_ci[1], ard_ci[2]))\n",
    "cat(sprintf(\"Relative risk:            %.3f\\n\", RR))\n",
    "cat(sprintf(\"Odds ratio:               %.3f\\n\", OR))\n",
    "cat(\"\\nAlways report ARD alongside RR/OR — relative measures are misleading without the baseline rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-infer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Permutation-Based Testing with `infer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-infer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer: tidy, transparent pipeline for hypothesis testing\n",
    "# Permutation test: no distributional assumptions required\n",
    "\n",
    "obs_stat <- cont_data %>%\n",
    "  infer::specify(richness ~ group) %>%\n",
    "  infer::calculate(stat=\"diff in means\", order=c(\"treatment\",\"control\"))\n",
    "\n",
    "null_dist <- cont_data %>%\n",
    "  infer::specify(richness ~ group) %>%\n",
    "  infer::hypothesize(null=\"independence\") %>%\n",
    "  infer::generate(reps=5000, type=\"permute\") %>%\n",
    "  infer::calculate(stat=\"diff in means\", order=c(\"treatment\",\"control\"))\n",
    "\n",
    "p_value_perm <- null_dist %>%\n",
    "  infer::get_p_value(obs_stat=obs_stat, direction=\"two-sided\")\n",
    "\n",
    "infer::visualize(null_dist) +\n",
    "  infer::shade_p_value(obs_stat=obs_stat, direction=\"two-sided\") +\n",
    "  labs(title=\"Permutation Null Distribution\",\n",
    "       subtitle=sprintf(\"Observed diff = %.2f | Permutation p = %.3f\",\n",
    "                        obs_stat$stat, p_value_perm$p_value),\n",
    "       x=\"Difference in means (treatment − control)\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Reporting only p-values without effect sizes and confidence intervals**  \n",
    "A p-value answers \"is there evidence of any effect?\" — not \"how large is the effect?\" A significant p=0.03 with d=0.08 and CI [0.01, 0.15] describes a statistically detectable but practically negligible difference. Always report the effect size, its confidence interval, and a plain-language description of what the difference means in context.\n",
    "\n",
    "**2. Reporting only relative risk or odds ratio for binary outcomes**  \n",
    "A relative risk of 2.0 sounds impressive but means very different things depending on the baseline: going from 0.001% to 0.002% is the same RR as going from 30% to 60%. Always report the absolute risk difference alongside RR and OR. For rare outcomes (p < 0.1), OR approximates RR; for common outcomes they diverge substantially.\n",
    "\n",
    "**3. Using Student's t-test (equal variances) by default**  \n",
    "Student's t-test assumes equal variances across groups. Welch's t-test (`var.equal=FALSE`, the R default) does not — and has almost identical power when variances are equal. There is no reason to use Student's t-test in practice. Always use Welch's.\n",
    "\n",
    "**4. Peeking at results before the pre-specified sample size is reached**  \n",
    "Running a t-test every week and stopping when p < 0.05 inflates Type I error dramatically — the actual α can exceed 20% even when the nominal level is 5%. Either commit to the pre-specified n and run the test once, or use a sequential testing framework with appropriate α spending. See `sequential_testing.ipynb`.\n",
    "\n",
    "**5. Treating non-significant results as evidence of no effect**  \n",
    "A non-significant A/B test does not mean the two conditions are equivalent — it means the data are consistent with the null, given the study's power. Report the confidence interval for the effect size: if the entire CI falls within the range of practically unimportant effects, you have reasonable evidence of equivalence. If the CI is wide, the study was underpowered.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

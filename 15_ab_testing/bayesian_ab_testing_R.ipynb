{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Bayesian A/B Testing\n",
    "\n",
    "## Overview\n",
    "\n",
    "Bayesian A/B testing answers the questions practitioners actually want to ask: \"What is the probability that B is better than A?\" and \"What is the expected loss from choosing the wrong variant?\" Frequentist tests answer neither — they only assess compatibility with the null hypothesis.\n",
    "\n",
    "**Frequentist vs. Bayesian A/B test:**\n",
    "\n",
    "| Question | Frequentist | Bayesian |\n",
    "|---|---|---|\n",
    "| Is there an effect? | p < α → reject H₀ | P(B > A \\| data) |\n",
    "| How big is it? | Point estimate + CI | Posterior distribution |\n",
    "| Can I stop early? | No (without correction) | Yes — always valid posterior |\n",
    "| Prior knowledge? | Not incorporated | Explicit prior |\n",
    "| Decision framework | Binary reject/accept | Expected loss |\n",
    "\n",
    "**Key outputs:**\n",
    "- **P(B > A):** probability treatment B has higher true value than A\n",
    "- **Expected loss:** expected regret from choosing the worse variant\n",
    "- **Credible interval:** 95% of posterior probability lies here (unlike CI: interpretable as probability)\n",
    "- **Posterior distribution:** full uncertainty about the effect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(bayesAB)       # bayesTest(), plot.bayesTest(), summary.bayesTest()\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Scenario A: binary outcome (species presence) ─────────────────────────────\n",
    "# Control: 28% presence rate; Treatment: 38% presence rate\n",
    "n_A <- 150; n_B <- 150\n",
    "p_A_true <- 0.28; p_B_true <- 0.38\n",
    "\n",
    "obs_A_binary <- rbinom(n_A, 1, p_A_true)\n",
    "obs_B_binary <- rbinom(n_B, 1, p_B_true)\n",
    "\n",
    "# ── Scenario B: continuous outcome (species richness) ────────────────────────\n",
    "obs_A_cont <- rnorm(n_A, mean=18, sd=5)\n",
    "obs_B_cont <- rnorm(n_B, mean=20, sd=5.2)\n",
    "\n",
    "cat(sprintf(\"Binary: A=%.3f, B=%.3f | Continuous: A=%.2f, B=%.2f\\n\",\n",
    "            mean(obs_A_binary), mean(obs_B_binary),\n",
    "            mean(obs_A_cont),   mean(obs_B_cont)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-binary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Binary Outcome: Beta-Binomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-binary-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta prior for conversion/presence rate\n",
    "# Beta(1,1) = uniform (no prior knowledge)\n",
    "# Beta(3,7) = weakly informative prior centred at 30%\n",
    "# Posterior: Beta(alpha + successes, beta + failures)\n",
    "\n",
    "# ── Fit Bayesian test ─────────────────────────────────────────────────────────\n",
    "bayes_bin <- bayesAB::bayesTest(\n",
    "  obs_A_binary,\n",
    "  obs_B_binary,\n",
    "  priors = c(alpha=3, beta=7),   # Beta(3,7): prior belief ≈ 30% baseline\n",
    "  distribution = \"bernoulli\",\n",
    "  n_samples    = 1e5\n",
    ")\n",
    "\n",
    "summary(bayes_bin)\n",
    "\n",
    "# ── Posterior distributions ───────────────────────────────────────────────────\n",
    "plot(bayes_bin, which=\"posteriors\")\n",
    "\n",
    "# ── Probability B > A and expected loss ───────────────────────────────────────\n",
    "p_b_better <- mean(bayes_bin$posteriors$B$`Probability` >\n",
    "                   bayes_bin$posteriors$A$`Probability`)\n",
    "cat(sprintf(\"\\nP(B > A): %.4f (%.1f%%)\\n\", p_b_better, p_b_better*100))\n",
    "\n",
    "# Expected loss from choosing A when B is actually better\n",
    "loss_choose_A <- mean(pmax(0,\n",
    "  bayes_bin$posteriors$B$`Probability` -\n",
    "  bayes_bin$posteriors$A$`Probability`\n",
    "))\n",
    "cat(sprintf(\"Expected loss (choosing A): %.4f\\n\", loss_choose_A))\n",
    "cat(\"→ Interpretable: expected regret from a wrong decision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-continuous",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Continuous Outcome: Normal-Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-cont-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal prior on the mean; Inverse-Gamma on variance\n",
    "# mu_0: prior mean; kappa_0: prior confidence in mean\n",
    "# alpha_0, beta_0: prior on variance\n",
    "\n",
    "bayes_cont <- bayesAB::bayesTest(\n",
    "  obs_A_cont,\n",
    "  obs_B_cont,\n",
    "  priors = c(mu_0=18, kappa_0=1, alpha_0=3, beta_0=75),\n",
    "  # mu_0=18: prior belief about mean; kappa_0=1: weak prior\n",
    "  # beta_0/alpha_0 ≈ 25 = prior variance estimate (SD≈5)\n",
    "  distribution = \"normal\",\n",
    "  n_samples    = 1e5\n",
    ")\n",
    "\n",
    "summary(bayes_cont)\n",
    "\n",
    "# ── Visualise posterior on mean difference ────────────────────────────────────\n",
    "post_diff <- bayes_cont$posteriors$B$Mu - bayes_cont$posteriors$A$Mu\n",
    "\n",
    "ggplot(tibble(diff=post_diff), aes(x=diff)) +\n",
    "  geom_histogram(bins=60, fill=\"#4a8fff\", alpha=0.7, color=\"white\") +\n",
    "  geom_vline(xintercept=0, linetype=\"dashed\", color=\"#ff6b6b\", linewidth=1) +\n",
    "  geom_vline(xintercept=quantile(post_diff, c(0.025,0.975)),\n",
    "             linetype=\"dotted\", color=\"gray40\") +\n",
    "  labs(\n",
    "    title  = \"Posterior Distribution of Mean Difference (B − A)\",\n",
    "    subtitle= sprintf(\"P(B > A) = %.1f%% | 95%% CrI: [%.2f, %.2f] | Median: %.2f\",\n",
    "                      mean(post_diff > 0)*100,\n",
    "                      quantile(post_diff, 0.025),\n",
    "                      quantile(post_diff, 0.975),\n",
    "                      median(post_diff)),\n",
    "    x=\"B − A (species)\", y=\"Posterior density\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-prior-sensitivity",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prior Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-prior-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results should be robust to reasonable prior choices\n",
    "# If conclusions change dramatically with the prior, the data are insufficient\n",
    "\n",
    "priors_to_test <- list(\n",
    "  flat     = c(alpha=1,  beta=1),   # uniform\n",
    "  weak     = c(alpha=3,  beta=7),   # centred at 30%\n",
    "  informed = c(alpha=6,  beta=14),  # stronger belief at 30%\n",
    "  wrong    = c(alpha=10, beta=10)   # prior at 50% (misspecified)\n",
    ")\n",
    "\n",
    "prior_results <- map_dfr(names(priors_to_test), function(nm) {\n",
    "  pr <- priors_to_test[[nm]]\n",
    "  bt <- bayesAB::bayesTest(\n",
    "    obs_A_binary, obs_B_binary,\n",
    "    priors       = pr,\n",
    "    distribution = \"bernoulli\",\n",
    "    n_samples    = 5e4\n",
    "  )\n",
    "  p_B_gt_A <- mean(bt$posteriors$B$Probability > bt$posteriors$A$Probability)\n",
    "  post_A   <- bt$posteriors$A$Probability\n",
    "  post_B   <- bt$posteriors$B$Probability\n",
    "  tibble(\n",
    "    prior       = nm,\n",
    "    prior_mean  = pr[\"alpha\"]/(pr[\"alpha\"]+pr[\"beta\"]),\n",
    "    P_B_gt_A    = round(p_B_gt_A, 4),\n",
    "    post_A_mean = round(mean(post_A), 4),\n",
    "    post_B_mean = round(mean(post_B), 4)\n",
    "  )\n",
    "})\n",
    "\n",
    "print(prior_results)\n",
    "# If P(B>A) is similar across priors: data dominate; conclusions are robust\n",
    "# If P(B>A) varies widely: data are weak; prior specification matters critically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Choosing a prior after seeing the data**  \n",
    "The prior must be specified before examining the data — it represents prior knowledge, not a tuning parameter to be adjusted until results look favourable. Trying multiple priors and reporting the one that gives the most appealing P(B > A) is equivalent to p-hacking. Pre-specify the prior and conduct a sensitivity analysis across a range of defensible alternatives.\n",
    "\n",
    "**2. Using P(B > A) as a decision threshold without considering expected loss**  \n",
    "P(B > A) = 0.95 sounds decisive, but the posterior distribution of the difference might be concentrated near zero — a high probability of a negligible effect. Expected loss integrates both the probability and the magnitude of being wrong. A P(B > A) = 0.85 with a large potential gain may be a better decision than P(B > A) = 0.97 with a tiny gain.\n",
    "\n",
    "**3. Claiming that Bayesian testing allows unlimited peeking**  \n",
    "Some Bayesian frameworks (e.g. mSPRT, always-valid inference) do allow continuous monitoring with proper error control. Standard Bayesian A/B testing with fixed priors does not — repeatedly checking the posterior and stopping when P(B > A) > 0.95 inflates error just as frequentist peeking does. Use a properly calibrated sequential Bayesian design if early stopping is needed.\n",
    "\n",
    "**4. Interpreting the credible interval as a confidence interval**  \n",
    "A 95% Bayesian credible interval [a, b] means: given the prior and data, there is a 95% posterior probability that the true parameter lies in [a, b]. This IS the intuitive interpretation most people incorrectly apply to frequentist CIs. They are philosophically different constructs — but the practical distinction matters mainly when priors are informative.\n",
    "\n",
    "**5. Using flat priors and calling the analysis \"objective\"**  \n",
    "A flat (uniform) prior is not objective — it still encodes assumptions (e.g. that a 1% conversion rate is as plausible as a 99% rate). For proportion testing, a weakly informative prior centred near the historical baseline is almost always more appropriate than a flat prior. Report the prior and its justification alongside results.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

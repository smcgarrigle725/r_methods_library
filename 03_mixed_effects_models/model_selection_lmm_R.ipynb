{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Model Selection for LMMs and GLMMs in R\n",
    "\n",
    "## Overview\n",
    "\n",
    "Model selection for mixed effects models involves two related but distinct decisions:\n",
    "\n",
    "1. **Random effects structure selection** — how complex should the random effects be? (random intercepts only vs. random intercepts + slopes)\n",
    "2. **Fixed effects selection** — which predictors should be in the model?\n",
    "\n",
    "These two decisions use different tools and should be approached in sequence: determine the maximal defensible random effects structure first, then select among fixed effects.\n",
    "\n",
    "| Tool | Use Case |\n",
    "|---|---|\n",
    "| Likelihood ratio test (LRT) | Compare nested models — formally tests whether a more complex model fits significantly better |\n",
    "| AIC / BIC | Compare non-nested models or rank a set of candidate models |\n",
    "| REML vs. ML | REML for comparing random effects structures; ML for comparing fixed effects |\n",
    "| `MuMIn::dredge()` | Automated multi-model comparison across fixed effects combinations |\n",
    "\n",
    "> **The keep-it-maximal principle (Barr et al. 2013):** In confirmatory analysis, fit the maximal random effects structure justified by the design. Simplify only if the model fails to converge or produces singular fits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(lme4)\n",
    "library(lmerTest)\n",
    "library(glmmTMB)\n",
    "library(MuMIn)        # dredge() for automated model comparison\n",
    "library(broom.mixed)\n",
    "library(performance)\n",
    "\n",
    "set.seed(42)\n",
    "data(sleepstudy, package = \"lme4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-reml-ml",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## REML vs. ML: When to Use Each\n",
    "\n",
    "This is one of the most common sources of confusion in mixed model fitting.\n",
    "\n",
    "| Comparison | Estimation Method | Why |\n",
    "|---|---|---|\n",
    "| Random effects structures (same fixed effects) | REML | REML gives unbiased variance estimates; LRT on REML likelihoods is valid for nested random effects |\n",
    "| Fixed effects structures (same random effects) | ML | REML likelihood depends on the fixed effects design matrix; cannot compare different fixed effects with REML |\n",
    "| Final reported model | REML | Variance component estimates are reported from the REML fit |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-reml-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Demonstrate REML vs. ML ───────────────────────────────────────────────────\n",
    "# These two models have DIFFERENT random effects → compare with REML\n",
    "m_ri   <- lmer(Reaction ~ Days + (1 | Subject),\n",
    "                data = sleepstudy, REML = TRUE)\n",
    "m_rs   <- lmer(Reaction ~ Days + (Days | Subject),\n",
    "                data = sleepstudy, REML = TRUE)\n",
    "\n",
    "# ── These two models have DIFFERENT fixed effects → refit with ML ─────────────\n",
    "m_days_ml  <- lmer(Reaction ~ Days + (Days | Subject),\n",
    "                    data = sleepstudy, REML = FALSE)\n",
    "m_null_ml  <- lmer(Reaction ~ 1    + (Days | Subject),\n",
    "                    data = sleepstudy, REML = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-lrt-random",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Select Random Effects Structure\n",
    "\n",
    "Compare nested random effects models using REML likelihood ratio tests. Start with the maximal structure supported by the design and simplify if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-lrt-random-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Candidate random effects structures ──────────────────────────────────────\n",
    "m1 <- lmer(Reaction ~ Days + (1 | Subject),\n",
    "            data = sleepstudy, REML = TRUE)               # random intercepts only\n",
    "m2 <- lmer(Reaction ~ Days + (Days || Subject),\n",
    "            data = sleepstudy, REML = TRUE)               # uncorrelated slopes\n",
    "m3 <- lmer(Reaction ~ Days + (Days | Subject),\n",
    "            data = sleepstudy, REML = TRUE)               # correlated slopes (maximal)\n",
    "\n",
    "# ── LRT: does adding random slopes improve fit? ───────────────────────────────\n",
    "anova(m1, m2, refit = FALSE)   # refit = FALSE keeps REML likelihoods\n",
    "# Significant chi-square = random slopes improve fit significantly\n",
    "\n",
    "# ── LRT: does adding intercept-slope correlation improve fit? ─────────────────\n",
    "anova(m2, m3, refit = FALSE)\n",
    "\n",
    "# ── Check for singular fit ────────────────────────────────────────────────────\n",
    "isSingular(m1); isSingular(m2); isSingular(m3)\n",
    "# TRUE = random effects structure too complex for this data\n",
    "\n",
    "# ── Select based on LRT results + singularity check ──────────────────────────\n",
    "# If m3 fits significantly better than m2 and is not singular: use m3\n",
    "# If singular: fall back to m2 or m1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-lrt-fixed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Select Fixed Effects Structure\n",
    "\n",
    "With random effects determined, compare fixed effects models using ML (not REML) likelihood ratio tests or information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-lrt-fixed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Simulate a richer dataset with multiple predictors ────────────────────────\n",
    "n_subj <- 30; n_obs <- 8\n",
    "subj_re <- rnorm(n_subj, 0, 20)\n",
    "\n",
    "rich_data <- tibble(\n",
    "  subject  = rep(paste0(\"s\", 1:n_subj), each = n_obs),\n",
    "  days     = rep(0:(n_obs-1), times = n_subj),\n",
    "  caffeine = rnorm(n_subj * n_obs, 200, 50),\n",
    "  age      = rep(sample(20:60, n_subj, replace = TRUE), each = n_obs),\n",
    "  reaction = 250 + 10*rep(0:(n_obs-1), times = n_subj) -\n",
    "             0.05*caffeine + 0.5*rep(age, each = n_obs) +\n",
    "             rep(subj_re, each = n_obs) + rnorm(n_subj * n_obs, 0, 25)\n",
    ")\n",
    "\n",
    "# ── Fit candidate fixed effects models (ML for comparison) ────────────────────\n",
    "rf <- list(\n",
    "  null     = lmer(reaction ~ 1 + (days | subject), data = rich_data, REML = FALSE),\n",
    "  days     = lmer(reaction ~ days + (days | subject), data = rich_data, REML = FALSE),\n",
    "  days_caf = lmer(reaction ~ days + caffeine + (days | subject), data = rich_data, REML = FALSE),\n",
    "  full     = lmer(reaction ~ days + caffeine + age + (days | subject), data = rich_data, REML = FALSE)\n",
    ")\n",
    "\n",
    "# ── LRT: sequential model comparison ─────────────────────────────────────────\n",
    "do.call(anova, rf)\n",
    "\n",
    "# ── AIC / BIC comparison table ────────────────────────────────────────────────\n",
    "map_dfr(names(rf), ~ {\n",
    "  g <- broom.mixed::glance(rf[[.x]])\n",
    "  tibble(model = .x, AIC = g$AIC, BIC = g$BIC, logLik = g$logLik)\n",
    "}) %>%\n",
    "  mutate(delta_AIC = AIC - min(AIC),\n",
    "         delta_BIC = BIC - min(BIC)) %>%\n",
    "  arrange(AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-dredge",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Model Inference with `MuMIn::dredge()`\n",
    "\n",
    "For exploratory analyses, `dredge()` fits all possible subsets of a global model and ranks them by AIC. Use with caution — it is a data-exploration tool, not a substitute for hypothesis-driven analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-dredge-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fit global model with ML ──────────────────────────────────────────────────\n",
    "global_model <- lmer(\n",
    "  reaction ~ days + caffeine + age + (days | subject),\n",
    "  data    = rich_data,\n",
    "  REML    = FALSE,\n",
    "  na.action = na.fail   # required by dredge()\n",
    ")\n",
    "\n",
    "# ── Dredge: all fixed effects subsets ─────────────────────────────────────────\n",
    "dredge_results <- MuMIn::dredge(global_model)\n",
    "print(dredge_results)\n",
    "# delta < 2: models with substantial support\n",
    "# delta 2-7: considerably less support\n",
    "# delta > 10: essentially no support\n",
    "\n",
    "# ── Model averaging: average coefficients weighted by AIC weights ─────────────\n",
    "# Use for inference when multiple models have similar support\n",
    "avg_model <- MuMIn::model.avg(dredge_results, subset = delta < 4)\n",
    "summary(avg_model)\n",
    "# 'full' coefficients: averaged over all models (zeroed for absent terms)\n",
    "# 'conditional' coefficients: averaged only over models containing that term\n",
    "\n",
    "# ── Variable importance ───────────────────────────────────────────────────────\n",
    "MuMIn::importance(avg_model)\n",
    "# Sum of AIC weights across all models containing each term\n",
    "# Higher = predictor appears in more high-weight models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-glmm-selection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Selection for GLMMs\n",
    "\n",
    "The same principles apply to GLMMs. REML is not available for GLMs, so all comparisons use ML (which is the default for `glmer` and `glmmTMB`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07-glmm-sel-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Simulate binary outcome data ──────────────────────────────────────────────\n",
    "n_sites <- 25; n_v <- 4; N <- n_sites * n_v\n",
    "site_re <- rnorm(n_sites, 0, 0.8)\n",
    "\n",
    "glmm_data <- tibble(\n",
    "  site   = rep(paste0(\"s\", 1:n_sites), each = n_v),\n",
    "  temp   = rnorm(N, 15, 3),\n",
    "  precip = rnorm(N, 50, 20),\n",
    "  lo     = -1 + 0.12*temp + 0.005*precip + rep(site_re, each = n_v),\n",
    "  y      = rbinom(N, 1, plogis(lo))\n",
    ") %>% select(-lo)\n",
    "\n",
    "# ── Candidate GLMM models ─────────────────────────────────────────────────────\n",
    "gm1 <- lme4::glmer(y ~ 1      + (1|site), data = glmm_data, family = binomial)\n",
    "gm2 <- lme4::glmer(y ~ temp   + (1|site), data = glmm_data, family = binomial)\n",
    "gm3 <- lme4::glmer(y ~ precip + (1|site), data = glmm_data, family = binomial)\n",
    "gm4 <- lme4::glmer(y ~ temp + precip + (1|site), data = glmm_data, family = binomial)\n",
    "\n",
    "# ── AIC comparison ────────────────────────────────────────────────────────────\n",
    "AIC(gm1, gm2, gm3, gm4) %>%\n",
    "  mutate(delta_AIC = AIC - min(AIC)) %>%\n",
    "  arrange(AIC)\n",
    "\n",
    "# ── LRT for nested models ─────────────────────────────────────────────────────\n",
    "anova(gm1, gm2)   # does adding temp improve fit?\n",
    "anova(gm2, gm4)   # does adding precip to temp model improve fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Comparing models with different fixed effects using REML**  \n",
    "REML likelihoods are not comparable across different fixed effects designs. Always refit with `REML = FALSE` before comparing fixed effects models with LRT or AIC.\n",
    "\n",
    "**2. Simplifying random effects before fixed effects are determined**  \n",
    "Determine random effects structure first using REML, then fix random effects and select fixed effects with ML. Doing this in the wrong order can lead to incorrect variance estimates.\n",
    "\n",
    "**3. Using dredge() for confirmatory hypothesis testing**  \n",
    "`dredge()` is an exploratory tool. Running it on confirmatory data and reporting the best model as if it were hypothesis-driven is p-hacking. Pre-register your model or treat dredge results as hypothesis-generating.\n",
    "\n",
    "**4. Treating ΔAIC < 2 as definitive support**  \n",
    "ΔAIC < 2 indicates comparable support — it does not mean models are equivalent. Report and discuss competing models rather than selecting a single winner.\n",
    "\n",
    "**5. Ignoring singular fit when building random effects**  \n",
    "A singular fit during random effects selection means the data cannot support that complexity. This is useful diagnostic information — simplify the random effects and note it in the methods.\n",
    "\n",
    "**6. Not reporting model selection criteria in methods**  \n",
    "Always document which information criterion was used, whether REML or ML was used for each comparison, and what the candidate model set was. Reproducibility requires this.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Linear Regression in R\n",
    "\n",
    "## Overview\n",
    "\n",
    "Linear regression models the relationship between a continuous response variable and one or more predictors by fitting a line (or hyperplane) that minimizes the sum of squared residuals. It is the foundation for most statistical modeling — understanding it deeply makes every more complex method easier to interpret.\n",
    "\n",
    "| Model | Use Case |\n",
    "|---|---|\n",
    "| Simple linear regression | One continuous predictor |\n",
    "| Multiple linear regression | Two or more predictors (continuous and/or categorical) |\n",
    "| Polynomial regression | Curved relationship between predictor and response |\n",
    "| Regression with interactions | Effect of one predictor depends on the value of another |\n",
    "\n",
    "## Applications by Sector\n",
    "\n",
    "| Sector | Example |\n",
    "|---|---|\n",
    "| **Ecology** | How does invertebrate density change with sediment carbonate chemistry? What environmental variables predict bird abundance across sites? |\n",
    "| **Healthcare** | How does patient age and BMI predict blood pressure? What factors predict length of hospital stay? |\n",
    "| **Finance** | How does interest rate predict loan amount? What features predict house sale price? |\n",
    "| **Insurance** | How do policyholder characteristics predict annual premium? What factors predict claim severity? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-assumptions",
   "metadata": {},
   "source": [
    "## Assumptions Checklist\n",
    "\n",
    "Review before fitting. Diagnostic tests are run inline after the model.\n",
    "\n",
    "- [ ] **Linearity:** The relationship between each predictor and the response is linear (check with scatterplots and residuals vs. fitted plot)\n",
    "- [ ] **Independence:** Observations are independent of one another (no repeated measures, no clustering)\n",
    "- [ ] **Homoscedasticity:** Residual variance is constant across all fitted values (check with scale-location plot)\n",
    "- [ ] **Normality of residuals:** Residuals are approximately normally distributed (check with Q-Q plot and Shapiro-Wilk) — less critical with large n\n",
    "- [ ] **No perfect multicollinearity:** Predictors are not perfectly correlated with one another (check with VIF)\n",
    "- [ ] **No influential outliers:** No single observation exerts undue leverage on the fitted line (check with Cook's distance)\n",
    "\n",
    "> **If linearity is violated:** Consider polynomial terms, log-transforming the response, or GAMs (see `04_gams/`)  \n",
    "> **If homoscedasticity is violated:** Consider log-transforming the response, weighted least squares, or robust standard errors  \n",
    "> **If independence is violated:** Use mixed effects models (see `03_mixed_effects_models/`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Libraries ────────────────────────────────────────────────────────────────\n",
    "library(tidyverse)    # data manipulation and visualization\n",
    "library(ggplot2)      # visualization\n",
    "library(car)          # VIF, Levene's, Type III SS\n",
    "library(performance)  # model diagnostics and R²\n",
    "library(effectsize)   # standardized coefficients\n",
    "library(broom)        # tidy model output\n",
    "library(ggpubr)       # publication-ready plots\n",
    "\n",
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "set.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-data",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use two built-in datasets:\n",
    "- `cars`: stopping distance (ft) as a function of speed (mph) — simple regression\n",
    "- `mtcars`: fuel efficiency (mpg) predicted by engine and vehicle characteristics — multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-data-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect data ──────────────────────────────────────────────────────────────\n",
    "head(cars)\n",
    "glimpse(mtcars)\n",
    "\n",
    "# ── Exploratory plots ─────────────────────────────────────────────────────────\n",
    "# Simple: speed vs. stopping distance\n",
    "ggplot(cars, aes(x = speed, y = dist)) +\n",
    "  geom_point(alpha = 0.7, color = \"#4a8fff\") +\n",
    "  geom_smooth(method = \"lm\", se = TRUE, color = \"#ff6b6b\") +\n",
    "  labs(title = \"Stopping Distance vs. Speed\",\n",
    "       subtitle = \"Visual check for linearity before fitting\",\n",
    "       x = \"Speed (mph)\", y = \"Stopping Distance (ft)\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Multiple: correlation matrix for mtcars predictors\n",
    "mtcars %>%\n",
    "  select(mpg, wt, hp, disp, cyl) %>%\n",
    "  cor() %>%\n",
    "  round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-simple-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "**Question:** Does speed predict stopping distance?  \n",
    "**Model:** dist = β₀ + β₁·speed + ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-simple-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fit model ────────────────────────────────────────────────────────────────\n",
    "model_simple <- lm(dist ~ speed, data = cars)\n",
    "\n",
    "# ── Summary ───────────────────────────────────────────────────────────────────\n",
    "summary(model_simple)\n",
    "# Coefficients: Estimate, Std. Error, t value, p-value\n",
    "# R²: proportion of variance in response explained by the model\n",
    "# Adjusted R²: penalizes for number of predictors — use for model comparison\n",
    "# Residual standard error: average prediction error in response units\n",
    "\n",
    "# ── Tidy output ───────────────────────────────────────────────────────────────\n",
    "broom::tidy(model_simple, conf.int = TRUE)   # coefficients with 95% CIs\n",
    "broom::glance(model_simple)                  # model-level statistics\n",
    "\n",
    "# ── Interpretation ────────────────────────────────────────────────────────────\n",
    "# Intercept (β₀): predicted dist when speed = 0 (may not be meaningful)\n",
    "# Slope (β₁):     for each 1 mph increase in speed, dist increases by β₁ ft\n",
    "# R²:             X% of variance in stopping distance is explained by speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-assumptions-testing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Assumptions Testing\n",
    "\n",
    "Always run diagnostics after fitting. Base R's `plot()` on an `lm` object produces four diagnostic plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-assumptions-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Four standard diagnostic plots ───────────────────────────────────────────\n",
    "par(mfrow = c(2, 2))\n",
    "plot(model_simple)\n",
    "par(mfrow = c(1, 1))\n",
    "\n",
    "# Plot 1 — Residuals vs. Fitted:\n",
    "#   Random scatter around 0 → linearity and homoscedasticity met\n",
    "#   Pattern or curve → non-linearity; fan shape → heteroscedasticity\n",
    "\n",
    "# Plot 2 — Normal Q-Q:\n",
    "#   Points on diagonal line → normality met\n",
    "#   Heavy tails or S-shape → non-normality\n",
    "\n",
    "# Plot 3 — Scale-Location:\n",
    "#   Flat red line, equal spread → homoscedasticity met\n",
    "#   Upward trend → variance increases with fitted values\n",
    "\n",
    "# Plot 4 — Residuals vs. Leverage:\n",
    "#   Points beyond Cook's distance dashed lines → influential observations\n",
    "#   High leverage ≠ influential unless residual is also large\n",
    "\n",
    "# ── Formal tests ──────────────────────────────────────────────────────────────\n",
    "# Normality of residuals\n",
    "shapiro.test(residuals(model_simple))\n",
    "\n",
    "# Homoscedasticity (Breusch-Pagan test)\n",
    "performance::check_heteroscedasticity(model_simple)\n",
    "\n",
    "# Overall assumption check (performance package)\n",
    "performance::check_model(model_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-multiple-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "**Question:** Which vehicle characteristics predict fuel efficiency?  \n",
    "**Model:** mpg = β₀ + β₁·wt + β₂·hp + β₃·cyl + ε\n",
    "\n",
    "> In multiple regression, each coefficient represents the effect of that predictor **holding all other predictors constant**. This is partial effect, not marginal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07-multiple-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fit model ────────────────────────────────────────────────────────────────\n",
    "model_multi <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n",
    "summary(model_multi)\n",
    "\n",
    "# ── Check multicollinearity: Variance Inflation Factor (VIF) ──────────────────\n",
    "car::vif(model_multi)\n",
    "# VIF < 5: acceptable\n",
    "# VIF 5-10: moderate concern — consider removing or combining predictors\n",
    "# VIF > 10: severe multicollinearity — model coefficients are unstable\n",
    "\n",
    "# ── Standardized coefficients (beta weights) ──────────────────────────────────\n",
    "# Allows comparison of predictor importance on the same scale\n",
    "effectsize::standardize_parameters(model_multi)\n",
    "\n",
    "# ── Confidence intervals for coefficients ─────────────────────────────────────\n",
    "confint(model_multi, level = 0.95)\n",
    "\n",
    "# ── Predicted values and residuals ───────────────────────────────────────────\n",
    "mtcars_aug <- broom::augment(model_multi)\n",
    "head(mtcars_aug)  # .fitted, .resid, .cooksd, .hat etc. added automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08-multi-diagnostics",
   "metadata": {},
   "source": [
    "### Diagnostics for Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08-multi-diag-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full diagnostic check ─────────────────────────────────────────────────────\n",
    "par(mfrow = c(2, 2))\n",
    "plot(model_multi)\n",
    "par(mfrow = c(1, 1))\n",
    "\n",
    "# ── Cook's distance: identify influential observations ────────────────────────\n",
    "cooksd <- cooks.distance(model_multi)\n",
    "threshold <- 4 / nrow(mtcars)  # common rule of thumb: 4/n\n",
    "influential <- which(cooksd > threshold)\n",
    "cat(\"Potentially influential observations:\", influential, \"\\n\")\n",
    "\n",
    "# Plot Cook's distances\n",
    "plot(cooksd, type = \"h\",\n",
    "     main = \"Cook's Distance\",\n",
    "     ylab = \"Cook's Distance\",\n",
    "     xlab = \"Observation Index\")\n",
    "abline(h = threshold, col = \"#ff6b6b\", lty = 2)\n",
    "text(influential, cooksd[influential],\n",
    "     labels = names(influential), pos = 4, cex = 0.8)\n",
    "\n",
    "# ── VIF plot ──────────────────────────────────────────────────────────────────\n",
    "vif_vals <- car::vif(model_multi)\n",
    "barplot(vif_vals,\n",
    "        main = \"Variance Inflation Factors\",\n",
    "        ylab = \"VIF\",\n",
    "        col  = ifelse(vif_vals > 5, \"#ff6b6b\", \"#4fffb0\"),\n",
    "        las  = 2)\n",
    "abline(h = 5,  lty = 2, col = \"orange\")\n",
    "abline(h = 10, lty = 2, col = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09-interactions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interactions and Polynomial Terms\n",
    "\n",
    "An interaction term tests whether the effect of one predictor depends on the value of another. A polynomial term allows the response to curve rather than follow a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09-interactions-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Interaction: does the effect of weight on mpg depend on cylinder count? ───\n",
    "model_int <- lm(mpg ~ wt * cyl, data = mtcars)\n",
    "# wt * cyl expands to: wt + cyl + wt:cyl\n",
    "# The wt:cyl term IS the interaction\n",
    "summary(model_int)\n",
    "\n",
    "# ── Visualize interaction ────────────────────────────────────────────────────\n",
    "mtcars %>%\n",
    "  mutate(cyl_f = factor(cyl)) %>%\n",
    "  ggplot(aes(x = wt, y = mpg, color = cyl_f)) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method = \"lm\", se = FALSE) +\n",
    "  scale_color_manual(values = c(\"#4a8fff\", \"#4fffb0\", \"#ffd166\")) +\n",
    "  labs(title = \"MPG ~ Weight, by Cylinder Count\",\n",
    "       subtitle = \"Non-parallel lines indicate an interaction\",\n",
    "       x = \"Weight (1000 lbs)\", y = \"MPG\", color = \"Cylinders\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── Polynomial regression: curvilinear relationship ───────────────────────────\n",
    "model_poly <- lm(dist ~ speed + I(speed^2), data = cars)\n",
    "# I() protects the arithmetic — without it, ^ has a special meaning in formulas\n",
    "summary(model_poly)\n",
    "\n",
    "# Compare linear vs. polynomial fit\n",
    "anova(model_simple, model_poly)\n",
    "# Significant F: polynomial fits significantly better than linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10-model-selection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10-model-selection-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare nested models with F-test ────────────────────────────────────────\n",
    "model_wt      <- lm(mpg ~ wt, data = mtcars)\n",
    "model_wt_hp   <- lm(mpg ~ wt + hp, data = mtcars)\n",
    "model_wt_hp_cyl <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n",
    "\n",
    "anova(model_wt, model_wt_hp, model_wt_hp_cyl)\n",
    "# Significant F: adding predictors significantly improves fit\n",
    "\n",
    "# ── AIC comparison (for non-nested models) ───────────────────────────────────\n",
    "AIC(model_wt, model_wt_hp, model_wt_hp_cyl)\n",
    "# Lower AIC = better fit penalized for complexity\n",
    "# ΔAIC > 2: meaningful difference; ΔAIC > 10: strong evidence\n",
    "\n",
    "# ── Adjusted R² comparison ───────────────────────────────────────────────────\n",
    "map_dfr(list(model_wt, model_wt_hp, model_wt_hp_cyl),\n",
    "        ~ broom::glance(.x) %>% select(adj.r.squared, AIC, BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11-reporting",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reporting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11-reporting-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Clean coefficient table ───────────────────────────────────────────────────\n",
    "broom::tidy(model_multi, conf.int = TRUE) %>%\n",
    "  mutate(across(where(is.numeric), ~ round(.x, 3)))\n",
    "\n",
    "# ── Model fit summary ─────────────────────────────────────────────────────────\n",
    "glance_out <- broom::glance(model_multi)\n",
    "cat(sprintf(\n",
    "  \"Multiple linear regression: F(%d, %d) = %.2f, p = %.4f, R² = %.3f, adj. R² = %.3f\\n\",\n",
    "  glance_out$df,\n",
    "  glance_out$df.residual,\n",
    "  glance_out$statistic,\n",
    "  glance_out$p.value,\n",
    "  glance_out$r.squared,\n",
    "  glance_out$adj.r.squared\n",
    "))\n",
    "\n",
    "# Standard reporting format:\n",
    "# \"Weight (β = -3.19, 95% CI [-4.22, -2.15], p < .001) and horsepower\n",
    "#  (β = -0.03, 95% CI [-0.05, -0.01], p = .002) were significant negative\n",
    "#  predictors of fuel efficiency. The model explained 84% of variance in\n",
    "#  mpg (F(3, 28) = 49.2, p < .001, adj. R² = .83).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Interpreting R² as a measure of model quality**  \n",
    "R² always increases when predictors are added, even if they add no real predictive value. Use adjusted R² for model comparison, or AIC/BIC for penalized comparison.\n",
    "\n",
    "**2. Ignoring multicollinearity**  \n",
    "Highly correlated predictors inflate standard errors and make individual coefficient estimates unstable. Always check VIF in multiple regression. If VIF > 10, consider removing one of the correlated predictors or using PCA/regularization.\n",
    "\n",
    "**3. Extrapolating beyond the range of the data**  \n",
    "A fitted line is only valid within the range of observed predictor values. Predicting far outside that range assumes linearity holds where you have no evidence for it.\n",
    "\n",
    "**4. Treating statistically significant predictors as practically important**  \n",
    "With large n, very small effects become significant. Always report and interpret standardized coefficients alongside p-values.\n",
    "\n",
    "**5. Forgetting to check for influential observations**  \n",
    "A single outlier with high leverage can dramatically shift regression coefficients. Always inspect Cook's distance. If influential points are found, fit the model with and without them and report both.\n",
    "\n",
    "**6. Using R² to compare models with different numbers of predictors**  \n",
    "Use adjusted R², AIC, or a formal F-test for nested model comparison instead.\n",
    "\n",
    "**7. Including too many predictors relative to n**  \n",
    "A rough guideline: at least 10-20 observations per predictor. Too many predictors relative to n leads to overfitting. Use regularized regression (see `regularized_regression.ipynb`) for high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "## When Linear Regression Is Not Appropriate\n",
    "\n",
    "| Situation | Better Approach |\n",
    "|---|---|\n",
    "| Binary outcome (yes/no) | Logistic regression (`logistic_regression.ipynb`) |\n",
    "| Count data (non-negative integers) | Poisson or negative binomial GLM (`glm_count_data.ipynb`) |\n",
    "| Non-linear relationship | GAMs (`04_gams/`) or polynomial regression |\n",
    "| Non-independent observations | Mixed effects models (`03_mixed_effects_models/`) |\n",
    "| Many correlated predictors | Regularized regression (`regularized_regression.ipynb`) |\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Regression Diagnostics in R\n",
    "\n",
    "## Overview\n",
    "\n",
    "Regression diagnostics evaluate whether the assumptions of a fitted model are met and whether any observations are exerting undue influence on the results. Running diagnostics is not optional — a model that violates its assumptions produces unreliable inference regardless of how significant the results look.\n",
    "\n",
    "This notebook covers diagnostics for **linear regression** in depth, with notes on how they extend (or differ) for logistic and other GLMs.\n",
    "\n",
    "| Diagnostic | What It Checks |\n",
    "|---|---|\n",
    "| Residuals vs. fitted | Linearity and homoscedasticity |\n",
    "| Normal Q-Q plot | Normality of residuals |\n",
    "| Scale-location plot | Homoscedasticity (spread of residuals) |\n",
    "| Residuals vs. leverage | Influential observations |\n",
    "| Cook's distance | Overall influence of each observation |\n",
    "| VIF | Multicollinearity among predictors |\n",
    "| Breusch-Pagan test | Formal test for heteroscedasticity |\n",
    "| Shapiro-Wilk test | Formal test for non-normality of residuals |\n",
    "\n",
    "## Applications by Sector\n",
    "\n",
    "| Sector | Example |\n",
    "|---|---|\n",
    "| **Ecology** | Are residuals from an invertebrate density model randomly distributed, or is there spatial autocorrelation? Does a single outlier site drive a significant treatment effect? |\n",
    "| **Healthcare** | Does a linear model for blood pressure have equal variance across predicted values, or does variance increase for high-risk patients? |\n",
    "| **Finance** | Are residuals from a pricing model homoscedastic, or does prediction error increase for high-value transactions? |\n",
    "| **Insurance** | Are there influential policyholders driving a claims model, and do results hold if they are excluded? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Libraries ────────────────────────────────────────────────────────────────\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(car)          # VIF, Bonferroni outlier test, ncvTest\n",
    "library(performance)  # check_model, check_heteroscedasticity\n",
    "library(lmtest)       # Breusch-Pagan test\n",
    "library(broom)        # augment() for tidy residuals\n",
    "library(ggfortify)    # autoplot() for lm diagnostic plots in ggplot2\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Fit reference models ──────────────────────────────────────────────────────\n",
    "# Linear model used throughout this notebook\n",
    "model_lm <- lm(mpg ~ wt + hp + cyl, data = mtcars)\n",
    "\n",
    "# Augmented data frame with residuals and influence measures\n",
    "mtcars_aug <- broom::augment(model_lm, data = mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-residual-plots",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Standard Diagnostic Plots\n",
    "\n",
    "Base R's `plot()` on an `lm` object returns four essential diagnostic plots. These should be the first thing you examine after fitting any linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-residual-plots-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Base R: four diagnostic plots ────────────────────────────────────────────\n",
    "par(mfrow = c(2, 2))\n",
    "plot(model_lm)\n",
    "par(mfrow = c(1, 1))\n",
    "\n",
    "# ── ggplot2 version (ggfortify) ───────────────────────────────────────────────\n",
    "ggfortify::autoplot(model_lm, which = 1:4,\n",
    "                    colour = \"#4a8fff\",\n",
    "                    smooth.colour = \"#ff6b6b\",\n",
    "                    label.size = 3) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-plot-interpretation",
   "metadata": {},
   "source": [
    "### Reading the Four Plots\n",
    "\n",
    "**Plot 1 — Residuals vs. Fitted:**\n",
    "- ✅ Random scatter around the horizontal zero line → linearity and homoscedasticity met\n",
    "- ❌ U-shaped curve → non-linearity; consider polynomial terms or GAMs\n",
    "- ❌ Fan shape (spread increases with fitted values) → heteroscedasticity\n",
    "\n",
    "**Plot 2 — Normal Q-Q:**\n",
    "- ✅ Points fall along the diagonal line → normality of residuals met\n",
    "- ❌ Heavy tails (S-shape) → heavier-than-normal tails\n",
    "- ❌ Points curve away from line → skewed residuals\n",
    "\n",
    "**Plot 3 — Scale-Location (Spread-Location):**\n",
    "- ✅ Flat red line with equally spread points → homoscedasticity met\n",
    "- ❌ Upward trend → variance increases with fitted values (heteroscedasticity)\n",
    "\n",
    "**Plot 4 — Residuals vs. Leverage:**\n",
    "- ✅ No points outside Cook's distance dashed lines → no highly influential points\n",
    "- ❌ Points beyond Cook's d = 0.5 or 1.0 → potentially influential observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-normality",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Normality of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-normality-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Shapiro-Wilk test on residuals ────────────────────────────────────────────\n",
    "shapiro.test(residuals(model_lm))\n",
    "# p > 0.05: no evidence against normality\n",
    "# Note: with large n, even minor deviations become significant\n",
    "# — rely on Q-Q plot for large samples\n",
    "\n",
    "# ── Histogram of residuals ────────────────────────────────────────────────────\n",
    "ggplot(mtcars_aug, aes(x = .resid)) +\n",
    "  geom_histogram(aes(y = after_stat(density)),\n",
    "                 bins = 10, fill = \"#4a8fff\", alpha = 0.7) +\n",
    "  geom_density(color = \"#ff6b6b\", linewidth = 1) +\n",
    "  stat_function(fun = dnorm,\n",
    "                args = list(mean = mean(mtcars_aug$.resid),\n",
    "                            sd   = sd(mtcars_aug$.resid)),\n",
    "                linetype = \"dashed\", color = \"gray40\") +\n",
    "  labs(title = \"Distribution of Residuals\",\n",
    "       subtitle = \"Dashed = normal distribution; solid = kernel density\",\n",
    "       x = \"Residual\", y = \"Density\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-heteroscedasticity",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Homoscedasticity (Equal Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-heteroscedasticity-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Breusch-Pagan test ────────────────────────────────────────────────────────\n",
    "lmtest::bptest(model_lm)\n",
    "# H0: homoscedasticity (constant variance)\n",
    "# p < 0.05: evidence of heteroscedasticity\n",
    "\n",
    "# ── Non-constant variance score test (car) ────────────────────────────────────\n",
    "car::ncvTest(model_lm)\n",
    "\n",
    "# ── Performance package wrapper ───────────────────────────────────────────────\n",
    "performance::check_heteroscedasticity(model_lm)\n",
    "\n",
    "# ── Residuals vs. fitted: manual ggplot ───────────────────────────────────────\n",
    "ggplot(mtcars_aug, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n",
    "  geom_point(color = \"#4a8fff\", alpha = 0.7) +\n",
    "  geom_smooth(method = \"loess\", se = FALSE, color = \"#ff6b6b\") +\n",
    "  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n",
    "  labs(title = \"Scale-Location Plot\",\n",
    "       subtitle = \"Flat line indicates homoscedasticity\",\n",
    "       x = \"Fitted Values\", y = \"√|Standardized Residuals|\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── If heteroscedasticity is found: options ───────────────────────────────────\n",
    "# 1. Log-transform the response: lm(log(y) ~ x)\n",
    "# 2. Robust standard errors (sandwich estimator)\n",
    "# 3. Weighted least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-multicollinearity",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07-multicollinearity-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Variance Inflation Factor (VIF) ──────────────────────────────────────────\n",
    "car::vif(model_lm)\n",
    "# VIF = 1:      no multicollinearity\n",
    "# VIF 1-5:      acceptable\n",
    "# VIF 5-10:     moderate — investigate\n",
    "# VIF > 10:     severe — model coefficients are unstable\n",
    "\n",
    "# ── Correlation matrix of predictors ─────────────────────────────────────────\n",
    "mtcars %>%\n",
    "  select(wt, hp, cyl) %>%\n",
    "  cor()\n",
    "# Correlations > |0.8| between predictors are a warning sign\n",
    "\n",
    "# ── Visual: correlation heatmap ───────────────────────────────────────────────\n",
    "mtcars %>%\n",
    "  select(wt, hp, cyl, disp, qsec) %>%\n",
    "  cor() %>%\n",
    "  as.data.frame() %>%\n",
    "  rownames_to_column(\"var1\") %>%\n",
    "  pivot_longer(-var1, names_to = \"var2\", values_to = \"r\") %>%\n",
    "  ggplot(aes(x = var1, y = var2, fill = r)) +\n",
    "  geom_tile() +\n",
    "  geom_text(aes(label = round(r, 2)), size = 3) +\n",
    "  scale_fill_gradient2(low = \"#ff6b6b\", mid = \"white\", high = \"#4a8fff\",\n",
    "                       midpoint = 0, limits = c(-1, 1)) +\n",
    "  labs(title = \"Predictor Correlation Matrix\",\n",
    "       x = NULL, y = NULL) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08-influential",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Influential Observations\n",
    "\n",
    "Three related concepts:\n",
    "- **Outlier:** observation with a large residual (unexpected response given predictors)\n",
    "- **High leverage:** observation with unusual predictor values (far from the center of the predictor space)\n",
    "- **Influential:** observation that substantially changes model coefficients when removed — requires both leverage and a large residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08-influential-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cook's distance ───────────────────────────────────────────────────────────\n",
    "# Measures how much all fitted values change when observation i is removed\n",
    "cooksd <- cooks.distance(model_lm)\n",
    "threshold_4n <- 4 / nrow(mtcars)  # rule of thumb: 4/n\n",
    "\n",
    "cat(\"Influential observations (Cook's d > 4/n):\\n\")\n",
    "print(cooksd[cooksd > threshold_4n])\n",
    "\n",
    "# ── Leverage (hat values) ─────────────────────────────────────────────────────\n",
    "# High leverage: h_ii > 2(p+1)/n where p = number of predictors\n",
    "p <- length(coef(model_lm)) - 1\n",
    "leverage_threshold <- 2 * (p + 1) / nrow(mtcars)\n",
    "hat_vals <- hatvalues(model_lm)\n",
    "cat(\"\\nHigh leverage observations (h > 2(p+1)/n):\\n\")\n",
    "print(hat_vals[hat_vals > leverage_threshold])\n",
    "\n",
    "# ── Standardized residuals ────────────────────────────────────────────────────\n",
    "std_resid <- rstandard(model_lm)\n",
    "cat(\"\\nLarge standardized residuals (|z| > 2):\\n\")\n",
    "print(std_resid[abs(std_resid) > 2])\n",
    "\n",
    "# ── Bonferroni outlier test ───────────────────────────────────────────────────\n",
    "# Tests whether the most extreme residual is a significant outlier\n",
    "# after Bonferroni correction for multiple testing\n",
    "car::outlierTest(model_lm)\n",
    "\n",
    "# ── Bubble plot: leverage vs. residual, sized by Cook's d ────────────────────\n",
    "mtcars_aug %>%\n",
    "  mutate(label = rownames(mtcars)) %>%\n",
    "  ggplot(aes(x = .hat, y = .std.resid, size = .cooksd, label = label)) +\n",
    "  geom_point(alpha = 0.6, color = \"#4a8fff\") +\n",
    "  geom_text(size = 2.5, nudge_y = 0.15, check_overlap = TRUE) +\n",
    "  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"#ff6b6b\") +\n",
    "  geom_vline(xintercept = leverage_threshold, linetype = \"dashed\", color = \"orange\") +\n",
    "  labs(title = \"Leverage vs. Standardized Residuals\",\n",
    "       subtitle = \"Size = Cook's distance; dashed lines = thresholds\",\n",
    "       x = \"Leverage (hat value)\",\n",
    "       y = \"Standardized Residual\",\n",
    "       size = \"Cook's d\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09-influential-handling",
   "metadata": {},
   "source": [
    "### Handling Influential Observations\n",
    "\n",
    "> **Do not automatically remove influential observations.** First investigate why they are influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09-influential-handling-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sensitivity analysis: fit with and without influential points ──────────────\n",
    "# Identify the most influential observation\n",
    "most_influential <- which.max(cooksd)\n",
    "cat(\"Most influential observation:\", rownames(mtcars)[most_influential], \"\\n\")\n",
    "\n",
    "# Fit model without it\n",
    "model_without <- lm(mpg ~ wt + hp + cyl,\n",
    "                    data = mtcars[-most_influential, ])\n",
    "\n",
    "# Compare coefficients\n",
    "comparison <- data.frame(\n",
    "  term      = names(coef(model_lm)),\n",
    "  full_data = round(coef(model_lm), 3),\n",
    "  removed   = round(coef(model_without), 3),\n",
    "  change_pct = round((coef(model_without) - coef(model_lm)) / abs(coef(model_lm)) * 100, 1)\n",
    ")\n",
    "print(comparison)\n",
    "# Large changes (> 20%) in key coefficients warrant investigation and reporting\n",
    "\n",
    "# Best practice:\n",
    "# 1. Report results from the full model\n",
    "# 2. Note that results are robust (or not) to removal of influential points\n",
    "# 3. Never silently remove observations without justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10-performance-check",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comprehensive Check with `performance`\n",
    "\n",
    "The `performance` package provides an integrated dashboard of all key diagnostics in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10-performance-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full model check ─────────────────────────────────────────────────────────\n",
    "performance::check_model(model_lm)\n",
    "# Generates a multi-panel diagnostic figure covering:\n",
    "# posterior predictive check, linearity, homoscedasticity,\n",
    "# influential observations, multicollinearity, normality of residuals\n",
    "\n",
    "# ── Model performance metrics ─────────────────────────────────────────────────\n",
    "performance::model_performance(model_lm)\n",
    "# Returns: AIC, BIC, R², adjusted R², RMSE, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11-glm-notes",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Diagnostics for GLMs (Logistic, Poisson, etc.)\n",
    "\n",
    "Standard residual plots are less interpretable for GLMs because residuals are not normally distributed by design. Use **DHARMa** for simulation-based residual diagnostics — see `03_mixed_effects_models/glmm_diagnostics.ipynb` for full coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11-glm-notes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── DHARMa: simulation-based residuals for GLMs ───────────────────────────────\n",
    "library(DHARMa)\n",
    "\n",
    "# Fit a GLM for illustration\n",
    "model_glm <- glm(am ~ wt + hp, data = mtcars, family = binomial)\n",
    "\n",
    "# Simulate residuals\n",
    "sim_resid <- DHARMa::simulateResiduals(model_glm, n = 1000)\n",
    "\n",
    "# Plot: uniform distribution of residuals = good fit\n",
    "plot(sim_resid)\n",
    "# Left plot: QQ plot — points on diagonal = good\n",
    "# Right plot: residuals vs. fitted — no pattern = good\n",
    "\n",
    "# Formal tests\n",
    "DHARMa::testUniformity(sim_resid)     # overall uniformity\n",
    "DHARMa::testDispersion(sim_resid)     # over/underdispersion\n",
    "DHARMa::testOutliers(sim_resid)       # outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Skipping diagnostics entirely**  \n",
    "The most common mistake. A significant p-value from a model with violated assumptions is not trustworthy. Always run diagnostics before reporting results.\n",
    "\n",
    "**2. Testing normality on raw data instead of residuals**  \n",
    "The normality assumption applies to model *residuals*, not the response variable. A skewed response variable can produce perfectly normal residuals if the model is well-specified.\n",
    "\n",
    "**3. Automatically removing influential observations**  \n",
    "Influential observations are not automatically errors. They may be the most scientifically interesting data points. Investigate first, then decide; always report both analyses if you remove any observations.\n",
    "\n",
    "**4. Confusing high leverage with influence**  \n",
    "An observation can have high leverage (unusual predictor values) without being influential if its residual is small. Influence requires both high leverage and a poor fit.\n",
    "\n",
    "**5. Using standard lm diagnostics for GLMs**  \n",
    "For logistic, Poisson, and other GLMs, standard residual plots are misleading. Use DHARMa simulation-based residuals instead.\n",
    "\n",
    "**6. Treating the 4/n Cook's distance rule as a hard threshold**  \n",
    "Rules of thumb (4/n, Cook's d > 1) are starting points for investigation, not automatic grounds for removal. Use them to flag observations for closer examination.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

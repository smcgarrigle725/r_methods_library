{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Sparse Methods: Sparse PCA and Independent Component Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "Standard PCA produces dense loadings — every variable contributes to every component. This makes components difficult to interpret when p is large. **Sparse PCA** adds an L1 penalty to loadings, forcing most to exactly zero: each component is defined by a small subset of variables.\n",
    "\n",
    "**Independent Component Analysis (ICA)** finds components that are statistically independent (not just uncorrelated). Where PCA maximises variance, ICA maximises non-Gaussianity — recovering the original source signals from mixed observations.\n",
    "\n",
    "**When to use each:**\n",
    "\n",
    "| Method | Use when |\n",
    "|---|---|\n",
    "| **Standard PCA** | Dimensionality reduction; visualisation; p moderately large |\n",
    "| **Sparse PCA** | p >> n; interpretability required; variable selection via loadings |\n",
    "| **ICA** | Mixed signals with non-Gaussian sources; signal separation; fMRI, EEG, remote sensing |\n",
    "\n",
    "**Sparse PCA vs. LASSO:** LASSO applies sparsity to regression coefficients; sparse PCA applies sparsity to PCA loadings. Both use L1 regularisation but solve different optimisation problems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(sparsepca)     # spca() — sparse PCA via elastic net\n",
    "library(PMA)           # SPC() — sparse PCA via LASSO (Witten et al.)\n",
    "library(fastICA)       # fastICA()\n",
    "library(factoextra)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── High-dimensional dataset: 200 sites, 40 environmental variables ───────────\n",
    "# Only 6 variables carry real signal; the rest are noise\n",
    "n  <- 200\n",
    "p  <- 40\n",
    "\n",
    "# Two latent factors\n",
    "f1 <- rnorm(n)  # nutrient gradient: drives vars 1-6\n",
    "f2 <- rnorm(n)  # hydrological stress: drives vars 7-12\n",
    "\n",
    "X_signal <- matrix(0, n, p)\n",
    "X_signal[, 1:6]  <- outer(f1, c(0.9, 0.85, 0.80, 0.75, 0.70, 0.65))\n",
    "X_signal[, 7:12] <- outer(f2, c(0.9, 0.85, 0.80, 0.75, 0.70, 0.65))\n",
    "X_noise  <- matrix(rnorm(n * p, 0, 0.8), n, p)\n",
    "X_raw    <- X_signal + X_noise\n",
    "X_sc     <- scale(X_raw)\n",
    "\n",
    "colnames(X_sc) <- c(paste0(\"nutrient_\", 1:6),\n",
    "                    paste0(\"hydro_\", 1:6),\n",
    "                    paste0(\"noise_\", 1:28))\n",
    "cat(sprintf(\"%d observations × %d features (%d signal + %d noise)\\n\",\n",
    "            n, p, 12, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-compare-loadings",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Standard PCA vs. Sparse PCA: Loadings Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-loadings-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standard PCA ──────────────────────────────────────────────────────────────\n",
    "pca_fit <- prcomp(X_sc, center=FALSE)\n",
    "\n",
    "# ── Sparse PCA (sparsepca::spca) ─────────────────────────────────────────────\n",
    "# alpha: L1 sparsity penalty (larger = more zeros)\n",
    "# beta:  L2 ridge penalty (stabilises when p > n)\n",
    "# k:     number of components\n",
    "spca_fit <- sparsepca::spca(\n",
    "  X_sc,\n",
    "  k     = 2,\n",
    "  alpha = 1e-3,   # start small; tune to get interpretable sparsity\n",
    "  beta  = 1e-4,\n",
    "  center = FALSE,\n",
    "  scale  = FALSE,\n",
    "  verbose = FALSE\n",
    ")\n",
    "\n",
    "# Compare: which variables load on PC1?\n",
    "loading_comparison <- tibble(\n",
    "  variable   = colnames(X_sc),\n",
    "  pca_pc1    = abs(pca_fit$rotation[, 1]),\n",
    "  spca_pc1   = abs(spca_fit$loadings[, 1]),\n",
    "  is_signal  = str_starts(variable, \"nutrient\") | str_starts(variable, \"hydro\")\n",
    ")\n",
    "\n",
    "p_dense <- ggplot(\n",
    "  loading_comparison %>% arrange(desc(pca_pc1)) %>%\n",
    "    mutate(variable=fct_inorder(variable)),\n",
    "  aes(x=variable, y=pca_pc1, fill=is_signal)\n",
    ") +\n",
    "  geom_col() +\n",
    "  scale_fill_manual(values=c(\"TRUE\"=\"#4a8fff\",\"FALSE\"=\"#cccccc\"),\n",
    "                    labels=c(\"TRUE\"=\"Signal\",\"FALSE\"=\"Noise\")) +\n",
    "  labs(title=\"Standard PCA PC1\", subtitle=\"Dense: all 40 variables contribute\",\n",
    "       x=NULL, y=\"|Loading|\", fill=NULL) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x=element_blank())\n",
    "\n",
    "p_sparse <- ggplot(\n",
    "  loading_comparison %>% arrange(desc(spca_pc1)) %>%\n",
    "    mutate(variable=fct_inorder(variable)),\n",
    "  aes(x=variable, y=spca_pc1, fill=is_signal)\n",
    ") +\n",
    "  geom_col() +\n",
    "  scale_fill_manual(values=c(\"TRUE\"=\"#4a8fff\",\"FALSE\"=\"#cccccc\"),\n",
    "                    labels=c(\"TRUE\"=\"Signal\",\"FALSE\"=\"Noise\")) +\n",
    "  labs(title=\"Sparse PCA PC1\", subtitle=\"Sparse: most noise variables shrunk to zero\",\n",
    "       x=NULL, y=\"|Loading|\", fill=NULL) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x=element_blank())\n",
    "\n",
    "(p_dense / p_sparse)\n",
    "\n",
    "cat(sprintf(\"\\nStandard PCA PC1: %d non-zero loadings\\n\",\n",
    "            sum(abs(pca_fit$rotation[,1]) > 1e-10)))\n",
    "cat(sprintf(\"Sparse PCA PC1:   %d non-zero loadings\\n\",\n",
    "            sum(abs(spca_fit$loadings[,1]) > 1e-10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-sparsity-tuning",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tuning Sparsity: Choosing Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-alpha-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade-off: more sparsity (larger alpha) → fewer variables, less variance explained\n",
    "alpha_grid <- c(1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2)\n",
    "\n",
    "alpha_results <- map_dfr(alpha_grid, function(a) {\n",
    "  fit <- sparsepca::spca(X_sc, k=2, alpha=a, beta=1e-4,\n",
    "                          center=FALSE, scale=FALSE, verbose=FALSE)\n",
    "  tibble(\n",
    "    alpha       = a,\n",
    "    nonzero_pc1 = sum(abs(fit$loadings[,1]) > 1e-10),\n",
    "    nonzero_pc2 = sum(abs(fit$loadings[,2]) > 1e-10),\n",
    "    pct_var_pc1 = fit$R2[1] * 100\n",
    "  )\n",
    "})\n",
    "\n",
    "print(alpha_results)\n",
    "\n",
    "p_nonzero <- ggplot(alpha_results, aes(x=log10(alpha), y=nonzero_pc1)) +\n",
    "  geom_line(color=\"#4a8fff\", linewidth=1) +\n",
    "  geom_point(size=3, color=\"#4a8fff\") +\n",
    "  geom_hline(yintercept=6, linetype=\"dashed\", color=\"#ff6b6b\") +\n",
    "  annotate(\"text\", x=min(log10(alpha_grid))+0.2, y=7,\n",
    "           label=\"True signal variables = 6\", color=\"#ff6b6b\", hjust=0) +\n",
    "  labs(title=\"Sparsity Tuning: Non-Zero Loadings vs. Alpha\",\n",
    "       x=\"log10(alpha)\", y=\"Non-zero loadings on PC1\") +\n",
    "  theme_minimal()\n",
    "\n",
    "p_var <- ggplot(alpha_results, aes(x=log10(alpha), y=pct_var_pc1)) +\n",
    "  geom_line(color=\"#ff6b6b\", linewidth=1) +\n",
    "  geom_point(size=3, color=\"#ff6b6b\") +\n",
    "  labs(title=\"Variance Explained vs. Alpha\",\n",
    "       x=\"log10(alpha)\", y=\"Variance explained by PC1 (%)\") +\n",
    "  theme_minimal()\n",
    "\n",
    "(p_nonzero | p_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-ica",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-ica-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA assumes observed variables are linear mixtures of independent sources\n",
    "# It recovers statistically independent (non-Gaussian) components\n",
    "# Unlike PCA components (uncorrelated), ICA components are independent\n",
    "\n",
    "# Simulate: two clearly non-Gaussian sources\n",
    "n_ica  <- 500\n",
    "s1 <- runif(n_ica, -1, 1)       # uniform distribution\n",
    "s2 <- sign(rnorm(n_ica)) * rexp(n_ica, 1)  # Laplace-like (super-Gaussian)\n",
    "S  <- cbind(s1, s2)\n",
    "\n",
    "# Mixing matrix\n",
    "A  <- matrix(c(0.8, 0.4, 0.3, 0.9), 2, 2)\n",
    "X_mixed <- t(A %*% t(S))  # observed mixtures\n",
    "\n",
    "# ICA: recover independent sources from mixtures\n",
    "ica_fit <- fastICA::fastICA(\n",
    "  X_mixed,\n",
    "  n.comp  = 2,\n",
    "  alg.typ = \"parallel\",\n",
    "  fun     = \"logcosh\",   # contrast function; alternatives: 'exp', 'kurtosis'\n",
    "  verbose = FALSE\n",
    ")\n",
    "\n",
    "# Compare original sources to recovered components\n",
    "source_df <- tibble(\n",
    "  s1_true = S[,1], s2_true = S[,2],\n",
    "  ic1     = ica_fit$S[,1], ic2 = ica_fit$S[,2],\n",
    "  obs1    = X_mixed[,1],   obs2 = X_mixed[,2]\n",
    ")\n",
    "\n",
    "p_mixed <- ggplot(source_df, aes(x=obs1, y=obs2)) +\n",
    "  geom_point(alpha=0.2, size=0.8, color=\"#cccccc\") +\n",
    "  labs(title=\"Observed Mixtures\", x=\"Observed 1\", y=\"Observed 2\") +\n",
    "  theme_minimal()\n",
    "\n",
    "p_sources <- ggplot(source_df, aes(x=s1_true, y=s2_true)) +\n",
    "  geom_point(alpha=0.2, size=0.8, color=\"#4a8fff\") +\n",
    "  labs(title=\"True Sources\", x=\"Source 1 (uniform)\", y=\"Source 2 (Laplace)\") +\n",
    "  theme_minimal()\n",
    "\n",
    "p_recovered <- ggplot(source_df, aes(x=ic1, y=ic2)) +\n",
    "  geom_point(alpha=0.2, size=0.8, color=\"#4fffb0\") +\n",
    "  labs(title=\"ICA-Recovered Components\",\n",
    "       subtitle=\"Should resemble true sources (up to sign and scale)\",\n",
    "       x=\"IC 1\", y=\"IC 2\") +\n",
    "  theme_minimal()\n",
    "\n",
    "(p_mixed | p_sources | p_recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Choosing sparsity penalty alpha without examining the sparsity–variance trade-off**  \n",
    "There is no automatic rule for choosing alpha in sparse PCA. Too small and loadings remain dense; too large and all variance is eliminated. Always plot non-zero loadings and variance explained against a range of alpha values, and select the value where the curve elbows — fewest variables needed to explain acceptable variance.\n",
    "\n",
    "**2. Treating sparse PCA components as if variance-explained statistics from standard PCA apply**  \n",
    "Sparse components are not orthogonal and do not partition variance cleanly. The variance explained by sparse components is not additive in the same way as PCA. Use the R² reported by `spca()` per component as an approximation, but do not sum them without accounting for overlap.\n",
    "\n",
    "**3. Applying ICA to data with Gaussian sources**  \n",
    "ICA identifies components by maximising non-Gaussianity. If the true sources are Gaussian, they are inherently unidentifiable by ICA — any rotation of the PCA solution is equally valid. ICA only provides a unique solution when sources are non-Gaussian. Check the marginal distributions of ICA components after fitting.\n",
    "\n",
    "**4. Ignoring ICA sign and permutation ambiguity**  \n",
    "ICA components are identified only up to sign (±) and ordering. The signs of ICA components are arbitrary — IC1 in one run may appear as −IC1 in another. Never interpret the sign of an ICA component without anchoring it to a reference variable or running multiple initialisations and aligning components.\n",
    "\n",
    "**5. Using sparse PCA when n < p without ridge regularisation**  \n",
    "When observations are fewer than variables, standard and sparse PCA can be numerically unstable. Always add a small ridge penalty (`beta > 0` in `spca()`) when p ≥ n to stabilise the solution. The ridge penalty shrinks loadings toward zero without enforcing exact sparsity — it complements the L1 sparsity penalty.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Confidence Intervals, Effect Sizes & Power Analysis in R\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers three closely related concepts that together provide a complete picture of statistical results — going beyond the binary signal of a p-value.\n",
    "\n",
    "| Concept | What It Tells You |\n",
    "|---|---|\n",
    "| **Confidence interval (CI)** | The plausible range for the true parameter value given the data |\n",
    "| **Effect size** | The magnitude of a difference or association, independent of sample size |\n",
    "| **Power analysis** | Whether a study has adequate sample size to detect an effect of a given size |\n",
    "\n",
    "> A p-value answers \"is there an effect?\". An effect size answers \"how big is it?\". A confidence interval answers \"what is our best estimate of its size?\". Power analysis answers \"could we have detected it if it existed?\". All four are needed to fully evaluate a result.\n",
    "\n",
    "## Applications by Sector\n",
    "\n",
    "| Sector | Example |\n",
    "|---|---|\n",
    "| **Ecology** | What is the estimated difference in invertebrate density between acidified and reference sediments, and how precisely is it estimated? Was the field survey adequately powered to detect a biologically meaningful difference? |\n",
    "| **Healthcare** | What is the effect size for a new treatment vs. standard care? How many patients are needed in a clinical trial to detect a clinically meaningful improvement? |\n",
    "| **Finance** | What is the 95% CI for mean portfolio return? How large a sample is needed to detect a 2% lift in conversion rate from a product change? |\n",
    "| **Insurance** | What is the estimated reduction in mean claim amount under a new policy, and is it practically meaningful? How many policyholders are needed to detect a 5% change in claim rate? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-assumptions",
   "metadata": {},
   "source": [
    "## Assumptions Checklist\n",
    "\n",
    "**Confidence intervals (parametric):**\n",
    "- [ ] Data are approximately normally distributed (or n is large enough for CLT)\n",
    "- [ ] Observations are independent\n",
    "- [ ] The appropriate standard error formula is used for the parameter of interest\n",
    "\n",
    "**Effect sizes:**\n",
    "- [ ] The chosen effect size measure is appropriate for the test used (see table below)\n",
    "- [ ] Effect sizes are interpreted in context — a \"small\" effect may be practically important or unimportant depending on the domain\n",
    "\n",
    "**Power analysis:**\n",
    "- [ ] Effect size is specified a priori — from prior literature, pilot data, or a minimum meaningful difference\n",
    "- [ ] Alpha level (type I error rate) is specified (typically 0.05)\n",
    "- [ ] Desired power is specified (typically 0.80 or 0.90)\n",
    "- [ ] The test type matches the planned analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Libraries ────────────────────────────────────────────────────────────────\n",
    "library(tidyverse)    # data manipulation and visualization\n",
    "library(effectsize)   # Cohen's d, eta-squared, omega-squared, Cramér's V\n",
    "library(pwr)          # power analysis for common tests\n",
    "library(ggplot2)      # visualization\n",
    "library(boot)         # bootstrap confidence intervals\n",
    "\n",
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "set.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-ci-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Confidence Intervals\n",
    "\n",
    "A 95% confidence interval means: if we repeated the study many times and computed a CI each time, 95% of those intervals would contain the true parameter value. It does **not** mean there is a 95% probability the true value lies in *this particular* interval.\n",
    "\n",
    "### CI for a Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-ci-mean-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CI for a single mean (from t-test output) ─────────────────────────────────\n",
    "result <- t.test(iris$Sepal.Length)\n",
    "cat(sprintf(\"Mean: %.2f, 95%% CI [%.2f, %.2f]\\n\",\n",
    "            result$estimate,\n",
    "            result$conf.int[1],\n",
    "            result$conf.int[2]))\n",
    "\n",
    "# ── Manual calculation ────────────────────────────────────────────────────────\n",
    "# CI = x̄ ± t(α/2, df) * (s / √n)\n",
    "n    <- length(iris$Sepal.Length)\n",
    "xbar <- mean(iris$Sepal.Length)\n",
    "se   <- sd(iris$Sepal.Length) / sqrt(n)\n",
    "t_crit <- qt(0.975, df = n - 1)  # two-tailed, alpha = 0.05\n",
    "\n",
    "cat(sprintf(\"Manual CI: [%.2f, %.2f]\\n\",\n",
    "            xbar - t_crit * se,\n",
    "            xbar + t_crit * se))\n",
    "\n",
    "# ── CIs for group means ───────────────────────────────────────────────────────\n",
    "iris %>%\n",
    "  group_by(Species) %>%\n",
    "  summarise(\n",
    "    n    = n(),\n",
    "    mean = mean(Sepal.Length),\n",
    "    se   = sd(Sepal.Length) / sqrt(n),\n",
    "    lower_95 = mean - qt(0.975, df = n-1) * se,\n",
    "    upper_95 = mean + qt(0.975, df = n-1) * se\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-ci-plot",
   "metadata": {},
   "source": [
    "### Visualizing Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-ci-plot-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Mean ± 95% CI plot ────────────────────────────────────────────────────────\n",
    "ci_df <- iris %>%\n",
    "  group_by(Species) %>%\n",
    "  summarise(\n",
    "    mean  = mean(Sepal.Length),\n",
    "    se    = sd(Sepal.Length) / sqrt(n()),\n",
    "    lower = mean - qt(0.975, df = n()-1) * se,\n",
    "    upper = mean + qt(0.975, df = n()-1) * se\n",
    "  )\n",
    "\n",
    "ggplot(ci_df, aes(x = Species, y = mean, color = Species)) +\n",
    "  geom_point(size = 4) +\n",
    "  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15, linewidth = 1) +\n",
    "  scale_color_manual(values = c(\"#4a8fff\", \"#4fffb0\", \"#ffd166\")) +\n",
    "  labs(title = \"Mean Sepal Length by Species\",\n",
    "       subtitle = \"Points = mean, bars = 95% confidence interval\",\n",
    "       y = \"Sepal Length (cm)\", x = \"Species\") +\n",
    "  theme_minimal() +\n",
    "  theme(legend.position = \"none\")\n",
    "# Non-overlapping CIs suggest significant differences;\n",
    "# overlapping CIs do NOT guarantee non-significance — run the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-bootstrap-ci",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals\n",
    "\n",
    "Use bootstrap CIs when the normality assumption is not met or when you need CIs for statistics without closed-form standard errors (medians, correlations, custom metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-bootstrap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bootstrap CI for the median ───────────────────────────────────────────────\n",
    "median_boot <- function(data, indices) median(data[indices])\n",
    "\n",
    "boot_result <- boot::boot(\n",
    "  data      = iris$Sepal.Length,\n",
    "  statistic = median_boot,\n",
    "  R         = 2000  # number of bootstrap resamples\n",
    ")\n",
    "\n",
    "boot::boot.ci(boot_result, type = \"perc\")  # percentile CI\n",
    "# 'bca' (bias-corrected accelerated) is generally preferred when n > 30\n",
    "boot::boot.ci(boot_result, type = \"bca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-effect-sizes-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Effect Sizes\n",
    "\n",
    "Effect sizes quantify the practical magnitude of a result. They are independent of sample size, making them comparable across studies.\n",
    "\n",
    "| Test | Effect Size | Measure | Small | Medium | Large |\n",
    "|---|---|---|---|---|---|\n",
    "| t-test | Cohen's *d* | Standardized mean difference | 0.2 | 0.5 | 0.8 |\n",
    "| ANOVA | Eta-squared η² | Proportion of variance explained | 0.01 | 0.06 | 0.14 |\n",
    "| ANOVA | Omega-squared ω² | Less biased version of η² | 0.01 | 0.06 | 0.14 |\n",
    "| Chi-square | Cramér's *V* | Association strength (0–1) | 0.1 | 0.3 | 0.5 |\n",
    "| Correlation | *r* | Pearson/Spearman correlation | 0.1 | 0.3 | 0.5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07-effect-sizes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cohen's d: two-sample comparison ─────────────────────────────────────────\n",
    "iris_two <- iris %>% filter(Species %in% c(\"setosa\", \"versicolor\"))\n",
    "effectsize::cohens_d(Sepal.Length ~ Species, data = iris_two)\n",
    "# Positive d: first group has larger mean; negative: second group larger\n",
    "# Report with sign and 95% CI\n",
    "\n",
    "# ── Cohen's d: one-sample ─────────────────────────────────────────────────────\n",
    "effectsize::cohens_d(iris$Sepal.Length, mu = 5.0)  # vs. hypothesized mean of 5\n",
    "\n",
    "# ── Eta-squared: ANOVA ────────────────────────────────────────────────────────\n",
    "model <- aov(Sepal.Length ~ Species, data = iris)\n",
    "effectsize::eta_squared(model)         # total eta-squared\n",
    "effectsize::eta_squared(model, partial = TRUE)   # partial (same for one-way)\n",
    "effectsize::omega_squared(model)       # omega-squared (preferred for small n)\n",
    "\n",
    "# ── Cramér's V: chi-square ────────────────────────────────────────────────────\n",
    "hair_eye <- margin.table(HairEyeColor, margin = c(1,2))\n",
    "effectsize::cramers_v(hair_eye)\n",
    "\n",
    "# ── Correlation effect size ───────────────────────────────────────────────────\n",
    "cor.test(iris$Sepal.Length, iris$Petal.Length)\n",
    "# r is already the effect size; extract with $estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08-power-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Power Analysis\n",
    "\n",
    "Statistical power is the probability of correctly rejecting H₀ when it is false (i.e., detecting an effect that truly exists). Four quantities are related — specify any three to solve for the fourth:\n",
    "\n",
    "- **n** — sample size per group\n",
    "- **d** (or f, w) — effect size\n",
    "- **α** — significance level (type I error rate, typically 0.05)\n",
    "- **power** — target power (typically 0.80 or 0.90)\n",
    "\n",
    "> Always conduct power analysis *before* data collection. Post-hoc power analysis on a non-significant result is not meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08-power-ttest-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Required n for a two-sample t-test ────────────────────────────────────────\n",
    "# Detect a medium effect (d = 0.5) at alpha = 0.05 with 80% power\n",
    "pwr::pwr.t.test(\n",
    "  d           = 0.5,    # Cohen's d\n",
    "  sig.level   = 0.05,\n",
    "  power       = 0.80,\n",
    "  type        = \"two.sample\",\n",
    "  alternative = \"two.sided\"\n",
    ")\n",
    "# n = required sample size PER GROUP\n",
    "\n",
    "# ── Power given a fixed n ─────────────────────────────────────────────────────\n",
    "pwr::pwr.t.test(\n",
    "  n           = 30,     # available n per group\n",
    "  d           = 0.5,\n",
    "  sig.level   = 0.05,\n",
    "  type        = \"two.sample\",\n",
    "  alternative = \"two.sided\"\n",
    ")\n",
    "\n",
    "# ── Minimum detectable effect given n and power ───────────────────────────────\n",
    "pwr::pwr.t.test(\n",
    "  n           = 30,\n",
    "  sig.level   = 0.05,\n",
    "  power       = 0.80,\n",
    "  type        = \"two.sample\",\n",
    "  alternative = \"two.sided\"\n",
    ")\n",
    "# d returned = smallest effect detectable with this design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09-power-other-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Power for one-way ANOVA ───────────────────────────────────────────────────\n",
    "# f = sqrt(eta² / (1 - eta²)); small = 0.10, medium = 0.25, large = 0.40\n",
    "pwr::pwr.anova.test(\n",
    "  k           = 3,      # number of groups\n",
    "  f           = 0.25,   # medium effect\n",
    "  sig.level   = 0.05,\n",
    "  power       = 0.80\n",
    ")\n",
    "\n",
    "# ── Power for chi-square test ─────────────────────────────────────────────────\n",
    "# w = Cramér's V (for 2x2); small = 0.1, medium = 0.3, large = 0.5\n",
    "pwr::pwr.chisq.test(\n",
    "  w           = 0.3,    # medium effect\n",
    "  df          = 1,      # (rows-1) * (cols-1)\n",
    "  sig.level   = 0.05,\n",
    "  power       = 0.80\n",
    ")\n",
    "\n",
    "# ── Power for correlation ─────────────────────────────────────────────────────\n",
    "pwr::pwr.r.test(\n",
    "  r           = 0.3,    # medium correlation\n",
    "  sig.level   = 0.05,\n",
    "  power       = 0.80,\n",
    "  alternative = \"two.sided\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10-power-curve-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Power curve: power as a function of n ─────────────────────────────────────\n",
    "n_seq    <- seq(10, 150, by = 5)\n",
    "power_df <- map_dfr(c(0.2, 0.5, 0.8), function(d) {\n",
    "  tibble(\n",
    "    n     = n_seq,\n",
    "    power = map_dbl(n_seq, ~ pwr::pwr.t.test(\n",
    "                                n = .x, d = d,\n",
    "                                sig.level = 0.05,\n",
    "                                type = \"two.sample\")$power),\n",
    "    effect = paste0(\"d = \", d)\n",
    "  )\n",
    "})\n",
    "\n",
    "ggplot(power_df, aes(x = n, y = power, color = effect)) +\n",
    "  geom_line(linewidth = 1) +\n",
    "  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray50\") +\n",
    "  scale_color_manual(values = c(\"#4a8fff\", \"#4fffb0\", \"#ffd166\")) +\n",
    "  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +\n",
    "  labs(title = \"Power Curves for Two-Sample t-Test\",\n",
    "       subtitle = \"Dashed line = 80% power threshold\",\n",
    "       x = \"Sample Size per Group (n)\",\n",
    "       y = \"Statistical Power\",\n",
    "       color = \"Effect Size\") +\n",
    "  theme_minimal()\n",
    "# Use this to communicate sample size requirements to collaborators or stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Reporting p-values without effect sizes**  \n",
    "Statistical significance depends on sample size — with large n, trivially small effects become significant. Always pair p-values with an effect size and CI.\n",
    "\n",
    "**2. Interpreting overlapping CIs as non-significant**  \n",
    "Two 95% CIs can overlap and the difference can still be statistically significant. Overlapping CIs roughly correspond to p < 0.05 only for 84% CIs, not 95% CIs. Run the test.\n",
    "\n",
    "**3. Using Cohen's benchmarks without domain context**  \n",
    "A \"small\" effect in psychology (d = 0.2) might be clinically meaningful in medicine or economically meaningful in finance. Define what constitutes a meaningful effect in your domain before collecting data.\n",
    "\n",
    "**4. Post-hoc power analysis on non-significant results**  \n",
    "Calculating power after observing a non-significant result and then concluding \"we were underpowered\" is circular. Power analysis should always be prospective.\n",
    "\n",
    "**5. Forgetting that power analysis gives n per group**  \n",
    "`pwr.t.test()` returns the required sample size per group for a two-sample test. Total n = n × number of groups.\n",
    "\n",
    "**6. Treating confidence intervals as Bayesian credible intervals**  \n",
    "A 95% CI does not mean \"there is a 95% probability the true value is in this range.\" It is a frequentist statement about the long-run coverage of the procedure.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}

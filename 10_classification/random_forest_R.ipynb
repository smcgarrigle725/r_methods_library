{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Random Forest Classification with `ranger`\n",
    "\n",
    "## Overview\n",
    "\n",
    "Random forests build an ensemble of decision trees, each trained on a bootstrap sample and a random subset of features. Predictions are aggregated by majority vote. The combination of bootstrap aggregation (bagging) and random feature selection reduces variance dramatically compared to single trees.\n",
    "\n",
    "**Key advantages:**\n",
    "- Strong predictive performance with minimal tuning\n",
    "- Out-of-bag (OOB) error estimate is a nearly unbiased held-out error estimate — no separate validation set needed during training\n",
    "- Permutation-based variable importance is unbiased for mixed variable types\n",
    "- Handles missing data, mixed types, and high-dimensional inputs well\n",
    "\n",
    "**`ranger` vs `randomForest`:** `ranger` is 2–10× faster, supports multithreading, and produces unbiased permutation importance. Prefer it for any non-trivial dataset.\n",
    "\n",
    "**Key hyperparameters:**\n",
    "\n",
    "| Parameter | Default | What it controls |\n",
    "|---|---|---|\n",
    "| `num.trees` | 500 | More trees = lower variance; 500–2000 typical |\n",
    "| `mtry` | √p (classification) | Features considered per split; main tuning knob |\n",
    "| `min.node.size` | 1 | Minimum terminal node size; increase to regularise |\n",
    "| `max.depth` | unlimited | Tree depth; NULL = fully grown |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(ranger)       # fast random forests\n",
    "library(vip)          # variable importance plots\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "n <- 600\n",
    "rf_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  slope_pct   = abs(rnorm(n, 10, 5)),\n",
    "  habitat     = factor(sample(c(\"reference\",\"restored\",\"degraded\"), n,\n",
    "                              replace=TRUE, prob=c(.35,.35,.30)),\n",
    "                       levels=c(\"reference\",\"restored\",\"degraded\")),\n",
    "  noise1      = rnorm(n),   # irrelevant features\n",
    "  noise2      = rnorm(n),\n",
    "  log_odds    = -1 + 0.6*water_qual - 0.7*nitrate - 0.3*distance_km +\n",
    "                0.01*elevation +\n",
    "                case_when(habitat==\"reference\"~0.8, habitat==\"restored\"~0.2,\n",
    "                          habitat==\"degraded\"~ -0.9),\n",
    "  present     = factor(rbinom(n, 1, plogis(log_odds)),\n",
    "                       levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "train_idx  <- sample(n, 450)\n",
    "train_data <- rf_data[train_idx,]\n",
    "test_data  <- rf_data[-train_idx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-fit",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fit a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-fit-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation +\n",
    "             slope_pct + habitat + noise1 + noise2,\n",
    "  data             = train_data,\n",
    "  num.trees        = 1000,\n",
    "  mtry             = 3,              # ~√8 predictors; tune below\n",
    "  min.node.size    = 5,              # classification default = 1; 5 adds regularisation\n",
    "  importance       = \"permutation\",  # unbiased; also: \"impurity\", \"impurity_corrected\"\n",
    "  probability      = TRUE,           # return probability predictions (not class)\n",
    "  num.threads      = 4,\n",
    "  seed             = 42\n",
    ")\n",
    "\n",
    "cat(sprintf(\"OOB prediction error: %.4f\\n\", rf_fit$prediction.error))\n",
    "# OOB error is a nearly unbiased estimate of test error\n",
    "# For probability forests: OOB Brier score (lower = better)\n",
    "# For classification forests (probability=FALSE): OOB misclassification rate\n",
    "\n",
    "print(rf_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-tune",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tuning `mtry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-tune-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtry is the most important hyperparameter for random forests\n",
    "# Tune by OOB error over a grid (no cross-validation needed)\n",
    "\n",
    "p_features <- 8   # number of predictors\n",
    "mtry_grid  <- unique(c(1, floor(sqrt(p_features)), floor(p_features/3),\n",
    "                        floor(p_features/2), p_features))\n",
    "\n",
    "mtry_results <- map_dfr(mtry_grid, function(m) {\n",
    "  rf <- ranger(\n",
    "    present ~ nitrate + water_qual + distance_km + elevation +\n",
    "               slope_pct + habitat + noise1 + noise2,\n",
    "    data=train_data, num.trees=500, mtry=m,\n",
    "    min.node.size=5, probability=TRUE,\n",
    "    num.threads=4, seed=42\n",
    "  )\n",
    "  tibble(mtry=m, oob_brier=rf$prediction.error)\n",
    "})\n",
    "\n",
    "print(mtry_results)\n",
    "best_mtry <- mtry_results %>% slice_min(oob_brier) %>% pull(mtry)\n",
    "cat(sprintf(\"Best mtry: %d\\n\", best_mtry))\n",
    "\n",
    "ggplot(mtry_results, aes(x=factor(mtry), y=oob_brier, group=1)) +\n",
    "  geom_line(color=\"#4a8fff\", linewidth=1) +\n",
    "  geom_point(size=3, color=\"#4a8fff\") +\n",
    "  geom_point(data=~filter(.x, mtry==best_mtry),\n",
    "             color=\"#ff6b6b\", size=5) +\n",
    "  labs(title=\"mtry Tuning via OOB Brier Score\",\n",
    "       subtitle=\"Red = selected value\",\n",
    "       x=\"mtry\", y=\"OOB Brier score\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-vi",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-vi-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final model with best mtry\n",
    "rf_final <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation +\n",
    "             slope_pct + habitat + noise1 + noise2,\n",
    "  data=train_data, num.trees=1000, mtry=best_mtry,\n",
    "  min.node.size=5, importance=\"permutation\",\n",
    "  probability=TRUE, num.threads=4, seed=42\n",
    ")\n",
    "\n",
    "# ── Permutation importance ────────────────────────────────────────────────────\n",
    "# Mean decrease in OOB accuracy when each variable is randomly permuted\n",
    "# Positive: variable is useful; negative: variable adds noise\n",
    "vi_df <- tibble(\n",
    "  variable   = names(rf_final$variable.importance),\n",
    "  importance = rf_final$variable.importance\n",
    ") %>% arrange(importance)\n",
    "\n",
    "ggplot(vi_df, aes(x=importance,\n",
    "                  y=fct_reorder(variable, importance),\n",
    "                  fill=importance > 0)) +\n",
    "  geom_col(alpha=0.85) +\n",
    "  scale_fill_manual(values=c(\"TRUE\"=\"#4a8fff\", \"FALSE\"=\"#ff6b6b\"),\n",
    "                    guide=\"none\") +\n",
    "  geom_vline(xintercept=0, color=\"gray30\") +\n",
    "  labs(title=\"Random Forest: Permutation Variable Importance\",\n",
    "       subtitle=\"Blue = useful; red = harmful (≈ noise). noise1/noise2 should be near zero.\",\n",
    "       x=\"Mean decrease in OOB accuracy when permuted\", y=NULL) +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── Test set evaluation ───────────────────────────────────────────────────────\n",
    "test_preds <- test_data %>%\n",
    "  mutate(\n",
    "    prob_present = predict(rf_final, data=test_data)$predictions[,\"present\"],\n",
    "    pred_class   = factor(ifelse(prob_present >= 0.5, \"present\",\"absent\"),\n",
    "                          levels=c(\"absent\",\"present\"))\n",
    "  )\n",
    "\n",
    "yardstick::metrics(test_preds, truth=present, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(test_preds, truth=present, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Using impurity importance instead of permutation importance**  \n",
    "Impurity importance (`importance=\"impurity\"`) is biased toward variables with many possible split points (continuous variables over binary ones). Use `importance=\"permutation\"` for unbiased ranking, especially with mixed variable types.\n",
    "\n",
    "**2. Not checking that `num.trees` is large enough for OOB error to stabilise**  \n",
    "OOB error is noisy with too few trees. Plot OOB error vs. number of trees with `ranger` by fitting with `keep.inbag=TRUE` or iteratively. For most problems, 500–1000 trees is sufficient; 2000+ rarely helps.\n",
    "\n",
    "**3. Treating OOB error as identical to test set error**  \n",
    "OOB error is nearly unbiased but is estimated on individual tree predictions, not the full ensemble. It tends to slightly overestimate the ensemble test error. Always confirm on a held-out test set before reporting.\n",
    "\n",
    "**4. Not tuning `mtry`**  \n",
    "The default `mtry = √p` is a good starting point but not always optimal. For datasets with many correlated features or where most features are noise, a smaller `mtry` often performs better. Tune over a 5-point grid using OOB Brier score.\n",
    "\n",
    "**5. Using random forests for formal statistical inference**  \n",
    "Random forests do not produce p-values or confidence intervals for variable importance. Permutation importance is a predictive measure, not an inferential one. For inference on individual predictors, use regression models. For exploratory variable screening, random forests are appropriate.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Gradient Boosting: XGBoost and LightGBM\n",
    "\n",
    "## Overview\n",
    "\n",
    "Gradient boosting builds an ensemble sequentially — each new tree corrects the residual errors of the previous ones. It is consistently one of the highest-performing methods on structured tabular data.\n",
    "\n",
    "**XGBoost vs. LightGBM:**\n",
    "\n",
    "| Feature | XGBoost | LightGBM |\n",
    "|---|---|---|\n",
    "| Tree growth | Level-wise | Leaf-wise (faster, may overfit on small data) |\n",
    "| Speed | Fast | Very fast (10–20× on large datasets) |\n",
    "| Memory | Moderate | Low (histogram-based binning) |\n",
    "| Categorical features | Requires encoding | Native support |\n",
    "| Typical sweet spot | n < 100k | n > 100k |\n",
    "| R package | `xgboost` | `lightgbm` |\n",
    "\n",
    "**In practice:** Start with XGBoost on smaller datasets (< 100k rows). Move to LightGBM if training time becomes a bottleneck. Both are tuned similarly.\n",
    "\n",
    "**Key hyperparameters:**\n",
    "\n",
    "| Parameter | XGBoost | LightGBM | Controls |\n",
    "|---|---|---|---|\n",
    "| Number of trees | `nrounds` | `num_iterations` | More trees = slower; stop early |\n",
    "| Learning rate | `eta` | `learning_rate` | Lower = more trees needed but better generalisation |\n",
    "| Tree depth | `max_depth` | `max_depth` / `num_leaves` | Complexity; risk of overfitting |\n",
    "| Subsampling | `subsample` | `bagging_fraction` | Row subsampling; reduces variance |\n",
    "| Feature fraction | `colsample_bytree` | `feature_fraction` | Column subsampling |\n",
    "| L1 regularisation | `alpha` | `lambda_l1` | Sparsity |\n",
    "| L2 regularisation | `lambda` | `lambda_l2` | Shrinkage |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(xgboost)      # gradient boosting\n",
    "library(lightgbm)     # fast gradient boosting\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "n <- 800\n",
    "gb_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  slope_pct   = abs(rnorm(n, 10, 5)),\n",
    "  habitat_ref = as.integer(sample(c(\"reference\",\"restored\",\"degraded\"), n,\n",
    "                              replace=TRUE, prob=c(.35,.35,.30)) == \"reference\"),\n",
    "  habitat_deg = as.integer(sample(c(\"reference\",\"restored\",\"degraded\"), n,\n",
    "                              replace=TRUE, prob=c(.35,.35,.30)) == \"degraded\"),\n",
    "  log_odds    = -1 + 0.6*water_qual - 0.7*nitrate - 0.3*distance_km +\n",
    "                0.8*habitat_ref - 0.9*habitat_deg,\n",
    "  label       = rbinom(n, 1, plogis(log_odds))  # 0/1 for XGBoost/LightGBM\n",
    ")\n",
    "\n",
    "feature_cols <- c(\"nitrate\",\"water_qual\",\"distance_km\",\"elevation\",\n",
    "                  \"slope_pct\",\"habitat_ref\",\"habitat_deg\")\n",
    "\n",
    "train_idx <- sample(n, 600)\n",
    "X_train   <- as.matrix(gb_data[train_idx,  feature_cols])\n",
    "X_test    <- as.matrix(gb_data[-train_idx, feature_cols])\n",
    "y_train   <- gb_data$label[train_idx]\n",
    "y_test    <- gb_data$label[-train_idx]\n",
    "\n",
    "cat(sprintf(\"Train: %d | Test: %d | Prevalence: %.1f%%\\n\",\n",
    "            length(y_train), length(y_test), mean(y_train)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-xgboost",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-xgb-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── DMatrix: XGBoost's native data format ────────────────────────────────────\n",
    "dtrain <- xgboost::xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  <- xgboost::xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "# ── Parameters ────────────────────────────────────────────────────────────────\n",
    "xgb_params <- list(\n",
    "  booster          = \"gbtree\",\n",
    "  objective        = \"binary:logistic\",  # output probabilities\n",
    "  eval_metric      = \"auc\",\n",
    "  eta              = 0.05,     # learning rate; lower = more trees needed\n",
    "  max_depth        = 4,        # tree depth; 3-6 typical\n",
    "  subsample        = 0.8,      # row subsampling\n",
    "  colsample_bytree = 0.8,      # column subsampling\n",
    "  min_child_weight = 5,        # min sum of instance weight in a child\n",
    "  lambda           = 1,        # L2 regularisation\n",
    "  alpha            = 0         # L1 regularisation\n",
    ")\n",
    "\n",
    "# ── Fit with early stopping ───────────────────────────────────────────────────\n",
    "# Early stopping: stop when validation AUC hasn't improved in 50 rounds\n",
    "xgb_cv <- xgboost::xgb.cv(\n",
    "  params   = xgb_params,\n",
    "  data     = dtrain,\n",
    "  nrounds  = 1000,\n",
    "  nfold    = 5,\n",
    "  early_stopping_rounds = 50,\n",
    "  verbose  = FALSE,\n",
    "  seed     = 42\n",
    ")\n",
    "best_nrounds <- xgb_cv$best_iteration\n",
    "cat(sprintf(\"Best nrounds (5-fold CV): %d\\n\", best_nrounds))\n",
    "\n",
    "# Final model at best nrounds\n",
    "xgb_fit <- xgboost::xgboost(\n",
    "  params  = xgb_params,\n",
    "  data    = dtrain,\n",
    "  nrounds = best_nrounds,\n",
    "  verbose = 0,\n",
    "  seed    = 42\n",
    ")\n",
    "\n",
    "# Predict\n",
    "xgb_probs <- predict(xgb_fit, dtest)\n",
    "xgb_preds <- tibble(\n",
    "  truth        = factor(y_test, levels=c(0,1), labels=c(\"absent\",\"present\")),\n",
    "  prob_present = xgb_probs,\n",
    "  pred_class   = factor(ifelse(xgb_probs>=0.5, \"present\",\"absent\"),\n",
    "                        levels=c(\"absent\",\"present\"))\n",
    ")\n",
    "cat(\"\\nXGBoost Test Metrics:\\n\")\n",
    "yardstick::metrics(xgb_preds, truth=truth, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(xgb_preds, truth=truth, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-lgbm",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-lgbm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LightGBM Dataset ─────────────────────────────────────────────────────────\n",
    "lgb_train <- lightgbm::lgb.Dataset(\n",
    "  X_train, label=y_train,\n",
    "  categorical_feature=c()   # specify categorical columns by index if any\n",
    ")\n",
    "lgb_test  <- lightgbm::lgb.Dataset.create.valid(lgb_train, X_test, label=y_test)\n",
    "\n",
    "# ── Parameters ────────────────────────────────────────────────────────────────\n",
    "lgb_params <- list(\n",
    "  objective         = \"binary\",\n",
    "  metric            = \"auc\",\n",
    "  learning_rate     = 0.05,\n",
    "  num_leaves        = 31,     # LightGBM uses num_leaves instead of max_depth\n",
    "                               # 2^max_depth is the theoretical max; keep < 2^max_depth\n",
    "  feature_fraction  = 0.8,\n",
    "  bagging_fraction  = 0.8,\n",
    "  bagging_freq      = 5,\n",
    "  min_data_in_leaf  = 10,\n",
    "  lambda_l1         = 0,\n",
    "  lambda_l2         = 1,\n",
    "  verbose           = -1\n",
    ")\n",
    "\n",
    "lgb_fit <- lightgbm::lgb.train(\n",
    "  params         = lgb_params,\n",
    "  data           = lgb_train,\n",
    "  nrounds        = 1000,\n",
    "  valids         = list(test=lgb_test),\n",
    "  early_stopping_rounds = 50,\n",
    "  verbose        = -1\n",
    ")\n",
    "cat(sprintf(\"LightGBM best iteration: %d\\n\", lgb_fit$best_iter))\n",
    "\n",
    "# Predict\n",
    "lgb_probs <- predict(lgb_fit, X_test)\n",
    "lgb_preds <- tibble(\n",
    "  truth        = factor(y_test, levels=c(0,1), labels=c(\"absent\",\"present\")),\n",
    "  prob_present = lgb_probs,\n",
    "  pred_class   = factor(ifelse(lgb_probs>=0.5, \"present\",\"absent\"),\n",
    "                        levels=c(\"absent\",\"present\"))\n",
    ")\n",
    "cat(\"\\nLightGBM Test Metrics:\\n\")\n",
    "yardstick::metrics(lgb_preds, truth=truth, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(lgb_preds, truth=truth, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-compare",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-vi-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost importance (gain: total improvement from splits using the feature)\n",
    "xgb_vi <- xgboost::xgb.importance(model=xgb_fit, feature_names=feature_cols) %>%\n",
    "  as_tibble() %>%\n",
    "  mutate(model=\"XGBoost\")\n",
    "\n",
    "# LightGBM importance\n",
    "lgb_vi <- lightgbm::lgb.importance(lgb_fit) %>%\n",
    "  as_tibble() %>%\n",
    "  rename(Feature=Feature, Gain=Gain) %>%\n",
    "  mutate(model=\"LightGBM\")\n",
    "\n",
    "vi_combined <- bind_rows(\n",
    "  xgb_vi %>% select(Feature, Gain, model),\n",
    "  lgb_vi  %>% select(Feature, Gain, model)\n",
    ") %>%\n",
    "  group_by(model) %>%\n",
    "  mutate(Gain_scaled = Gain / max(Gain) * 100) %>%\n",
    "  ungroup()\n",
    "\n",
    "ggplot(vi_combined,\n",
    "       aes(x=Gain_scaled,\n",
    "           y=reorder(Feature, Gain_scaled),\n",
    "           fill=model)) +\n",
    "  geom_col(position=\"dodge\", alpha=0.8) +\n",
    "  scale_fill_manual(values=c(XGBoost=\"#4a8fff\", LightGBM=\"#4fffb0\")) +\n",
    "  labs(title=\"Feature Importance: XGBoost vs. LightGBM\",\n",
    "       subtitle=\"Gain (scaled to 100); rankings should be similar between models\",\n",
    "       x=\"Gain (scaled)\", y=NULL, fill=\"Model\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Not using early stopping**  \n",
    "Without early stopping, gradient boosting will overfit as `nrounds` increases. Always use cross-validation or a validation set with `early_stopping_rounds`. The number of trees is a regularisation parameter, not a fixed choice.\n",
    "\n",
    "**2. Setting a high learning rate with few trees**  \n",
    "A high `eta` (>0.1) makes learning fast but usually produces a worse model than a low `eta` (0.01–0.05) with more trees (found via early stopping). Lower learning rate + more trees is almost always better given enough time.\n",
    "\n",
    "**3. Using LightGBM's `num_leaves` without understanding it**  \n",
    "LightGBM grows trees leaf-wise rather than level-wise. `num_leaves` controls complexity directly — a model with `num_leaves=127` is far more complex than `max_depth=7` (2^7=128 leaves, but leaf-wise growth often concentrates on a few branches). On small datasets (n < 10k), reduce `num_leaves` to avoid overfitting.\n",
    "\n",
    "**4. Using gain-based importance as a definitive ranking**  \n",
    "Gain importance in XGBoost and LightGBM is biased toward features with many possible split points. Use SHAP values for a more reliable and interpretable importance ranking — see `shap_explainability.ipynb`.\n",
    "\n",
    "**5. Not tuning regularisation parameters alongside tree parameters**  \n",
    "Most practitioners tune `max_depth`, `eta`, and `nrounds` but forget `subsample`, `colsample_bytree`, `lambda`, and `alpha`. Subsampling and regularisation are critical for avoiding overfitting on small or noisy datasets. Tune them together using a grid or random search.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

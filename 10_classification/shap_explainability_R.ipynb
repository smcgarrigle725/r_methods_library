{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# SHAP Values for Model Explainability with `shapviz`\n",
    "\n",
    "## Overview\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game-theoretic framework for explaining any machine learning model's predictions. A SHAP value for feature j on observation i represents the contribution of feature j to the prediction for observation i, averaged over all possible feature subsets.\n",
    "\n",
    "**Key properties:**\n",
    "- **Local:** Explains individual predictions, not just global averages\n",
    "- **Consistent:** If model A relies more on feature X than model B, SHAP values for X will be higher in A\n",
    "- **Additive:** Σ SHAP values = prediction − expected prediction\n",
    "- **Model-agnostic:** Works with any model (though tree SHAP is exact and fast for XGBoost/LightGBM/ranger)\n",
    "\n",
    "**SHAP value types:**\n",
    "\n",
    "| Type | What it shows |\n",
    "|---|---|\n",
    "| **Mean |SHAP|** | Global feature importance — how much each feature contributes on average |\n",
    "| **Beeswarm plot** | Distribution of SHAP values across all observations; shows direction and spread |\n",
    "| **Dependence plot** | SHAP value vs. feature value; shows marginal effect + interaction with a second feature |\n",
    "| **Waterfall plot** | Local: how each feature drove a specific prediction away from the baseline |\n",
    "| **Force plot** | Local: compact visual of positive/negative contributions per observation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(xgboost)    # tree SHAP is exact and fast for XGBoost\n",
    "library(shapviz)    # SHAP visualisation\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Dataset ───────────────────────────────────────────────────────────────────\n",
    "n <- 600\n",
    "shap_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  slope_pct   = abs(rnorm(n, 10, 5)),\n",
    "  habitat_ref = as.integer(runif(n) < 0.35),\n",
    "  habitat_deg = as.integer(runif(n) < 0.30),\n",
    "  log_odds    = -1 + 0.7*water_qual - 0.8*nitrate - 0.25*distance_km +\n",
    "                0.9*habitat_ref - 1.0*habitat_deg,\n",
    "  label       = rbinom(n, 1, plogis(log_odds))\n",
    ")\n",
    "\n",
    "feature_cols <- c(\"nitrate\",\"water_qual\",\"distance_km\",\"elevation\",\n",
    "                  \"slope_pct\",\"habitat_ref\",\"habitat_deg\")\n",
    "\n",
    "train_idx  <- sample(n, 450)\n",
    "X_train    <- as.matrix(shap_data[train_idx,  feature_cols])\n",
    "X_explain  <- as.matrix(shap_data[-train_idx, feature_cols])  # SHAP computed on this\n",
    "y_train    <- shap_data$label[train_idx]\n",
    "\n",
    "dtrain <- xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# ── Fit XGBoost ───────────────────────────────────────────────────────────────\n",
    "xgb_fit <- xgboost(\n",
    "  data    = dtrain,\n",
    "  params  = list(objective=\"binary:logistic\", eta=0.05, max_depth=4,\n",
    "                 subsample=0.8, colsample_bytree=0.8, eval_metric=\"auc\"),\n",
    "  nrounds = 200,\n",
    "  verbose = 0,\n",
    "  seed    = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-compute",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compute SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-shap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapviz builds the SHAP object from the XGBoost model + data to explain\n",
    "# XGBoost computes exact tree SHAP values efficiently\n",
    "shap_obj <- shapviz::shapviz(xgb_fit, X_pred=X_explain)\n",
    "\n",
    "cat(\"SHAP object structure:\\n\")\n",
    "print(shap_obj)\n",
    "# shap_obj$S: SHAP value matrix (n_observations × n_features)\n",
    "# shap_obj$X: feature value matrix\n",
    "# baseline: E[f(x)] — the expected prediction (the reference for all SHAP values)\n",
    "\n",
    "# Check: for any observation, sum of SHAP values + baseline = raw prediction\n",
    "obs_1_shap  <- colSums(shap_obj$S[1,,drop=FALSE])\n",
    "obs_1_pred  <- predict(xgb_fit, X_explain[1,,drop=FALSE], outputmargin=TRUE)\n",
    "cat(sprintf(\"\\nObs 1: SHAP sum + baseline = %.4f; raw prediction = %.4f\\n\",\n",
    "            sum(obs_1_shap) + attr(shap_obj, \"baseline\"),\n",
    "            obs_1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-global",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Global Importance: Beeswarm and Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-global-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Beeswarm plot (aka SHAP summary plot) ────────────────────────────────────\n",
    "# Each dot = one observation\n",
    "# x-axis: SHAP value (positive = pushed toward presence; negative = absence)\n",
    "# Color: feature value (red = high; blue = low)\n",
    "shapviz::sv_importance(\n",
    "  shap_obj,\n",
    "  kind   = \"beeswarm\",   # \"bar\" for mean|SHAP|; \"beeswarm\" for full distribution\n",
    "  max_display = 7\n",
    ") +\n",
    "  labs(title=\"SHAP Beeswarm: Global Feature Importance\",\n",
    "       subtitle=\"Each dot = one observation. Spread = variability of effect across observations.\")\n",
    "\n",
    "# ── Bar plot: mean |SHAP| ─────────────────────────────────────────────────────\n",
    "shapviz::sv_importance(\n",
    "  shap_obj,\n",
    "  kind       = \"bar\",\n",
    "  max_display = 7,\n",
    "  fill       = \"#4a8fff\"\n",
    ") +\n",
    "  labs(title=\"SHAP Mean |SHAP|: Global Feature Ranking\",\n",
    "       subtitle=\"Average absolute contribution; comparable to permutation importance but more informative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-dependence",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dependence Plots: How Features Drive Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-dep-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot: SHAP value vs. feature value\n",
    "# Shape of the relationship = how the model uses this feature\n",
    "# Color by a second feature: reveals interactions\n",
    "\n",
    "p1 <- shapviz::sv_dependence(\n",
    "  shap_obj,\n",
    "  v          = \"water_qual\",    # primary feature on x-axis\n",
    "  color_var  = \"nitrate\",       # interaction feature\n",
    "  alpha      = 0.5\n",
    ") +\n",
    "  labs(title=\"SHAP Dependence: water_qual\",\n",
    "       subtitle=\"Slope = how model uses water_qual; colour = nitrate interaction\")\n",
    "\n",
    "p2 <- shapviz::sv_dependence(\n",
    "  shap_obj,\n",
    "  v         = \"nitrate\",\n",
    "  color_var = \"water_qual\",\n",
    "  alpha     = 0.5\n",
    ") +\n",
    "  labs(title=\"SHAP Dependence: nitrate\")\n",
    "\n",
    "(p1 | p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-local",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Local Explanations: Waterfall and Force Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-local-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two contrasting observations to explain\n",
    "test_probs <- predict(xgb_fit, X_explain)\n",
    "obs_high   <- which.max(test_probs)   # most confidently predicted present\n",
    "obs_low    <- which.min(test_probs)   # most confidently predicted absent\n",
    "\n",
    "# ── Waterfall plot for obs_high ───────────────────────────────────────────────\n",
    "# Shows how each feature pushed the prediction away from the baseline\n",
    "shapviz::sv_waterfall(shap_obj, row_id=obs_high) +\n",
    "  labs(title=sprintf(\"Waterfall: Observation %d (high probability = %.3f)\",\n",
    "                     obs_high, test_probs[obs_high]))\n",
    "\n",
    "# ── Force plot for obs_low ────────────────────────────────────────────────────\n",
    "# Compact horizontal version of the waterfall\n",
    "shapviz::sv_force(shap_obj, row_id=obs_low) +\n",
    "  labs(title=sprintf(\"Force Plot: Observation %d (low probability = %.3f)\",\n",
    "                     obs_low, test_probs[obs_low]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Using gain-based feature importance instead of SHAP for final reporting**  \n",
    "Gain importance (the default in XGBoost and LightGBM) is biased toward features with many split points. SHAP values are theoretically grounded and consistent. Always use mean |SHAP| as the primary importance measure.\n",
    "\n",
    "**2. Confusing SHAP values with the feature effect in the original space**  \n",
    "SHAP values for `binary:logistic` are on the log-odds scale (the raw model output), not the probability scale. A SHAP value of +1 does not mean \"1 unit increase in probability.\" Use `predict(xgb_fit, type='response')` for probabilities, but note that SHAP values for probability outputs are not additive — always use log-odds SHAP for decomposition.\n",
    "\n",
    "**3. Computing SHAP values on the training set**  \n",
    "SHAP values computed on the training set reflect training fit, not generalisation. Always compute SHAP values on a held-out test or validation set for reporting.\n",
    "\n",
    "**4. Treating SHAP as a causal explanation**  \n",
    "SHAP values quantify how much each feature contributes to a model's prediction — they explain the model, not the data-generating process. A high SHAP value for nitrate means the model uses nitrate heavily, not that nitrate causally determines presence. Causal claims require causal designs.\n",
    "\n",
    "**5. Using kernel SHAP when tree SHAP is available**  \n",
    "Kernel SHAP is a model-agnostic approximation and is computationally expensive (O(2^p) or slow Monte Carlo). Tree SHAP is exact and very fast for XGBoost, LightGBM, and ranger. Always use tree SHAP for tree-based models via `shapviz(model, X_pred=X)`.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

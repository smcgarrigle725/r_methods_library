{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "No single metric captures all aspects of classifier performance. The right metric depends on the class distribution and the relative costs of false positives vs. false negatives.\n",
    "\n",
    "**Metric reference:**\n",
    "\n",
    "| Metric | Formula | Use when |\n",
    "|---|---|---|\n",
    "| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Classes balanced and costs equal |\n",
    "| **Sensitivity / Recall** | TP/(TP+FN) | Missing a positive is costly (disease screening) |\n",
    "| **Specificity** | TN/(TN+FP) | False alarms are costly |\n",
    "| **Precision / PPV** | TP/(TP+FP) | Targeted intervention; FP is expensive |\n",
    "| **F1 score** | 2×(Prec×Rec)/(Prec+Rec) | Imbalanced classes; balanced precision/recall |\n",
    "| **F-beta** | (1+β²)×(P×R)/((β²P)+R) | β>1: recall prioritised; β<1: precision prioritised |\n",
    "| **AUC-ROC** | Area under ROC curve | Threshold-independent; discriminative ability |\n",
    "| **AUC-PR** | Area under PR curve | Better for heavily imbalanced classes |\n",
    "| **Brier score** | mean((p−y)²) | Calibration + discrimination for probability outputs |\n",
    "| **Log-loss** | −mean(y log p + (1−y) log(1−p)) | Probability calibration; penalises confident errors |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(yardstick)    # tidymodels metrics\n",
    "library(probably)     # calibration plots\n",
    "library(patchwork)\n",
    "library(ranger)       # for a model to evaluate\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Simulate predictions from two models on a test set ────────────────────────\n",
    "n_test <- 300\n",
    "eval_data <- tibble(\n",
    "  truth         = factor(rbinom(n_test, 1, 0.35),\n",
    "                         levels=c(0,1), labels=c(\"absent\",\"present\")),\n",
    "  # Model A: good discrimination, slightly overconfident\n",
    "  prob_A        = pmin(pmax(\n",
    "    ifelse(truth==\"present\",\n",
    "           rbeta(n_test, 6, 2),\n",
    "           rbeta(n_test, 2, 6)) * 1.1, 0.01), 0.99),\n",
    "  # Model B: poorer discrimination but better calibrated\n",
    "  prob_B        = pmin(pmax(\n",
    "    ifelse(truth==\"present\",\n",
    "           rbeta(n_test, 4, 3),\n",
    "           rbeta(n_test, 3, 4)), 0.01), 0.99),\n",
    "  pred_A        = factor(ifelse(prob_A >= 0.5, \"present\",\"absent\"),\n",
    "                         levels=c(\"absent\",\"present\")),\n",
    "  pred_B        = factor(ifelse(prob_B >= 0.5, \"present\",\"absent\"),\n",
    "                         levels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "cat(sprintf(\"Prevalence: %.1f%%\\n\", mean(eval_data$truth==\"present\")*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-confusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Confusion Matrix and Derived Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-cm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_A <- yardstick::conf_mat(eval_data, truth=truth, estimate=pred_A)\n",
    "print(cm_A)\n",
    "autoplot(cm_A, type=\"heatmap\") +\n",
    "  labs(title=\"Confusion Matrix: Model A\") +\n",
    "  scale_fill_gradient(low=\"#f0f4ff\", high=\"#4a8fff\")\n",
    "\n",
    "# All metrics at once\n",
    "all_metrics <- metric_set(\n",
    "  accuracy, sensitivity, specificity, precision, recall,\n",
    "  f_meas, kap\n",
    ")\n",
    "\n",
    "bind_rows(\n",
    "  all_metrics(eval_data, truth=truth, estimate=pred_A) %>% mutate(model=\"A\"),\n",
    "  all_metrics(eval_data, truth=truth, estimate=pred_B) %>% mutate(model=\"B\")\n",
    ") %>%\n",
    "  pivot_wider(names_from=model, values_from=.estimate, id_cols=.metric) %>%\n",
    "  mutate(across(c(A,B), ~round(.x,3))) %>%\n",
    "  print()\n",
    "\n",
    "# event_level: which level is the \"positive\" class\n",
    "# In yardstick, the first level is negative by default\n",
    "# Our levels are c(\"absent\",\"present\") → present is the positive (second level)\n",
    "# Always set event_level=\"second\" or reorder levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-roc-pr",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-roc-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ROC curves ────────────────────────────────────────────────────────────────\n",
    "roc_A <- yardstick::roc_curve(eval_data, truth, prob_A, event_level=\"second\")\n",
    "roc_B <- yardstick::roc_curve(eval_data, truth, prob_B, event_level=\"second\")\n",
    "\n",
    "auc_A <- yardstick::roc_auc(eval_data, truth, prob_A, event_level=\"second\")$.estimate\n",
    "auc_B <- yardstick::roc_auc(eval_data, truth, prob_B, event_level=\"second\")$.estimate\n",
    "\n",
    "p_roc <- bind_rows(\n",
    "  roc_A %>% mutate(model=sprintf(\"Model A (AUC=%.3f)\", auc_A)),\n",
    "  roc_B %>% mutate(model=sprintf(\"Model B (AUC=%.3f)\", auc_B))\n",
    ") %>%\n",
    "  ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +\n",
    "  geom_path(linewidth=1) +\n",
    "  geom_abline(linetype=\"dashed\", color=\"gray50\") +\n",
    "  scale_color_manual(values=c(\"#4a8fff\",\"#ff6b6b\")) +\n",
    "  labs(title=\"ROC Curves\", x=\"1 - Specificity (FPR)\",\n",
    "       y=\"Sensitivity (TPR)\", color=NULL) +\n",
    "  theme_minimal() + theme(legend.position=\"bottom\")\n",
    "\n",
    "# ── Precision-Recall curves ───────────────────────────────────────────────────\n",
    "pr_A  <- yardstick::pr_curve(eval_data, truth, prob_A, event_level=\"second\")\n",
    "pr_B  <- yardstick::pr_curve(eval_data, truth, prob_B, event_level=\"second\")\n",
    "prauc_A <- yardstick::pr_auc(eval_data, truth, prob_A, event_level=\"second\")$.estimate\n",
    "prauc_B <- yardstick::pr_auc(eval_data, truth, prob_B, event_level=\"second\")$.estimate\n",
    "\n",
    "p_pr <- bind_rows(\n",
    "  pr_A %>% mutate(model=sprintf(\"Model A (AUC-PR=%.3f)\", prauc_A)),\n",
    "  pr_B %>% mutate(model=sprintf(\"Model B (AUC-PR=%.3f)\", prauc_B))\n",
    ") %>%\n",
    "  ggplot(aes(x=recall, y=precision, color=model)) +\n",
    "  geom_path(linewidth=1) +\n",
    "  geom_hline(yintercept=mean(eval_data$truth==\"present\"),\n",
    "             linetype=\"dashed\", color=\"gray50\") +\n",
    "  scale_color_manual(values=c(\"#4a8fff\",\"#ff6b6b\")) +\n",
    "  labs(title=\"Precision-Recall Curves\",\n",
    "       subtitle=\"Dashed = no-skill baseline (prevalence)\",\n",
    "       x=\"Recall\", y=\"Precision\", color=NULL) +\n",
    "  theme_minimal() + theme(legend.position=\"bottom\")\n",
    "\n",
    "(p_roc | p_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-calibration",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-cal-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration: do predicted probabilities match observed frequencies?\n",
    "# A model predicting 0.7 should be correct ~70% of the time\n",
    "\n",
    "# ── Reliability diagram (bin calibration) ─────────────────────────────────────\n",
    "n_bins <- 10\n",
    "cal_df <- bind_rows(\n",
    "  eval_data %>%\n",
    "    mutate(prob=prob_A, model=\"Model A\",\n",
    "           bin=cut(prob_A, breaks=seq(0,1,length.out=n_bins+1), include.lowest=TRUE)),\n",
    "  eval_data %>%\n",
    "    mutate(prob=prob_B, model=\"Model B\",\n",
    "           bin=cut(prob_B, breaks=seq(0,1,length.out=n_bins+1), include.lowest=TRUE))\n",
    ") %>%\n",
    "  group_by(model, bin) %>%\n",
    "  summarise(\n",
    "    n         = n(),\n",
    "    mean_prob = mean(prob),\n",
    "    frac_pos  = mean(truth==\"present\"),\n",
    "    .groups   = \"drop\"\n",
    "  )\n",
    "\n",
    "ggplot(cal_df, aes(x=mean_prob, y=frac_pos, color=model, size=n)) +\n",
    "  geom_abline(slope=1, intercept=0, linetype=\"dashed\", color=\"gray50\") +\n",
    "  geom_point(alpha=0.8) +\n",
    "  geom_line(aes(group=model), linewidth=0.7, alpha=0.6) +\n",
    "  scale_color_manual(values=c(\"#4a8fff\",\"#ff6b6b\")) +\n",
    "  scale_size_continuous(range=c(2,8)) +\n",
    "  labs(title=\"Calibration Curves\",\n",
    "       subtitle=\"Points on the diagonal = perfect calibration; above = underconfident; below = overconfident\",\n",
    "       x=\"Mean predicted probability\", y=\"Fraction positive\",\n",
    "       color=NULL, size=\"n in bin\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Brier score: mean squared error for probabilities; lower = better\n",
    "brier_A <- mean((as.integer(eval_data$truth==\"present\") - eval_data$prob_A)^2)\n",
    "brier_B <- mean((as.integer(eval_data$truth==\"present\") - eval_data$prob_B)^2)\n",
    "cat(sprintf(\"Brier score — A: %.4f | B: %.4f\\n\", brier_A, brier_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Reporting accuracy as the primary metric with imbalanced classes**  \n",
    "With 10% prevalence, a model predicting \"absent\" every time achieves 90% accuracy. Always report sensitivity, specificity, and AUC-PR alongside accuracy when classes are imbalanced. See `imbalanced_data.ipynb`.\n",
    "\n",
    "**2. Comparing models by ROC-AUC when the class is rare**  \n",
    "ROC-AUC is insensitive to class imbalance — both axes of the ROC curve normalise by the true class size. AUC-PR is far more sensitive to performance differences on the minority class and is the preferred metric when prevalence is below 10%.\n",
    "\n",
    "**3. Ignoring calibration when probabilities matter**  \n",
    "AUC-ROC measures discrimination (ranking ability), not calibration (probability accuracy). A model with AUC = 0.85 can be severely miscalibrated. When predicted probabilities are used to make decisions (threshold setting, risk scores, cost-benefit analysis), calibration must be assessed and reported.\n",
    "\n",
    "**4. Evaluating on the same data used for threshold selection**  \n",
    "Selecting a classification threshold on the test set, then reporting metrics at that threshold on the test set is optimistic. Use a separate validation set for threshold selection, or use bootstrap resampling to correct for optimism.\n",
    "\n",
    "**5. Not specifying which class is the positive event**  \n",
    "In `yardstick`, the first factor level is treated as negative by default. If your levels are `c(\"absent\", \"present\")`, sensitivity = TP/(TP+FN) where positive = absent — the opposite of what you intend. Always set `event_level=\"second\"` or reorder factor levels explicitly.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Decision Tree Classification with `rpart`\n",
    "\n",
    "## Overview\n",
    "\n",
    "Decision trees partition the feature space into rectangular regions using recursive binary splits. They are the foundation of random forests and gradient boosting, and are valuable for their interpretability even when outperformed by ensembles.\n",
    "\n",
    "**Strengths:** Fully interpretable; handles mixed feature types natively; no scaling needed; captures non-linear and interaction effects automatically.\n",
    "\n",
    "**Weaknesses:** High variance (small data changes produce very different trees); prone to overfitting without pruning; generally lower predictive performance than ensembles.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "| Parameter | What it controls | Typical range |\n",
    "|---|---|---|\n",
    "| `cp` | Complexity penalty — governs pruning | Cross-validate; start from 0 |\n",
    "| `maxdepth` | Maximum tree depth | 3–10; shallower = more interpretable |\n",
    "| `minsplit` | Min observations to attempt a split | 10–20 |\n",
    "| `minbucket` | Min observations in a terminal node | 5–10 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(rpart)          # recursive partitioning\n",
    "library(rpart.plot)     # rpart.plot(), prp()\n",
    "library(yardstick)      # metrics\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "n <- 500\n",
    "tree_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  habitat     = factor(sample(c(\"reference\",\"restored\",\"degraded\"), n,\n",
    "                              replace=TRUE, prob=c(.35,.35,.30)),\n",
    "                       levels=c(\"reference\",\"restored\",\"degraded\")),\n",
    "  log_odds    = -1 + 0.6*water_qual - 0.7*nitrate - 0.3*distance_km +\n",
    "                case_when(habitat==\"reference\"~0.8, habitat==\"restored\"~0.2,\n",
    "                          habitat==\"degraded\"~ -0.9),\n",
    "  present     = factor(rbinom(n, 1, plogis(log_odds)),\n",
    "                       levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "train_idx  <- sample(n, 375)\n",
    "train_data <- tree_data[train_idx,]\n",
    "test_data  <- tree_data[-train_idx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-fit",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fit and Visualise a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-fit-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a full tree (cp=0: no pruning yet)\n",
    "tree_full <- rpart(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + habitat,\n",
    "  data    = train_data,\n",
    "  method  = \"class\",           # classification (use 'anova' for regression)\n",
    "  parms   = list(split=\"gini\"), # gini impurity (or 'information' for entropy)\n",
    "  control = rpart.control(\n",
    "    cp       = 0,     # grow full tree first; prune separately\n",
    "    minsplit = 10,\n",
    "    minbucket = 5,\n",
    "    maxdepth = 10\n",
    "  )\n",
    ")\n",
    "\n",
    "# ── Cross-validation plot to select cp ───────────────────────────────────────\n",
    "plotcp(tree_full)\n",
    "# x-axis: cp value; y-axis: cross-validated error\n",
    "# Select the leftmost point within 1 SE of the minimum (1-SE rule)\n",
    "printcp(tree_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-prune",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-prune-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select cp using the 1-SE rule: sparsest model within 1 SE of minimum CV error\n",
    "cp_table <- as_tibble(tree_full$cptable)\n",
    "min_xerr  <- min(cp_table$xerror)\n",
    "min_xstd  <- cp_table$xstd[which.min(cp_table$xerror)]\n",
    "cp_1se    <- cp_table %>%\n",
    "  filter(xerror <= min_xerr + min_xstd) %>%\n",
    "  slice_max(CP) %>%\n",
    "  pull(CP)\n",
    "\n",
    "cat(sprintf(\"Selected cp (1-SE rule): %.5f\\n\", cp_1se))\n",
    "\n",
    "# Prune to the selected cp\n",
    "tree_pruned <- prune(tree_full, cp=cp_1se)\n",
    "\n",
    "# ── Visualise the pruned tree ─────────────────────────────────────────────────\n",
    "rpart.plot(\n",
    "  tree_pruned,\n",
    "  type    = 4,          # type 4: label at each node with variable name and split\n",
    "  extra   = 106,        # show class probabilities + node %\n",
    "  box.palette = c(\"#ffcccc\",\"#cce0ff\"),  # red=absent, blue=present\n",
    "  shadow.col  = \"gray60\",\n",
    "  main    = \"Pruned Decision Tree: Species Presence\",\n",
    "  cex     = 0.8\n",
    ")\n",
    "# Each node shows: predicted class | P(present) | % of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-evaluate",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation and Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-eval-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test_preds <- test_data %>%\n",
    "  mutate(\n",
    "    prob_present = predict(tree_pruned, newdata=test_data, type=\"prob\")[,\"present\"],\n",
    "    pred_class   = predict(tree_pruned, newdata=test_data, type=\"class\")\n",
    "  )\n",
    "\n",
    "# Metrics\n",
    "yardstick::metrics(test_preds, truth=present, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(test_preds, truth=present, prob_present,\n",
    "                   event_level=\"second\") %>% print()\n",
    "\n",
    "# ── Variable importance ───────────────────────────────────────────────────────\n",
    "# rpart variable importance: sum of improvement in gini impurity per variable\n",
    "vi_df <- tibble(\n",
    "  variable   = names(tree_full$variable.importance),\n",
    "  importance = tree_full$variable.importance\n",
    ") %>%\n",
    "  mutate(importance_scaled = importance / max(importance) * 100) %>%\n",
    "  arrange(desc(importance_scaled))\n",
    "\n",
    "ggplot(vi_df, aes(x=importance_scaled,\n",
    "                  y=fct_reorder(variable, importance_scaled))) +\n",
    "  geom_col(fill=\"#4a8fff\", alpha=0.8) +\n",
    "  labs(title=\"Decision Tree Variable Importance\",\n",
    "       subtitle=\"Sum of impurity improvement; scaled to 100\",\n",
    "       x=\"Importance (scaled)\", y=NULL) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Plotting and interpreting an unpruned tree**  \n",
    "An unpruned tree is overfit to the training data — its structure reflects noise, not signal. Always prune using `cp` selected by cross-validation (`plotcp()` → 1-SE rule) before interpreting the tree.\n",
    "\n",
    "**2. Using a single decision tree as the final model for prediction**  \n",
    "Single trees have high variance — small changes in training data produce very different trees. Use random forests or gradient boosting for prediction tasks. Single trees are primarily useful for interpretability and as teaching tools.\n",
    "\n",
    "**3. Interpreting variable importance as a unique measure of importance**  \n",
    "rpart variable importance is the total gain in Gini impurity improvement. It is biased toward variables with many possible split points (continuous variables vs. binary). For unbiased importance, use conditional importance from `party::ctree()` or permutation importance from random forests.\n",
    "\n",
    "**4. Not setting a minimum node size**  \n",
    "Without `minbucket`, trees can grow terminal nodes containing a single observation. Such nodes perfectly memorise training data and have zero predictive value. Always set `minsplit` and `minbucket`.\n",
    "\n",
    "**5. Using default gini without considering entropy**  \n",
    "For heavily imbalanced classes, entropy (`split=\"information\"`) sometimes produces better-calibrated probability estimates than Gini. Check both when class imbalance is present.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

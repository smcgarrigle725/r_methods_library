{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Support Vector Machines for Classification with `kernlab`\n",
    "\n",
    "## Overview\n",
    "\n",
    "Support Vector Machines (SVMs) find the maximum-margin hyperplane separating two classes. The **kernel trick** implicitly maps features into a high-dimensional space without computing the transformation explicitly, allowing SVMs to capture non-linear decision boundaries.\n",
    "\n",
    "**Kernels:**\n",
    "\n",
    "| Kernel | Formula | When to use |\n",
    "|---|---|---|\n",
    "| **Linear** | $\\langle x, x'\\rangle$ | High-dimensional sparse data (text, genomics); linearly separable classes |\n",
    "| **RBF / Gaussian** | $\\exp(-\\sigma\\|x-x'\\|^2)$ | Most common default; handles non-linear boundaries |\n",
    "| **Polynomial** | $(\\langle x,x'\\rangle + c)^d$ | Image or signal data with structured interactions |\n",
    "\n",
    "**Key hyperparameters:**\n",
    "\n",
    "| Parameter | Controls | Typical range |\n",
    "|---|---|---|\n",
    "| `C` | Margin width vs. misclassification penalty; larger C = harder margin | 0.01–1000; tune on log scale |\n",
    "| `sigma` (RBF) | Kernel width; larger = smoother boundary | Estimated by `sigest()`; tune around this |\n",
    "\n",
    "**When SVMs shine:** High-dimensional feature spaces (p >> n), sparse data, problems where margin maximisation aligns with the task (e.g. spectral classification, gene expression). When n is large (> 50k) and features are tabular, gradient boosting usually outperforms SVMs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(kernlab)     # ksvm() — SVM with kernel methods\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Simulate: species presence with non-linear boundary ───────────────────────\n",
    "n <- 400\n",
    "svm_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  slope_pct   = abs(rnorm(n, 10, 5)),\n",
    "  # Non-linear interaction term drives presence\n",
    "  log_odds    = -2 + 0.6*water_qual - 0.7*nitrate - 0.25*distance_km +\n",
    "                0.4*(water_qual > 6.5 & nitrate < 3),   # interaction threshold\n",
    "  present     = factor(rbinom(n, 1, plogis(log_odds)),\n",
    "                       levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ") %>%\n",
    "  # SVMs require scaled features — critical step\n",
    "  mutate(across(c(nitrate, water_qual, distance_km, elevation, slope_pct), scale))\n",
    "\n",
    "cat(sprintf(\"Prevalence: %.1f%% present\\n\", mean(svm_data$present==\"present\")*100))\n",
    "\n",
    "train_idx  <- sample(n, 300)\n",
    "train_data <- svm_data[train_idx, ]\n",
    "test_data  <- svm_data[-train_idx, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-linear",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-linear-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel: good baseline when features >> observations or data is sparse\n",
    "svm_linear <- kernlab::ksvm(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + slope_pct,\n",
    "  data     = train_data,\n",
    "  kernel   = \"vanilladot\",   # linear kernel\n",
    "  C        = 1,              # regularisation parameter\n",
    "  prob.model = TRUE          # needed for probability estimates (slower)\n",
    ")\n",
    "print(svm_linear)\n",
    "# Number of support vectors: observations on or inside the margin\n",
    "# Training error: in-sample misclassification\n",
    "\n",
    "# Predictions\n",
    "test_preds_lin <- test_data %>%\n",
    "  mutate(\n",
    "    pred_class   = predict(svm_linear, newdata=test_data, type=\"response\"),\n",
    "    prob_present = predict(svm_linear, newdata=test_data, type=\"probabilities\")[,\"present\"]\n",
    "  )\n",
    "\n",
    "yardstick::metrics(test_preds_lin, truth=present, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(test_preds_lin, truth=present, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-rbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RBF Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-rbf-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Estimate sigma using sigest() ─────────────────────────────────────────────\n",
    "# sigest() estimates the range of sigma values where the kernel is informative\n",
    "# Use the geometric mean of the 0.1 and 0.9 quantiles as the starting point\n",
    "X_train <- model.matrix(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + slope_pct - 1,\n",
    "  data=train_data\n",
    ")\n",
    "sigma_range <- kernlab::sigest(X_train, frac=0.5)\n",
    "cat(sprintf(\"sigest sigma range: [%.4f, %.4f]\\n\",\n",
    "            sigma_range[1], sigma_range[3]))\n",
    "sigma_start <- sigma_range[2]   # median estimate\n",
    "\n",
    "# Fit RBF SVM with estimated sigma\n",
    "svm_rbf <- kernlab::ksvm(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + slope_pct,\n",
    "  data       = train_data,\n",
    "  kernel     = \"rbfdot\",\n",
    "  kpar       = list(sigma=sigma_start),\n",
    "  C          = 1,\n",
    "  prob.model = TRUE\n",
    ")\n",
    "print(svm_rbf)\n",
    "\n",
    "test_preds_rbf <- test_data %>%\n",
    "  mutate(\n",
    "    pred_class   = predict(svm_rbf, newdata=test_data, type=\"response\"),\n",
    "    prob_present = predict(svm_rbf, newdata=test_data, type=\"probabilities\")[,\"present\"]\n",
    "  )\n",
    "\n",
    "yardstick::metrics(test_preds_rbf, truth=present, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(test_preds_rbf, truth=present, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-tune",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tuning C and Sigma via Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-tune-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over C and sigma\n",
    "# C and sigma interact: tune together on a 2D grid\n",
    "C_grid     <- c(0.1, 1, 10, 100)\n",
    "sigma_grid <- sigma_range[c(1,2,3)]   # low, mid, high from sigest\n",
    "\n",
    "tune_results <- expand_grid(C=C_grid, sigma=sigma_grid) %>%\n",
    "  mutate(\n",
    "    cv_error = map2_dbl(C, sigma, function(c_val, sig_val) {\n",
    "      # ksvm with cross=5 uses 5-fold CV internally\n",
    "      fit <- kernlab::ksvm(\n",
    "        present ~ nitrate + water_qual + distance_km + elevation + slope_pct,\n",
    "        data       = train_data,\n",
    "        kernel     = \"rbfdot\",\n",
    "        kpar       = list(sigma=sig_val),\n",
    "        C          = c_val,\n",
    "        cross      = 5,        # 5-fold CV\n",
    "        prob.model = FALSE\n",
    "      )\n",
    "      cross(fit)   # returns CV error\n",
    "    })\n",
    "  )\n",
    "\n",
    "print(tune_results %>% arrange(cv_error))\n",
    "\n",
    "best_params <- tune_results %>% slice_min(cv_error)\n",
    "cat(sprintf(\"\\nBest: C=%.1f, sigma=%.4f, CV error=%.4f\\n\",\n",
    "            best_params$C, best_params$sigma, best_params$cv_error))\n",
    "\n",
    "# Heatmap of CV error\n",
    "ggplot(tune_results,\n",
    "       aes(x=factor(C), y=factor(round(sigma,4)), fill=cv_error)) +\n",
    "  geom_tile() +\n",
    "  geom_text(aes(label=round(cv_error,3)), size=3) +\n",
    "  scale_fill_gradient(low=\"#4fffb0\", high=\"#ff6b6b\") +\n",
    "  labs(title=\"SVM Tuning: CV Error by C and sigma\",\n",
    "       x=\"C (regularisation)\", y=\"sigma (RBF width)\", fill=\"CV error\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-final",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-final-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_final <- kernlab::ksvm(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + slope_pct,\n",
    "  data       = train_data,\n",
    "  kernel     = \"rbfdot\",\n",
    "  kpar       = list(sigma=best_params$sigma),\n",
    "  C          = best_params$C,\n",
    "  prob.model = TRUE\n",
    ")\n",
    "\n",
    "test_preds_final <- test_data %>%\n",
    "  mutate(\n",
    "    pred_class   = predict(svm_final, newdata=test_data, type=\"response\"),\n",
    "    prob_present = predict(svm_final, newdata=test_data, type=\"probabilities\")[,\"present\"]\n",
    "  )\n",
    "\n",
    "# Full metrics\n",
    "metric_set(accuracy, sensitivity, specificity, f_meas)(\n",
    "  test_preds_final, truth=present, estimate=pred_class, event_level=\"second\"\n",
    ") %>% print()\n",
    "yardstick::roc_auc(test_preds_final, present, prob_present,\n",
    "                   event_level=\"second\") %>% print()\n",
    "\n",
    "# ── Decision boundary visualisation (2D slice) ────────────────────────────────\n",
    "# Fix distance, elevation, slope at their means; vary nitrate and water_qual\n",
    "grid_2d <- expand_grid(\n",
    "  nitrate    = seq(min(svm_data$nitrate), max(svm_data$nitrate), length.out=80),\n",
    "  water_qual = seq(min(svm_data$water_qual), max(svm_data$water_qual), length.out=80)\n",
    ") %>%\n",
    "  mutate(\n",
    "    distance_km = 0, elevation = 0, slope_pct = 0,  # at scaled mean (=0)\n",
    "    pred = predict(svm_final, newdata=., type=\"response\")\n",
    "  )\n",
    "\n",
    "ggplot(grid_2d, aes(x=nitrate, y=water_qual, fill=pred)) +\n",
    "  geom_raster(alpha=0.4) +\n",
    "  geom_point(data=test_data,\n",
    "             aes(x=nitrate, y=water_qual, color=present, fill=NULL),\n",
    "             size=1.5, alpha=0.7) +\n",
    "  scale_fill_manual(values=c(absent=\"#ffcccc\", present=\"#cce0ff\")) +\n",
    "  scale_color_manual(values=c(absent=\"#cc4444\", present=\"#4a8fff\")) +\n",
    "  labs(title=\"SVM Decision Boundary (RBF Kernel)\",\n",
    "       subtitle=\"Shown in nitrate-water_qual plane at mean values of other features\",\n",
    "       x=\"Nitrate (scaled)\", y=\"Water quality (scaled)\",\n",
    "       fill=\"Predicted\", color=\"Observed\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Not scaling features before fitting an SVM**  \n",
    "SVMs are sensitive to feature scale — a predictor measured in metres dominates one measured in millimetres in Euclidean distance calculations. Always standardise all features to zero mean and unit variance before fitting. The kernel computation is meaningless without scaling.\n",
    "\n",
    "**2. Tuning C without tuning sigma**  \n",
    "C and sigma interact. A large sigma makes the kernel very smooth and C has limited effect; a small sigma allows fine-grained boundaries and C is critical. Always tune both jointly on a 2D grid, not sequentially.\n",
    "\n",
    "**3. Using `prob.model=TRUE` without understanding its cost**  \n",
    "Platt scaling (`prob.model=TRUE`) fits a logistic model on the SVM scores to produce probability estimates. It adds computational cost and the probabilities can be poorly calibrated, especially near the decision boundary. If you only need class labels, set `prob.model=FALSE`.\n",
    "\n",
    "**4. Choosing the RBF kernel by default without trying linear first**  \n",
    "For high-dimensional data (p >> n), the linear kernel often matches or beats RBF with far less tuning. The linear kernel is also fully interpretable — coefficients can be extracted from the primal form. Always benchmark the linear kernel before adding kernel complexity.\n",
    "\n",
    "**5. Using SVMs on very large datasets without a subsampling strategy**  \n",
    "Training SVMs scales as O(n² ) to O(n³) with the number of training observations — practically intractable for n > 50k with the standard `ksvm` formulation. For large datasets, use gradient boosting or logistic regression. If SVMs are required, use `liquidSVM` or a stochastic approximation.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

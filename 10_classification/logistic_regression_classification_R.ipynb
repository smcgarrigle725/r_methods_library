{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Logistic Regression for Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "Logistic regression is the canonical linear classifier. It models the log-odds of a binary outcome as a linear combination of predictors, producing calibrated probability estimates. It should be the **default starting point** for any binary classification task — before reaching for random forests or gradient boosting.\n",
    "\n",
    "**Why start here:**\n",
    "- Coefficients are directly interpretable as odds ratios\n",
    "- Probability outputs are naturally calibrated\n",
    "- Computationally trivial even on large datasets\n",
    "- Sets a performance baseline: if XGBoost barely beats logistic regression, the gain may not justify the complexity\n",
    "- Assumption violations are diagnosable and often fixable\n",
    "\n",
    "**Extensions covered:**\n",
    "- Regularised logistic regression (ridge / LASSO / elastic net) via `glmnet` — handles many predictors and multicollinearity\n",
    "- Threshold selection — default 0.5 is rarely optimal; optimise for the task metric\n",
    "\n",
    "## Applications by Sector\n",
    "\n",
    "| Sector | Binary outcome |\n",
    "|---|---|\n",
    "| **Ecology** | Presence / absence of a species at a site |\n",
    "| **Healthcare** | Disease positive / negative; readmission yes / no |\n",
    "| **Insurance** | Claim yes / no; fraud yes / no |\n",
    "| **Finance** | Default yes / no; churn yes / no |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(glmnet)      # regularised logistic regression\n",
    "library(yardstick)   # tidymodels metrics: roc_auc, f_meas, etc.\n",
    "library(probably)    # threshold optimisation\n",
    "library(patchwork)\n",
    "library(broom)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Simulate: species presence/absence at 400 sites ───────────────────────────\n",
    "n <- 400\n",
    "clf_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  habitat     = factor(sample(c(\"reference\",\"restored\",\"degraded\"), n,\n",
    "                              replace=TRUE, prob=c(.35,.35,.30)),\n",
    "                       levels=c(\"reference\",\"restored\",\"degraded\")),\n",
    "  log_odds    = -1 + 0.5*water_qual - 0.6*nitrate - 0.3*distance_km +\n",
    "                case_when(habitat==\"reference\"~0.8, habitat==\"restored\"~0.2,\n",
    "                          habitat==\"degraded\"~ -0.9),\n",
    "  present     = factor(rbinom(n, 1, plogis(log_odds)),\n",
    "                       levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "cat(sprintf(\"Prevalence: %.1f%% present\\n\", mean(clf_data$present==\"present\")*100))\n",
    "\n",
    "# Train/test split (75/25)\n",
    "train_idx  <- sample(n, floor(0.75*n))\n",
    "train_data <- clf_data[train_idx,]\n",
    "test_data  <- clf_data[-train_idx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-glm",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Standard Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-glm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "m_logit <- glm(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation + habitat,\n",
    "  data   = train_data,\n",
    "  family = binomial(link=\"logit\")\n",
    ")\n",
    "summary(m_logit)\n",
    "\n",
    "# ── Odds ratios ───────────────────────────────────────────────────────────────\n",
    "or_table <- broom::tidy(m_logit, exponentiate=TRUE, conf.int=TRUE) %>%\n",
    "  filter(term != \"(Intercept)\") %>%\n",
    "  mutate(across(c(estimate, conf.low, conf.high), ~round(.x,3)))\n",
    "print(or_table)\n",
    "# OR > 1: predictor associated with higher probability of presence\n",
    "# OR < 1: predictor associated with lower probability of presence\n",
    "\n",
    "# ── Predictions on test set ───────────────────────────────────────────────────\n",
    "test_preds <- test_data %>%\n",
    "  mutate(\n",
    "    prob_present = predict(m_logit, newdata=test_data, type=\"response\"),\n",
    "    pred_class   = factor(ifelse(prob_present >= 0.5, \"present\", \"absent\"),\n",
    "                          levels=c(\"absent\",\"present\"))\n",
    "  )\n",
    "\n",
    "# Quick accuracy metrics\n",
    "yardstick::metrics(test_preds, truth=present, estimate=pred_class) %>% print()\n",
    "yardstick::roc_auc(test_preds, truth=present, prob_present,\n",
    "                   event_level=\"second\") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-regularised",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Regularised Logistic Regression with `glmnet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-glmnet-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glmnet requires a model matrix (no formula interface)\n",
    "X_train <- model.matrix(present ~ nitrate + water_qual + distance_km +\n",
    "                          elevation + habitat, data=train_data)[,-1]\n",
    "y_train <- train_data$present\n",
    "X_test  <- model.matrix(present ~ nitrate + water_qual + distance_km +\n",
    "                          elevation + habitat, data=test_data)[,-1]\n",
    "\n",
    "# ── Ridge (alpha=0), LASSO (alpha=1), Elastic net (alpha=0.5) ─────────────────\n",
    "# Cross-validate to select lambda for each\n",
    "cv_ridge <- cv.glmnet(X_train, y_train, family=\"binomial\", alpha=0,\n",
    "                       type.measure=\"auc\", nfolds=10)\n",
    "cv_lasso <- cv.glmnet(X_train, y_train, family=\"binomial\", alpha=1,\n",
    "                       type.measure=\"auc\", nfolds=10)\n",
    "cv_enet  <- cv.glmnet(X_train, y_train, family=\"binomial\", alpha=0.5,\n",
    "                       type.measure=\"auc\", nfolds=10)\n",
    "\n",
    "cat(sprintf(\"Ridge CV-AUC (lambda.min): %.4f\\n\", max(cv_ridge$cvm)))\n",
    "cat(sprintf(\"LASSO CV-AUC (lambda.min): %.4f\\n\", max(cv_lasso$cvm)))\n",
    "cat(sprintf(\"Elastic net CV-AUC:        %.4f\\n\", max(cv_enet$cvm)))\n",
    "\n",
    "# ── LASSO coefficient path ────────────────────────────────────────────────────\n",
    "plot(cv_lasso$glmnet.fit, xvar=\"lambda\", label=TRUE)\n",
    "abline(v=log(cv_lasso$lambda.min), lty=2, col=\"red\")\n",
    "abline(v=log(cv_lasso$lambda.1se), lty=2, col=\"blue\")\n",
    "# lambda.min: minimises CV error\n",
    "# lambda.1se: sparsest model within 1 SE of minimum — preferred for parsimony\n",
    "\n",
    "# ── Non-zero LASSO coefficients at lambda.1se ────────────────────────────────\n",
    "lasso_coef <- coef(cv_lasso, s=\"lambda.1se\")\n",
    "print(lasso_coef[lasso_coef[,1] != 0, , drop=FALSE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-threshold",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Threshold Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-thresh-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default threshold of 0.5 maximises accuracy under equal class prevalence\n",
    "# When classes are imbalanced OR costs of FP/FN differ, optimise the threshold\n",
    "\n",
    "# ── Youden's J: maximises sensitivity + specificity - 1 (balanced threshold) ──\n",
    "roc_data <- yardstick::roc_curve(\n",
    "  test_preds, truth=present, prob_present, event_level=\"second\"\n",
    ")\n",
    "\n",
    "youden_thresh <- roc_data %>%\n",
    "  mutate(j = sensitivity + specificity - 1) %>%\n",
    "  slice_max(j, n=1) %>%\n",
    "  pull(.threshold)\n",
    "\n",
    "cat(sprintf(\"Optimal threshold (Youden's J): %.3f\\n\", youden_thresh))\n",
    "\n",
    "# ── Precision-Recall optimal threshold ────────────────────────────────────────\n",
    "pr_data <- yardstick::pr_curve(\n",
    "  test_preds, truth=present, prob_present, event_level=\"second\"\n",
    ")\n",
    "\n",
    "f1_thresh <- pr_data %>%\n",
    "  mutate(f1 = 2*(precision*recall)/(precision+recall)) %>%\n",
    "  slice_max(f1, n=1) %>%\n",
    "  pull(.threshold)\n",
    "\n",
    "cat(sprintf(\"Optimal threshold (F1):         %.3f\\n\", f1_thresh))\n",
    "\n",
    "# ── Compare performance at each threshold ─────────────────────────────────────\n",
    "eval_at_thresh <- function(preds, prob_col, thresh, label) {\n",
    "  preds %>%\n",
    "    mutate(pred = factor(ifelse({{prob_col}} >= thresh, \"present\",\"absent\"),\n",
    "                         levels=c(\"absent\",\"present\"))) %>%\n",
    "    yardstick::metrics(truth=present, estimate=pred) %>%\n",
    "    mutate(threshold=label)\n",
    "}\n",
    "\n",
    "bind_rows(\n",
    "  eval_at_thresh(test_preds, prob_present, 0.5,           \"0.5 (default)\"),\n",
    "  eval_at_thresh(test_preds, prob_present, youden_thresh, \"Youden\"),\n",
    "  eval_at_thresh(test_preds, prob_present, f1_thresh,     \"Max F1\")\n",
    ") %>% pivot_wider(names_from=.metric, values_from=.estimate) %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-plot",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-plot-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Odds ratio forest plot ────────────────────────────────────────────────────\n",
    "p_or <- or_table %>%\n",
    "  ggplot(aes(x=estimate, xmin=conf.low, xmax=conf.high,\n",
    "             y=fct_reorder(term, estimate))) +\n",
    "  geom_pointrange(color=\"#4a8fff\", linewidth=0.8, fatten=3) +\n",
    "  geom_vline(xintercept=1, linetype=\"dashed\", color=\"gray50\") +\n",
    "  scale_x_log10() +\n",
    "  labs(title=\"Logistic Regression: Odds Ratios\",\n",
    "       x=\"Odds Ratio (log scale)\", y=NULL) +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── ROC curve ────────────────────────────────────────────────────────────────\n",
    "auc_val <- yardstick::roc_auc(test_preds, present, prob_present,\n",
    "                               event_level=\"second\")$.estimate\n",
    "p_roc <- autoplot(roc_data) +\n",
    "  annotate(\"text\", x=0.7, y=0.1,\n",
    "           label=sprintf(\"AUC = %.3f\", auc_val), size=4) +\n",
    "  labs(title=\"ROC Curve\") + theme_minimal()\n",
    "\n",
    "(p_or | p_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Using accuracy as the primary metric with imbalanced classes**  \n",
    "A model predicting \"absent\" for every site achieves 60% accuracy if prevalence is 40%. Always report AUC-ROC, sensitivity, specificity, and F1 alongside accuracy. See `model_evaluation.ipynb` and `imbalanced_data.ipynb`.\n",
    "\n",
    "**2. Not checking the linearity-of-log-odds assumption for continuous predictors**  \n",
    "Logistic regression assumes a linear relationship between each continuous predictor and the log-odds. Check this with partial residual plots (`car::crPlots()`) or by adding spline terms. A strongly non-linear relationship will hurt performance and bias coefficient estimates.\n",
    "\n",
    "**3. Using 0.5 as the classification threshold without justification**  \n",
    "0.5 is only optimal when classes are balanced and false positives and false negatives have equal cost. In ecology (rare species detection) or medicine (disease screening), the costs differ dramatically. Always optimise the threshold explicitly.\n",
    "\n",
    "**4. Interpreting LASSO-excluded variables as unimportant**  \n",
    "LASSO sets some coefficients to exactly zero as a selection mechanism. A zero coefficient means \"not selected given the other variables at this regularisation level\" — not \"the variable has no effect.\" Correlated variables may be arbitrarily selected/excluded by LASSO.\n",
    "\n",
    "**5. Forgetting to check for complete separation**  \n",
    "If a predictor perfectly separates the two classes, `glm()` will return extreme coefficients and inflated standard errors without an error. Check with `car::Anova()` or use `logistf` (Firth's penalised likelihood) when separation is suspected.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

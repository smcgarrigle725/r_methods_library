{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance in Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "Class imbalance occurs when one class vastly outnumbers the other. Standard classifiers optimise accuracy, which becomes meaningless when the minority class is rare — a model predicting the majority class every time achieves high accuracy while being completely useless.\n",
    "\n",
    "**Strategies, in order of preference:**\n",
    "\n",
    "| Strategy | Mechanism | Recommended for |\n",
    "|---|---|---|\n",
    "| **Better metrics** | AUC-PR, F1, sensitivity instead of accuracy | Always; should always be the first step |\n",
    "| **Threshold adjustment** | Lower threshold to flag more positives | When model is well-calibrated |\n",
    "| **Cost-sensitive learning** | Weight minority class in the loss function | Most principled; no data modification |\n",
    "| **ROSE** | Synthetic oversampling + undersampling combined | Moderate imbalance (1:10–1:50) |\n",
    "| **SMOTE** | Synthetic minority class generation | Moderate imbalance; continuous features |\n",
    "| **Random undersampling** | Discard majority class instances | Large datasets where data loss is tolerable |\n",
    "\n",
    "**Rule of thumb:** Try cost-sensitive learning first (no data modification, principled). Add ROSE or SMOTE if cost-sensitive learning is insufficient. Avoid combining both simultaneously.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(ROSE)         # ROSE resampling\n",
    "library(themis)       # SMOTE and other resampling (tidymodels)\n",
    "library(ranger)       # random forest\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ── Simulate: severe class imbalance (5% rare species present) ────────────────\n",
    "n <- 1000\n",
    "imbal_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  log_odds    = -4 + 0.6*water_qual - 0.7*nitrate - 0.2*distance_km,\n",
    "  present     = factor(rbinom(n, 1, plogis(log_odds)),\n",
    "                       levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "cat(sprintf(\"Class distribution: %d absent (%.1f%%), %d present (%.1f%%)\\n\",\n",
    "            sum(imbal_data$present==\"absent\"),\n",
    "            mean(imbal_data$present==\"absent\")*100,\n",
    "            sum(imbal_data$present==\"present\"),\n",
    "            mean(imbal_data$present==\"present\")*100))\n",
    "\n",
    "train_idx  <- sample(n, 750)\n",
    "train_data <- imbal_data[train_idx,]\n",
    "test_data  <- imbal_data[-train_idx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-baseline",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline: Unweighted Model (Demonstrates the Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-base-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive model: ignores imbalance\n",
    "rf_baseline <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data=train_data, num.trees=500, probability=TRUE,\n",
    "  num.threads=4, seed=42\n",
    ")\n",
    "\n",
    "test_preds_base <- test_data %>%\n",
    "  mutate(\n",
    "    prob = predict(rf_baseline, data=test_data)$predictions[,\"present\"],\n",
    "    pred = factor(ifelse(prob>=0.5, \"present\",\"absent\"), levels=c(\"absent\",\"present\"))\n",
    "  )\n",
    "\n",
    "cat(\"Baseline (unweighted, threshold=0.5):\\n\")\n",
    "cat(sprintf(\"  Accuracy:    %.3f\\n\",\n",
    "            yardstick::accuracy(test_preds_base, present, pred)$.estimate))\n",
    "cat(sprintf(\"  Sensitivity: %.3f\\n\",\n",
    "            yardstick::sensitivity(test_preds_base, present, pred, event_level=\"second\")$.estimate))\n",
    "cat(sprintf(\"  AUC-ROC:     %.3f\\n\",\n",
    "            yardstick::roc_auc(test_preds_base, present, prob, event_level=\"second\")$.estimate))\n",
    "cat(sprintf(\"  AUC-PR:      %.3f\\n\",\n",
    "            yardstick::pr_auc(test_preds_base, present, prob, event_level=\"second\")$.estimate))\n",
    "# Expect high accuracy but near-zero sensitivity: the model rarely predicts \"present\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-cost",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Strategy 1: Cost-Sensitive Learning (Class Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-cost-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight each training observation inversely proportional to class frequency\n",
    "class_weights <- ifelse(\n",
    "  train_data$present == \"present\",\n",
    "  1 / mean(train_data$present == \"present\"),   # upweight minority\n",
    "  1 / mean(train_data$present == \"absent\")     # downweight majority\n",
    ")\n",
    "\n",
    "rf_weighted <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data=train_data, num.trees=500, probability=TRUE,\n",
    "  case.weights=class_weights,\n",
    "  num.threads=4, seed=42\n",
    ")\n",
    "\n",
    "test_preds_wtd <- test_data %>%\n",
    "  mutate(\n",
    "    prob = predict(rf_weighted, data=test_data)$predictions[,\"present\"],\n",
    "    pred = factor(ifelse(prob>=0.5, \"present\",\"absent\"), levels=c(\"absent\",\"present\"))\n",
    "  )\n",
    "\n",
    "cat(\"Weighted model (threshold=0.5):\\n\")\n",
    "cat(sprintf(\"  Sensitivity: %.3f\\n\",\n",
    "            yardstick::sensitivity(test_preds_wtd, present, pred, event_level=\"second\")$.estimate))\n",
    "cat(sprintf(\"  AUC-PR:      %.3f\\n\",\n",
    "            yardstick::pr_auc(test_preds_wtd, present, prob, event_level=\"second\")$.estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-smote",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Strategy 2: ROSE and SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-smote-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ROSE: Random Over-Sampling Examples ──────────────────────────────────────\n",
    "# Generates synthetic samples from a smoothed version of the empirical distribution\n",
    "rose_result <- ROSE::ROSE(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data = train_data,\n",
    "  N    = nrow(train_data),  # target size (same as original)\n",
    "  seed = 42\n",
    ")\n",
    "train_rose <- rose_result$data %>%\n",
    "  mutate(present = factor(present, levels=c(0,1), labels=c(\"absent\",\"present\")))\n",
    "\n",
    "cat(sprintf(\"ROSE: %d absent, %d present\\n\",\n",
    "            sum(train_rose$present==\"absent\"),\n",
    "            sum(train_rose$present==\"present\")))\n",
    "\n",
    "rf_rose <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data=train_rose, num.trees=500, probability=TRUE,\n",
    "  num.threads=4, seed=42\n",
    ")\n",
    "\n",
    "test_preds_rose <- test_data %>%\n",
    "  mutate(\n",
    "    prob = predict(rf_rose, data=test_data)$predictions[,\"present\"],\n",
    "    pred = factor(ifelse(prob>=0.5, \"present\",\"absent\"), levels=c(\"absent\",\"present\"))\n",
    "  )\n",
    "\n",
    "# ── SMOTE via themis ──────────────────────────────────────────────────────────\n",
    "# SMOTE creates synthetic minority samples by interpolating between neighbours\n",
    "library(recipes)\n",
    "smote_rec <- recipe(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data = train_data\n",
    ") %>%\n",
    "  themis::step_smote(present, over_ratio=1, seed=42) %>%  # over_ratio=1: balance 1:1\n",
    "  prep()\n",
    "\n",
    "train_smote <- bake(smote_rec, new_data=NULL)\n",
    "cat(sprintf(\"SMOTE: %d absent, %d present\\n\",\n",
    "            sum(train_smote$present==\"absent\"),\n",
    "            sum(train_smote$present==\"present\")))\n",
    "\n",
    "rf_smote <- ranger(\n",
    "  present ~ nitrate + water_qual + distance_km + elevation,\n",
    "  data=train_smote, num.trees=500, probability=TRUE,\n",
    "  num.threads=4, seed=42\n",
    ")\n",
    "\n",
    "test_preds_smote <- test_data %>%\n",
    "  mutate(\n",
    "    prob = predict(rf_smote, data=test_data)$predictions[,\"present\"],\n",
    "    pred = factor(ifelse(prob>=0.5, \"present\",\"absent\"), levels=c(\"absent\",\"present\"))\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-compare",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-compare-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics <- function(preds, prob_col=\"prob\", label) {\n",
    "  tibble(\n",
    "    strategy    = label,\n",
    "    accuracy    = yardstick::accuracy(preds, present, pred)$.estimate,\n",
    "    sensitivity = yardstick::sensitivity(preds, present, pred,\n",
    "                    event_level=\"second\")$.estimate,\n",
    "    specificity = yardstick::specificity(preds, present, pred,\n",
    "                    event_level=\"second\")$.estimate,\n",
    "    f1          = yardstick::f_meas(preds, present, pred,\n",
    "                    event_level=\"second\")$.estimate,\n",
    "    auc_roc     = yardstick::roc_auc(preds, present, !!sym(prob_col),\n",
    "                    event_level=\"second\")$.estimate,\n",
    "    auc_pr      = yardstick::pr_auc(preds, present, !!sym(prob_col),\n",
    "                    event_level=\"second\")$.estimate\n",
    "  )\n",
    "}\n",
    "\n",
    "bind_rows(\n",
    "  compute_metrics(test_preds_base,  label=\"Unweighted\"),\n",
    "  compute_metrics(test_preds_wtd,   label=\"Cost-sensitive\"),\n",
    "  compute_metrics(test_preds_rose,  label=\"ROSE\"),\n",
    "  compute_metrics(test_preds_smote, label=\"SMOTE\")\n",
    ") %>%\n",
    "  mutate(across(where(is.numeric), ~round(.x, 3))) %>%\n",
    "  print()\n",
    "\n",
    "# Key comparison: sensitivity and AUC-PR improve across strategies\n",
    "# Accuracy may drop — this is expected and acceptable when the goal is\n",
    "# detecting rare positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Applying SMOTE before splitting into train/test**  \n",
    "The most critical error: synthetic samples should only be created in the training set. If SMOTE is applied to the full dataset before splitting, synthetic samples from the same neighbourhood as real training observations appear in the test set, inflating performance estimates. Always split first, then resample only the training partition.\n",
    "\n",
    "**2. Evaluating on a resampled test set**  \n",
    "The test set should always reflect the true class distribution. Never apply resampling to the test set — doing so changes the prevalence and makes metrics uninterpretable in real-world terms.\n",
    "\n",
    "**3. Using SMOTE with categorical features without special handling**  \n",
    "SMOTE interpolates between nearest neighbours in feature space. Interpolating categorical features (e.g. habitat type) is not meaningful and produces invalid synthetic observations. Use `themis::step_smotenc()` for mixed feature types, or apply cost-sensitive learning instead.\n",
    "\n",
    "**4. Optimising for accuracy with imbalanced classes**  \n",
    "Accuracy should not be reported as the primary metric when classes are imbalanced. Use AUC-PR, sensitivity at a fixed specificity, or F1 score. Report the class distribution alongside all metrics.\n",
    "\n",
    "**5. Applying extreme oversampling ratios without cross-validating**  \n",
    "Oversampling to a perfect 1:1 ratio is not always optimal — the best ratio depends on the model and dataset. Cross-validate over the oversampling ratio (e.g. 1:1, 1:2, 1:5 minority:majority) as a hyperparameter.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

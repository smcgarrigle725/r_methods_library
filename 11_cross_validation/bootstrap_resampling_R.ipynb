{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Bootstrap Resampling for Model Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "The bootstrap draws repeated samples with replacement from the dataset, fits the model on each sample, and evaluates on either the out-of-bag (OOB) observations or the original dataset. It estimates the sampling distribution of any statistic without analytical assumptions.\n",
    "\n",
    "**Bootstrap estimators for model performance:**\n",
    "\n",
    "| Estimator | Formula | Properties |\n",
    "|---|---|---|\n",
    "| **Naive bootstrap** | Error on original data using bootstrap model | Optimistically biased — evaluated on (partly) seen data |\n",
    "| **OOB bootstrap** | Error on ~36.8% not sampled | Pessimistically biased for small n |\n",
    "| **.632 bootstrap** | 0.368 × training error + 0.632 × OOB error | Corrects for both biases; best for small n |\n",
    "| **.632+ bootstrap** | Adapts .632 weight to no-information rate | Best for nearly perfect or random classifiers |\n",
    "\n",
    "**Why ~36.8%?** Each observation has probability $1-(1-1/n)^n \\approx 1-e^{-1} \\approx 0.632$ of being selected at least once in a bootstrap sample of size n, so ~36.8% of observations are left out on average.\n",
    "\n",
    "**When to use bootstrap over k-fold CV:**\n",
    "- Very small samples (n < 30): fewer holdout folds means each OOB set is more representative\n",
    "- Estimating the uncertainty of a performance metric (CI on AUC, RMSE)\n",
    "- Estimating the uncertainty of model coefficients\n",
    "- The .632 bootstrap for corrected error estimation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(rsample)     # bootstraps()\n",
    "library(recipes)\n",
    "library(parsnip)\n",
    "library(workflows)\n",
    "library(tune)\n",
    "library(yardstick)\n",
    "library(broom)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# Small dataset to demonstrate bootstrap advantages\n",
    "n_small <- 60\n",
    "boot_data <- tibble(\n",
    "  nitrate    = rnorm(n_small, 3, 1.2),\n",
    "  water_qual = rnorm(n_small, 6, 1.5),\n",
    "  elevation  = rnorm(n_small, 200, 80),\n",
    "  present    = factor(rbinom(n_small, 1,\n",
    "                   plogis(-1 + 0.5*water_qual - 0.6*nitrate)),\n",
    "                   levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "cat(sprintf(\"n=%d | Prevalence: %.1f%%\\n\",\n",
    "            n_small, mean(boot_data$present==\"present\")*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-performance",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bootstrap Performance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-perf-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 200 bootstrap resamples\n",
    "boot_splits <- rsample::bootstraps(boot_data, times=200, strata=present)\n",
    "\n",
    "rec <- recipe(present ~ nitrate + water_qual + elevation, data=boot_data) %>%\n",
    "  step_normalize(all_numeric_predictors())\n",
    "\n",
    "logit_spec <- logistic_reg() %>% set_engine(\"glm\")\n",
    "wf         <- workflow() %>% add_recipe(rec) %>% add_model(logit_spec)\n",
    "\n",
    "# OOB performance: evaluate on the ~36.8% not sampled\n",
    "boot_results <- tune::fit_resamples(\n",
    "  wf,\n",
    "  resamples = boot_splits,\n",
    "  metrics   = metric_set(roc_auc, accuracy),\n",
    "  control   = control_resamples(save_pred=TRUE)\n",
    ")\n",
    "\n",
    "boot_summary <- collect_metrics(boot_results)\n",
    "print(boot_summary)\n",
    "\n",
    "# Distribution of AUC across bootstrap resamples\n",
    "boot_auc_dist <- collect_metrics(boot_results, summarize=FALSE) %>%\n",
    "  filter(.metric==\"roc_auc\")\n",
    "\n",
    "ggplot(boot_auc_dist, aes(x=.estimate)) +\n",
    "  geom_histogram(bins=30, fill=\"#4a8fff\", alpha=0.8, color=NA) +\n",
    "  geom_vline(xintercept=mean(boot_auc_dist$.estimate),\n",
    "             color=\"#ff6b6b\", linewidth=1, linetype=\"dashed\") +\n",
    "  labs(title=\"Bootstrap Distribution of AUC-ROC\",\n",
    "       subtitle=\"Each bar = one bootstrap resample; red = mean\",\n",
    "       x=\"AUC-ROC\", y=\"Count\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-632",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The .632 Bootstrap Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-632-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .632 bootstrap corrects for the pessimism of OOB evaluation:\n",
    "# error_632 = 0.368 * training_error + 0.632 * oob_error\n",
    "\n",
    "compute_632 <- function(splits, wf, metric_fn) {\n",
    "  map_dfr(splits$splits, function(sp) {\n",
    "    train_fold <- analysis(sp)\n",
    "    oob_fold   <- assessment(sp)\n",
    "\n",
    "    fit_obj  <- fit(wf, data=train_fold)\n",
    "\n",
    "    # Training error (apparent error)\n",
    "    train_preds <- augment(fit_obj, new_data=train_fold)\n",
    "    train_err   <- 1 - metric_fn(train_preds, truth=present,\n",
    "                                  .pred_present, event_level=\"second\")$.estimate\n",
    "\n",
    "    # OOB error\n",
    "    oob_preds <- augment(fit_obj, new_data=oob_fold)\n",
    "    oob_err   <- 1 - metric_fn(oob_preds, truth=present,\n",
    "                                .pred_present, event_level=\"second\")$.estimate\n",
    "\n",
    "    tibble(train_err=train_err, oob_err=oob_err,\n",
    "           err_632 = 0.368*train_err + 0.632*oob_err)\n",
    "  })\n",
    "}\n",
    "\n",
    "boot_50 <- rsample::bootstraps(boot_data, times=50, strata=present)\n",
    "err_df  <- compute_632(boot_50, wf, yardstick::roc_auc)\n",
    "\n",
    "err_summary <- tibble(\n",
    "  estimator   = c(\"Training (apparent)\", \"OOB\", \".632 Bootstrap\"),\n",
    "  mean_error  = c(mean(err_df$train_err),\n",
    "                  mean(err_df$oob_err),\n",
    "                  mean(err_df$err_632)),\n",
    "  se          = c(sd(err_df$train_err)/sqrt(50),\n",
    "                  sd(err_df$oob_err)/sqrt(50),\n",
    "                  sd(err_df$err_632)/sqrt(50))\n",
    ") %>%\n",
    "  mutate(\n",
    "    mean_auc  = round(1 - mean_error, 4),\n",
    "    se        = round(se, 4)\n",
    "  )\n",
    "\n",
    "print(err_summary)\n",
    "# Training AUC is optimistic (upper bound)\n",
    "# OOB AUC is pessimistic (lower bound, especially at small n)\n",
    "# .632 is between the two — the most honest estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-coef-ci",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bootstrap Confidence Intervals for Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-ci-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CIs for logistic regression coefficients\n",
    "# Useful when parametric assumptions are in doubt\n",
    "\n",
    "boot_coefs <- map_dfr(\n",
    "  boot_50$splits,\n",
    "  function(sp) {\n",
    "    d <- analysis(sp)\n",
    "    m <- glm(present ~ nitrate + water_qual + elevation,\n",
    "             data=d, family=binomial)\n",
    "    broom::tidy(m) %>% select(term, estimate)\n",
    "  }\n",
    ")\n",
    "\n",
    "# Percentile bootstrap CI\n",
    "ci_df <- boot_coefs %>%\n",
    "  group_by(term) %>%\n",
    "  summarise(\n",
    "    boot_mean = mean(estimate),\n",
    "    ci_low    = quantile(estimate, 0.025),\n",
    "    ci_high   = quantile(estimate, 0.975),\n",
    "    .groups   = \"drop\"\n",
    "  ) %>%\n",
    "  filter(term != \"(Intercept)\") %>%\n",
    "  mutate(across(where(is.numeric), ~round(.x, 3)))\n",
    "\n",
    "# Parametric CI for comparison\n",
    "param_fit <- glm(present ~ nitrate + water_qual + elevation,\n",
    "                 data=boot_data, family=binomial)\n",
    "param_ci  <- broom::tidy(param_fit, conf.int=TRUE) %>%\n",
    "  filter(term != \"(Intercept)\") %>%\n",
    "  select(term, estimate, conf.low, conf.high)\n",
    "\n",
    "# Side-by-side comparison\n",
    "inner_join(ci_df, param_ci, by=\"term\") %>%\n",
    "  transmute(\n",
    "    term,\n",
    "    param_estimate = round(estimate, 3),\n",
    "    param_ci       = sprintf(\"[%.3f, %.3f]\", conf.low, conf.high),\n",
    "    boot_estimate  = boot_mean,\n",
    "    boot_ci        = sprintf(\"[%.3f, %.3f]\", ci_low, ci_high)\n",
    "  ) %>% print()\n",
    "\n",
    "# Substantial agreement: good sign for parametric assumptions\n",
    "# Large divergence: parametric assumptions may be violated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Using the naive bootstrap (training error on bootstrap samples) as the performance estimate**  \n",
    "The naive bootstrap evaluates the model on data it was trained on. Because bootstrap samples contain ~63% of the original observations, the model has partially seen the evaluation data — giving an optimistically biased estimate. Always use OOB evaluation or the .632 estimator.\n",
    "\n",
    "**2. Using too few bootstrap resamples**  \n",
    "The variance of bootstrap estimates decreases with the number of resamples. For stable confidence intervals, use B ≥ 1000 resamples. For quick estimates during development, B = 100–200 is adequate, but do not report CIs from fewer than 500 resamples.\n",
    "\n",
    "**3. Not using the .632 bootstrap with small samples**  \n",
    "With n < 50, OOB folds contain only ~18 observations on average — giving noisy per-fold estimates. The .632 estimator reduces this noise by blending training and OOB performance. At larger n (> 200), OOB and .632 converge and the distinction matters less.\n",
    "\n",
    "**4. Forgetting to refit preprocessing inside each bootstrap resample**  \n",
    "The same leakage problem applies to bootstrap CV: scaling, imputation, and encoding must be refit inside each resample. Using `fit_resamples()` with a `workflow()` handles this correctly.\n",
    "\n",
    "**5. Using percentile bootstrap CIs when the sampling distribution is skewed**  \n",
    "The percentile CI assumes the bootstrap distribution is symmetric around the estimate. For skewed distributions (e.g., variance parameters, small AUC values), use the BCa (bias-corrected and accelerated) bootstrap instead, available in the `boot` package via `boot.ci(type=\"bca\")`.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

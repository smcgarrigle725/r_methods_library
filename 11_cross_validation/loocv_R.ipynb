{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "## Overview\n",
    "\n",
    "LOOCV is k-fold CV taken to its extreme: k = n. Each observation serves as the validation set exactly once, trained on all remaining n−1 observations.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "| Property | LOOCV | k-fold (k=10) |\n",
    "|---|---|---|\n",
    "| **Bias** | Nearly unbiased | Small downward bias (trains on less data) |\n",
    "| **Variance** | High (estimates are correlated) | Lower |\n",
    "| **Computation** | O(n) model fits | O(k) model fits |\n",
    "| **Practical** | Only for fast models or analytic shortcuts | General purpose |\n",
    "\n",
    "**When LOOCV is appropriate:**\n",
    "- Small datasets (n < 50) where k-fold CV wastes too much training data\n",
    "- Linear models where the LOOCV estimate has an analytic shortcut (no refitting)\n",
    "- When nearly unbiased error estimation is critical and compute is available\n",
    "\n",
    "**Analytic shortcut for linear models:** For OLS, the LOOCV MSE can be computed without refitting n models:\n",
    "$$\\text{LOOCV MSE} = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{e_i}{1 - h_{ii}}\\right)^2$$\n",
    "where $e_i$ is the residual and $h_{ii}$ is the leverage of observation $i$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(rsample)\n",
    "library(parsnip)\n",
    "library(workflows)\n",
    "library(recipes)\n",
    "library(tune)\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# Small dataset where LOOCV is appropriate\n",
    "n_small <- 40\n",
    "loocv_data <- tibble(\n",
    "  nitrate    = rnorm(n_small, 3, 1.2),\n",
    "  water_qual = rnorm(n_small, 6, 1.5),\n",
    "  elevation  = rnorm(n_small, 200, 80),\n",
    "  richness   = round(25 - 2.5*nitrate + 1.5*water_qual + rnorm(n_small, 0, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-analytic",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analytic LOOCV for Linear Models (No Refitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-analytic-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Analytic shortcut: LOOCV MSE from leverage ────────────────────────────────\n",
    "lm_fit <- lm(richness ~ nitrate + water_qual + elevation, data=loocv_data)\n",
    "\n",
    "# Leverage values from the hat matrix diagonal\n",
    "h_ii <- hatvalues(lm_fit)\n",
    "e_i  <- residuals(lm_fit)\n",
    "\n",
    "# LOOCV MSE: mean of (residual / (1 - leverage))^2\n",
    "loocv_mse_analytic <- mean((e_i / (1 - h_ii))^2)\n",
    "loocv_rmse_analytic <- sqrt(loocv_mse_analytic)\n",
    "\n",
    "# Compare to brute-force LOOCV\n",
    "loocv_rmse_brute <- map_dbl(seq_len(nrow(loocv_data)), function(i) {\n",
    "  m   <- lm(richness ~ nitrate + water_qual + elevation,\n",
    "             data=loocv_data[-i,])\n",
    "  pred <- predict(m, newdata=loocv_data[i,])\n",
    "  (loocv_data$richness[i] - pred)^2\n",
    "}) %>% mean() %>% sqrt()\n",
    "\n",
    "cat(sprintf(\"Analytic LOOCV RMSE:    %.4f\\n\", loocv_rmse_analytic))\n",
    "cat(sprintf(\"Brute-force LOOCV RMSE: %.4f  (should match)\\n\", loocv_rmse_brute))\n",
    "\n",
    "# ── Plot leverage vs. residual ────────────────────────────────────────────────\n",
    "tibble(\n",
    "  id        = seq_len(n_small),\n",
    "  leverage  = h_ii,\n",
    "  residual  = e_i,\n",
    "  loo_error = (e_i / (1 - h_ii))^2\n",
    ") %>%\n",
    "  ggplot(aes(x=leverage, y=residual^2, size=loo_error)) +\n",
    "  geom_point(color=\"#4a8fff\", alpha=0.7) +\n",
    "  geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray50\") +\n",
    "  labs(title=\"LOOCV: Leverage vs. Squared Residual\",\n",
    "       subtitle=\"Size = LOO error contribution; high-leverage + high-residual = influential\",\n",
    "       x=\"Leverage (h_ii)\", y=\"Squared residual\", size=\"LOO error\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-rsample",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LOOCV with `rsample` (Any Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-rsample-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small datasets with non-linear models\n",
    "n_class <- 50\n",
    "loo_class_data <- tibble(\n",
    "  nitrate    = rnorm(n_class, 3, 1.2),\n",
    "  water_qual = rnorm(n_class, 6, 1.5),\n",
    "  elevation  = rnorm(n_class, 200, 80),\n",
    "  present    = factor(rbinom(n_class, 1,\n",
    "                       plogis(-1 + 0.5*water_qual - 0.6*nitrate)),\n",
    "                      levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "loo_splits <- rsample::loo_cv(loo_class_data)   # creates n resamples\n",
    "cat(sprintf(\"LOOCV resamples: %d\\n\", nrow(loo_splits)))\n",
    "\n",
    "# Logistic regression workflow\n",
    "rec_loo <- recipe(present ~ nitrate + water_qual + elevation,\n",
    "                  data=loo_class_data) %>%\n",
    "  step_normalize(all_numeric_predictors())\n",
    "\n",
    "logit_spec <- parsnip::logistic_reg() %>% set_engine(\"glm\")\n",
    "\n",
    "wf_loo <- workflow() %>% add_recipe(rec_loo) %>% add_model(logit_spec)\n",
    "\n",
    "loo_results <- tune::fit_resamples(\n",
    "  wf_loo,\n",
    "  resamples = loo_splits,\n",
    "  metrics   = metric_set(roc_auc, accuracy)\n",
    ")\n",
    "\n",
    "collect_metrics(loo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-compare",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LOOCV vs. k-Fold: When Does It Matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-compare-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple datasets of different sizes and compare LOOCV vs 10-fold\n",
    "compare_cv <- map_dfr(c(30, 50, 100, 200, 500), function(n_sim) {\n",
    "  d <- tibble(\n",
    "    x = rnorm(n_sim),\n",
    "    y = 3 + 2*x + rnorm(n_sim)\n",
    "  )\n",
    "  spec <- linear_reg() %>% set_engine(\"lm\")\n",
    "  rec  <- recipe(y ~ x, data=d)\n",
    "  wf   <- workflow() %>% add_recipe(rec) %>% add_model(spec)\n",
    "\n",
    "  loo_est <- fit_resamples(wf, loo_cv(d),\n",
    "                           metrics=metric_set(rmse)) %>%\n",
    "    collect_metrics() %>% pull(mean)\n",
    "\n",
    "  kf_est <- fit_resamples(wf, vfold_cv(d, v=10),\n",
    "                          metrics=metric_set(rmse)) %>%\n",
    "    collect_metrics() %>% pull(mean)\n",
    "\n",
    "  tibble(n=n_sim, loocv_rmse=loo_est, kfold_rmse=kf_est,\n",
    "         diff=abs(loo_est - kf_est))\n",
    "})\n",
    "\n",
    "print(compare_cv)\n",
    "# Differences should shrink as n increases\n",
    "# At small n, LOOCV is more reliable\n",
    "# At large n, 10-fold is equally reliable and much faster\n",
    "\n",
    "ggplot(compare_cv, aes(x=n)) +\n",
    "  geom_line(aes(y=loocv_rmse, color=\"LOOCV\"), linewidth=1) +\n",
    "  geom_line(aes(y=kfold_rmse, color=\"10-fold CV\"), linewidth=1) +\n",
    "  scale_color_manual(values=c(LOOCV=\"#4a8fff\", `10-fold CV`=\"#ff6b6b\")) +\n",
    "  labs(title=\"LOOCV vs. 10-Fold CV: Agreement Increases with n\",\n",
    "       x=\"Sample size\", y=\"CV RMSE estimate\", color=NULL) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Using LOOCV for large datasets**  \n",
    "LOOCV requires n model fits. For n=1000 with a random forest, this is 1000 × the cost of one fit — hours of computation. For n > 100, 10-fold repeated CV is a better trade-off between bias and variance. Reserve LOOCV for n < 50 or fast analytic models.\n",
    "\n",
    "**2. Expecting lower LOOCV error than k-fold**  \n",
    "LOOCV uses n−1 observations for training (the most data possible), so it has the lowest bias. But its error estimates are highly variable — the n estimates are correlated because they share almost identical training sets. The variance of LOOCV can actually be *higher* than 10-fold CV, even though the bias is lower.\n",
    "\n",
    "**3. Using the analytic LOOCV shortcut for GLMs without checking assumptions**  \n",
    "The leverage-based formula applies exactly to OLS. For generalised linear models (logistic, Poisson), the analytic approximation using GLM influence measures is only approximate. Use brute-force refitting or `rsample::loo_cv()` for classification problems.\n",
    "\n",
    "**4. Interpreting LOOCV as an estimate of future individual predictions**  \n",
    "LOOCV estimates generalisation to a new observation from the same population as the training data. It does not estimate performance on observations from a different distribution, time period, or geographic region.\n",
    "\n",
    "**5. Not accounting for preprocessing in LOOCV**  \n",
    "The same preprocessing leakage issue applies to LOOCV: preprocessing must be refit inside each LOO iteration. Using `workflow()` with `fit_resamples()` handles this correctly.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"

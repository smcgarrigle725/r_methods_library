{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "R", "language": "R", "name": "ir"},
  "language_info": {"name": "R"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01-overview",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: Grid Search and Random Search\n",
    "\n",
    "## Overview\n",
    "\n",
    "Hyperparameters are model settings fixed before training — unlike parameters (weights, coefficients), they are not learned from data. Tuning finds the hyperparameter values that minimise CV error on the training set.\n",
    "\n",
    "**Tuning strategies:**\n",
    "\n",
    "| Strategy | How it works | When to use |\n",
    "|---|---|---|\n",
    "| **Grid search** | Exhaustive evaluation of all combinations | ≤ 2 hyperparameters; small grids |\n",
    "| **Random search** | Random samples from the hyperparameter space | ≥ 3 hyperparameters; large spaces |\n",
    "| **Latin hypercube** | Space-filling design; better coverage than random | When budget is fixed and coverage matters |\n",
    "| **Bayesian optimisation** | Learns which regions to explore next | Very expensive models; not covered here |\n",
    "\n",
    "**Key insight (Bergstra & Bengio 2012):** Random search outperforms grid search for high-dimensional hyperparameter spaces because any given parameter is unlikely to be the most important one. With a fixed budget, random search covers the space better than a regular grid.\n",
    "\n",
    "**The tuning-evaluation split:** Hyperparameter tuning uses cross-validation on the training set. The test set is never involved in tuning. For a rigorous unbiased performance estimate after tuning, use nested CV — see `nested_cv.ipynb`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02-setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(rsample)\n",
    "library(recipes)\n",
    "library(parsnip)\n",
    "library(workflows)\n",
    "library(tune)        # tune_grid(), tune_bayes()\n",
    "library(dials)       # hyperparameter ranges\n",
    "library(yardstick)\n",
    "library(patchwork)\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "n <- 600\n",
    "tune_data <- tibble(\n",
    "  nitrate     = rnorm(n, 3, 1.2),\n",
    "  water_qual  = rnorm(n, 6, 1.5),\n",
    "  distance_km = rexp(n, 0.4),\n",
    "  elevation   = rnorm(n, 200, 80),\n",
    "  slope_pct   = abs(rnorm(n, 10, 5)),\n",
    "  present     = factor(rbinom(n, 1,\n",
    "                   plogis(-1 + 0.6*water_qual - 0.7*nitrate - 0.2*distance_km)),\n",
    "                   levels=c(0,1), labels=c(\"absent\",\"present\"))\n",
    ")\n",
    "\n",
    "init_split <- rsample::initial_split(tune_data, prop=0.80, strata=present)\n",
    "train_data <- rsample::training(init_split)\n",
    "test_data  <- rsample::testing(init_split)\n",
    "cv_folds   <- rsample::vfold_cv(train_data, v=5, strata=present)\n",
    "\n",
    "# Base recipe\n",
    "rec <- recipe(present ~ nitrate + water_qual + distance_km + elevation + slope_pct,\n",
    "              data=train_data) %>%\n",
    "  step_normalize(all_numeric_predictors())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03-grid",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03-grid-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest: tune mtry and min_n\n",
    "rf_spec_tune <- rand_forest(\n",
    "  mode  = \"classification\",\n",
    "  trees = 500,\n",
    "  mtry  = tune(),    # tune() marks this as a tuning parameter\n",
    "  min_n = tune()     # minimum node size\n",
    ") %>%\n",
    "  set_engine(\"ranger\", seed=42)\n",
    "\n",
    "wf_tune <- workflow() %>% add_recipe(rec) %>% add_model(rf_spec_tune)\n",
    "\n",
    "# ── Regular (exhaustive) grid ─────────────────────────────────────────────────\n",
    "grid_regular <- dials::grid_regular(\n",
    "  mtry(range=c(1, 5)),\n",
    "  min_n(range=c(2, 20)),\n",
    "  levels=c(5, 4)   # 5 mtry × 4 min_n = 20 combinations\n",
    ")\n",
    "cat(sprintf(\"Grid size: %d combinations\\n\", nrow(grid_regular)))\n",
    "print(grid_regular)\n",
    "\n",
    "# Run grid search\n",
    "grid_results <- tune::tune_grid(\n",
    "  wf_tune,\n",
    "  resamples = cv_folds,\n",
    "  grid      = grid_regular,\n",
    "  metrics   = metric_set(roc_auc),\n",
    "  control   = control_grid(verbose=FALSE)\n",
    ")\n",
    "\n",
    "# Best hyperparameters\n",
    "show_best(grid_results, metric=\"roc_auc\", n=5)\n",
    "best_grid <- select_best(grid_results, metric=\"roc_auc\")\n",
    "cat(sprintf(\"\\nBest: mtry=%d, min_n=%d\\n\", best_grid$mtry, best_grid$min_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04-random",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04-random-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost: larger hyperparameter space — random search preferred\n",
    "xgb_spec_tune <- boost_tree(\n",
    "  mode            = \"classification\",\n",
    "  trees           = 300,\n",
    "  tree_depth      = tune(),\n",
    "  learn_rate      = tune(),\n",
    "  mtry            = tune(),\n",
    "  min_n           = tune(),\n",
    "  loss_reduction  = tune()\n",
    ") %>%\n",
    "  set_engine(\"xgboost\", seed=42)\n",
    "\n",
    "wf_xgb <- workflow() %>% add_recipe(rec) %>% add_model(xgb_spec_tune)\n",
    "\n",
    "# Latin hypercube design: better coverage than pure random\n",
    "grid_random <- dials::grid_latin_hypercube(\n",
    "  tree_depth(),\n",
    "  learn_rate(),\n",
    "  mtry(range=c(1, 5)),\n",
    "  min_n(),\n",
    "  loss_reduction(),\n",
    "  size = 30   # 30 random combinations\n",
    ")\n",
    "cat(sprintf(\"Random grid: %d combinations (vs. exhaustive grid of %d)\\n\",\n",
    "            nrow(grid_random),\n",
    "            prod(c(5, 5, 5, 5, 5))))\n",
    "\n",
    "random_results <- tune::tune_grid(\n",
    "  wf_xgb,\n",
    "  resamples = cv_folds,\n",
    "  grid      = grid_random,\n",
    "  metrics   = metric_set(roc_auc),\n",
    "  control   = control_grid(verbose=FALSE)\n",
    ")\n",
    "\n",
    "show_best(random_results, metric=\"roc_auc\", n=5)\n",
    "best_xgb <- select_best(random_results, metric=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-visualise",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualising the Tuning Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Grid search: heatmap of AUC by mtry × min_n ──────────────────────────────\n",
    "p_grid <- collect_metrics(grid_results) %>%\n",
    "  filter(.metric==\"roc_auc\") %>%\n",
    "  ggplot(aes(x=factor(mtry), y=factor(min_n), fill=mean)) +\n",
    "  geom_tile() +\n",
    "  geom_text(aes(label=round(mean,3)), size=3) +\n",
    "  scale_fill_gradient(low=\"#ffcccc\", high=\"#4a8fff\") +\n",
    "  labs(title=\"Grid Search: AUC-ROC by mtry × min_n\",\n",
    "       x=\"mtry\", y=\"min_n\", fill=\"Mean AUC\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# ── Random search: marginal effect plots ──────────────────────────────────────\n",
    "p_rand <- collect_metrics(random_results) %>%\n",
    "  filter(.metric==\"roc_auc\") %>%\n",
    "  pivot_longer(c(tree_depth, learn_rate, mtry, min_n),\n",
    "               names_to=\"param\", values_to=\"value\") %>%\n",
    "  ggplot(aes(x=value, y=mean)) +\n",
    "  geom_point(color=\"#4a8fff\", alpha=0.7) +\n",
    "  geom_smooth(color=\"#ff6b6b\", se=FALSE, method=\"loess\", span=0.8) +\n",
    "  facet_wrap(~param, scales=\"free_x\", ncol=2) +\n",
    "  labs(title=\"Random Search: Marginal Effect of Each Hyperparameter\",\n",
    "       subtitle=\"Each point = one evaluated combination\",\n",
    "       x=\"Parameter value\", y=\"Mean AUC-ROC\") +\n",
    "  theme_minimal()\n",
    "\n",
    "(p_grid / p_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06-finalise",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Finalise and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06-final-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-SE rule: select simplest model within 1 SE of the best\n",
    "# Prefer simpler models when performance difference is within noise\n",
    "best_1se <- select_by_one_std_err(grid_results, metric=\"roc_auc\",\n",
    "                                   desc(min_n))   # prefer larger min_n (simpler)\n",
    "\n",
    "cat(\"Best model (max AUC):\")\n",
    "print(best_grid)\n",
    "cat(\"\\n1-SE rule (simpler model):\")\n",
    "print(best_1se)\n",
    "\n",
    "# Finalise workflow: fix hyperparameters and refit on full training set\n",
    "final_wf <- finalize_workflow(wf_tune, best_1se)\n",
    "final_fit <- last_fit(final_wf, split=init_split,\n",
    "                      metrics=metric_set(roc_auc, accuracy, f_meas))\n",
    "\n",
    "cat(\"\\nTest set performance (evaluated once):\\n\")\n",
    "collect_metrics(final_fit) %>%\n",
    "  mutate(.estimate=round(.estimate, 4)) %>%\n",
    "  print()\n",
    "# last_fit() refits on the full training set and evaluates on the test set\n",
    "# This is the final, unbiased performance estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07-pitfalls",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "**1. Tuning hyperparameters using the test set**  \n",
    "Any use of the test set to inform a modelling decision — including hyperparameter choice — contaminates it. After selecting hyperparameters via CV on the training set, `last_fit()` evaluates on the test set exactly once. Do not iterate based on test set results.\n",
    "\n",
    "**2. Using exhaustive grid search for ≥ 3 parameters**  \n",
    "A 5-level grid over 5 parameters is 5⁵ = 3125 combinations. At 5-fold CV, that is 15,625 model fits. Random search or Latin hypercube sampling with 30–100 candidates achieves similar or better results in a fraction of the computation because most parameters are not the most important one.\n",
    "\n",
    "**3. Reporting CV performance from the tuning step as the final model performance**  \n",
    "The CV AUC used to select hyperparameters is optimistically biased — the hyperparameters were chosen to maximise it. This is the motivation for nested CV. Always use `last_fit()` on the held-out test set, or use nested CV, for the reported performance.\n",
    "\n",
    "**4. Not using the 1-SE rule when appropriate**  \n",
    "The best hyperparameters by mean CV metric may differ from a simpler model by much less than 1 SE. The 1-SE rule (`select_by_one_std_err()`) selects the simplest model within sampling noise of the best — it reduces complexity and overfitting risk at essentially no cost in expected performance.\n",
    "\n",
    "**5. Fixing `trees` during tuning for tree ensembles**  \n",
    "For XGBoost and LightGBM, `nrounds` (number of trees) should be determined by early stopping, not fixed in advance. Fix a maximum and use early stopping on a validation fold inside each CV fold. Fixing too few trees underestimates potential; fixing too many risks overfitting without stopping.\n",
    "\n",
    "---\n",
    "*r_methods_library · Samantha McGarrigle · [github.com/samantha-mcgarrigle](https://github.com/samantha-mcgarrigle)*"
   ]
  }
 ]
}"
